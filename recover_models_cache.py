#!/usr/bin/env python3
"""
æ¨¡å‹ç¼“å­˜æ¢å¤è„šæœ¬
ä» model_discovery_log.json æ¢å¤å®Œæ•´çš„ discovered_models.json ç¼“å­˜
"""

import json
import os
from datetime import datetime

def recover_models_cache():
    """ä»å‘ç°æ—¥å¿—æ¢å¤æ¨¡å‹ç¼“å­˜"""
    
    # è¯»å–å‘ç°æ—¥å¿—
    log_file = "cache/model_discovery_log.json"
    cache_file = "cache/discovered_models.json"
    
    if not os.path.exists(log_file):
        print(f"ERROR: å‘ç°æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
        return False
    
    print(f"INFO: è¯»å–å‘ç°æ—¥å¿—: {log_file}")
    with open(log_file, 'r', encoding='utf-8') as f:
        log = json.load(f)
    
    discovered = log.get('discovered_models', {})
    if not discovered:
        print("ERROR: æ—¥å¿—ä¸­æ²¡æœ‰å‘ç°çš„æ¨¡å‹æ•°æ®")
        return False
    
    print(f"SUCCESS: å‘ç° {len(discovered)} ä¸ªæ¸ é“çš„æ¨¡å‹æ•°æ®")
    
    # ç»Ÿè®¡æ¨¡å‹æ•°é‡
    total_models = 0
    new_cache = {}
    
    for channel_id, data in discovered.items():
        models = data.get('models', [])
        model_count = len(models)
        total_models += model_count
        
        # ç”ŸæˆAPI keyçº§åˆ«çš„ç¼“å­˜é”®
        api_key_hash = data.get('api_key_hash', 'unknown')
        if api_key_hash == 'unknown':
            # å¦‚æœæ²¡æœ‰API key hashï¼Œç”Ÿæˆä¸€ä¸ªç®€å•çš„
            api_key_hash = channel_id[-8:] if len(channel_id) >= 8 else channel_id
            
        cache_key = f"{channel_id}_{api_key_hash}"
        
        # æ„å»ºç¼“å­˜æ¡ç›®
        cache_entry = {
            "channel_id": channel_id,
            "provider": data.get('provider', 'unknown'),
            "base_url": data.get('base_url', ''),
            "models_url": data.get('models_url', ''),
            "models": models,
            "model_count": model_count,
            "last_updated": data.get('last_updated', datetime.now().isoformat()),
            "status": "success",
            "cache_key": cache_key,
            "api_key_hash": api_key_hash,
            "user_level": data.get('user_level', 'unknown'),
            "response_data": data.get('response_data', {}),
            "migrated_from_legacy": False,
            "recovered_from_log": True,
            "recovered_at": datetime.now().isoformat()
        }
        
        new_cache[cache_key] = cache_entry
        print(f"  âœ… {channel_id}: {model_count} models -> {cache_key}")
    
    print(f"\nğŸ“Š æ¢å¤ç»Ÿè®¡:")
    print(f"  - æ€»æ¸ é“æ•°: {len(new_cache)}")
    print(f"  - æ€»æ¨¡å‹æ•°: {total_models}")
    
    # å¤‡ä»½ç°æœ‰ç¼“å­˜
    if os.path.exists(cache_file):
        backup_file = f"{cache_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        print(f"ğŸ’¾ å¤‡ä»½ç°æœ‰ç¼“å­˜: {backup_file}")
        with open(backup_file, 'w', encoding='utf-8') as f:
            with open(cache_file, 'r', encoding='utf-8') as original:
                f.write(original.read())
    
    # å†™å…¥æ–°ç¼“å­˜
    print(f"ğŸ’¾ å†™å…¥æ¢å¤çš„ç¼“å­˜: {cache_file}")
    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(new_cache, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ‰ ç¼“å­˜æ¢å¤å®Œæˆï¼")
    print(f"   - æ¢å¤äº† {len(new_cache)} ä¸ªæ¸ é“")
    print(f"   - æ¢å¤äº† {total_models} ä¸ªæ¨¡å‹")
    print(f"   - ä»åŸæ¥çš„ 18 ä¸ªæ¨¡å‹å¢åŠ åˆ° {total_models} ä¸ªæ¨¡å‹")
    
    return True

if __name__ == "__main__":
    print("ğŸ”§ æ™ºèƒ½AIè·¯ç”±å™¨ - æ¨¡å‹ç¼“å­˜æ¢å¤å·¥å…·")
    print("=" * 50)
    
    if recover_models_cache():
        print("\nâœ… æ¢å¤æˆåŠŸï¼è¯·é‡æ–°å¯åŠ¨æœåŠ¡å™¨ä»¥åŠ è½½æ–°çš„ç¼“å­˜æ•°æ®ã€‚")
    else:
        print("\nâŒ æ¢å¤å¤±è´¥ï¼è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å’Œæƒé™ã€‚")