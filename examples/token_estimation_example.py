#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tokené¢„ä¼°å’Œæ¨¡å‹ä¼˜åŒ–åŠŸèƒ½æ¼”ç¤º
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.utils.token_estimator import get_token_estimator, get_model_optimizer, TaskComplexity
from core.utils.api_key_cache_manager import get_api_key_cache_manager
from core.yaml_config import get_yaml_config_loader

def demo_token_estimation():
    """æ¼”ç¤ºTokené¢„ä¼°åŠŸèƒ½"""
    print("ğŸ§  Tokené¢„ä¼°åŠŸèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    estimator = get_token_estimator()
    
    # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡
    test_cases = [
        {
            "name": "ç®€å•å¯¹è¯",
            "messages": [
                {"role": "user", "content": "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
            ]
        },
        {
            "name": "æ–‡æ¡£æ€»ç»“",
            "messages": [
                {"role": "user", "content": "è¯·å¸®æˆ‘æ€»ç»“ä»¥ä¸‹æ–‡æ¡£çš„ä¸»è¦å†…å®¹ï¼šè¿™æ˜¯ä¸€ä»½å…³äºäººå·¥æ™ºèƒ½å‘å±•å†ç¨‹çš„é•¿ç¯‡æ–‡æ¡£ï¼ŒåŒ…å«äº†ä»å›¾çµæµ‹è¯•åˆ°ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„å®Œæ•´å‘å±•å†ç¨‹..."}
            ]
        },
        {
            "name": "ä»£ç ç”Ÿæˆ",
            "messages": [
                {"role": "user", "content": "è¯·å¸®æˆ‘å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œå®ç°äºŒå‰æœç´¢æ ‘çš„æ’å…¥ã€åˆ é™¤å’ŒæŸ¥æ‰¾æ“ä½œï¼Œè¦æ±‚ä»£ç è¦æœ‰è¯¦ç»†æ³¨é‡Šå¹¶ä¸”åŒ…å«å•å…ƒæµ‹è¯•ã€‚"}
            ]
        },
        {
            "name": "ä¸“å®¶çº§åˆ†æ",
            "messages": [
                {"role": "user", "content": "è¯·ä»æŠ€æœ¯æ¶æ„ã€å¸‚åœºå‰æ™¯ã€å•†ä¸šæ¨¡å¼ã€ç«äº‰åˆ†æç­‰å¤šä¸ªç»´åº¦ï¼Œæ·±å…¥åˆ†æChatGPTå¯¹æ•´ä¸ªAIè¡Œä¸šçš„å½±å“ï¼Œå¹¶æå‡ºæœªæ¥5å¹´çš„å‘å±•é¢„æµ‹ã€‚è¦æ±‚åˆ†ææ·±å…¥ã€é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘ã€‚"}
            ]
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{i}. {case['name']}")
        print("-" * 30)
        
        # æ‰§è¡ŒTokené¢„ä¼°
        estimate = estimator.estimate_tokens(case['messages'])
        
        print(f"ğŸ“Š è¾“å…¥Tokens: {estimate.input_tokens}")
        print(f"ğŸ“Š é¢„ä¼°è¾“å‡ºTokens: {estimate.estimated_output_tokens}")
        print(f"ğŸ“Š æ€»è®¡Tokens: {estimate.total_tokens}")
        print(f"ğŸ“Š ä»»åŠ¡å¤æ‚åº¦: {estimate.task_complexity.value.upper()}")
        print(f"ğŸ“Š é¢„ä¼°ç½®ä¿¡åº¦: {estimate.confidence:.1%}")
        
        # æ ¹æ®å¤æ‚åº¦ç»™å‡ºå»ºè®®
        complexity_advice = {
            TaskComplexity.SIMPLE: "å»ºè®®ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼Œå¦‚GPT-3.5æˆ–å°å‚æ•°é‡æ¨¡å‹",
            TaskComplexity.MODERATE: "å»ºè®®ä½¿ç”¨ä¸­ç­‰è§„æ¨¡æ¨¡å‹ï¼Œå¹³è¡¡è´¨é‡å’Œæˆæœ¬",
            TaskComplexity.COMPLEX: "å»ºè®®ä½¿ç”¨é«˜æ€§èƒ½æ¨¡å‹ï¼Œå¦‚GPT-4æˆ–å¤§å‚æ•°é‡æ¨¡å‹",
            TaskComplexity.EXPERT: "å¼ºçƒˆå»ºè®®ä½¿ç”¨é¡¶çº§æ¨¡å‹ï¼Œç¡®ä¿è¾“å‡ºè´¨é‡"
        }
        print(f"ğŸ’¡ ä½¿ç”¨å»ºè®®: {complexity_advice[estimate.task_complexity]}")

def demo_model_optimization():
    """æ¼”ç¤ºæ¨¡å‹ä¼˜åŒ–åŠŸèƒ½"""
    print("\n\nğŸ¯ æ¨¡å‹ä¼˜åŒ–åŠŸèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    optimizer = get_model_optimizer()
    estimator = get_token_estimator()
    
    # æ¨¡æ‹Ÿä¸€ä¸ªä¸­ç­‰å¤æ‚åº¦çš„ä»»åŠ¡
    messages = [
        {"role": "user", "content": "è¯·å¸®æˆ‘å†™ä¸€ç¯‡å…³äºPythonè£…é¥°å™¨çš„æŠ€æœ¯åšå®¢ï¼ŒåŒ…å«åŸºæœ¬æ¦‚å¿µã€ä½¿ç”¨åœºæ™¯å’Œä»£ç ç¤ºä¾‹ã€‚"}
    ]
    
    token_estimate = estimator.estimate_tokens(messages)
    print(f"ğŸ“Š Tokené¢„ä¼°: {token_estimate.total_tokens} tokens ({token_estimate.task_complexity.value})")
    
    # æ¨¡æ‹Ÿå¯ç”¨æ¸ é“
    mock_channels = [
        {
            'id': 'ch_gpt4o_mini',
            'model_name': 'gpt-4o-mini',
            'provider': 'openai',
            'input_price': 0.15,  # $0.15/1M tokens
            'output_price': 0.60,  # $0.60/1M tokens
        },
        {
            'id': 'ch_gpt4',
            'model_name': 'gpt-4',
            'provider': 'openai',
            'input_price': 30.0,  # $30/1M tokens
            'output_price': 60.0,  # $60/1M tokens
        },
        {
            'id': 'ch_claude_haiku',
            'model_name': 'claude-3-haiku',
            'provider': 'anthropic',
            'input_price': 0.25,  # $0.25/1M tokens
            'output_price': 1.25,  # $1.25/1M tokens
        },
        {
            'id': 'ch_llama_free',
            'model_name': 'llama-3.1-8b',
            'provider': 'groq',
            'input_price': 0.0,   # å…è´¹
            'output_price': 0.0,  # å…è´¹
        },
        {
            'id': 'ch_qwen_free',
            'model_name': 'qwen2.5-7b',
            'provider': 'siliconflow',
            'input_price': 0.0,   # å…è´¹
            'output_price': 0.0,  # å…è´¹
        }
    ]
    
    # æµ‹è¯•ä¸åŒä¼˜åŒ–ç­–ç•¥
    strategies = ['cost_first', 'quality_first', 'speed_first', 'balanced']
    
    for strategy in strategies:
        print(f"\nğŸ“ˆ {strategy.upper()} ç­–ç•¥æ¨è:")
        print("-" * 25)
        
        recommendations = optimizer.recommend_models(
            token_estimate, mock_channels, strategy
        )
        
        for i, rec in enumerate(recommendations[:3], 1):
            cost_str = f"${rec.estimated_cost:.6f}" if rec.estimated_cost > 0 else "å…è´¹"
            print(f"  {i}. {rec.model_name}")
            print(f"     ğŸ’° é¢„ä¼°æˆæœ¬: {cost_str}")
            print(f"     â±ï¸  é¢„ä¼°æ—¶é—´: {rec.estimated_time:.1f}ç§’")
            print(f"     â­ è´¨é‡è¯„åˆ†: {rec.quality_score:.2f}")
            print(f"     ğŸ“ æ¨èç†ç”±: {rec.reason}")
            print()

def demo_api_key_cache():
    """æ¼”ç¤ºAPI Keyçº§åˆ«ç¼“å­˜åŠŸèƒ½"""
    print("\n\nğŸ”‘ API Keyçº§åˆ«ç¼“å­˜æ¼”ç¤º")
    print("=" * 50)
    
    cache_manager = get_api_key_cache_manager()
    
    # æ¨¡æ‹Ÿä¸åŒAPI Keyçš„æ¨¡å‹å‘ç°æ•°æ®
    test_data = [
        {
            'channel_id': 'ch_openrouter_001',
            'api_key': 'demo-free-user-key-123',
            'provider': 'openrouter',
            'models_data': {
                'models': [
                    'mistralai/mistral-7b-instruct:free',
                    'meta-llama/llama-3-8b-instruct:free',
                    'google/gemma-7b-it:free'
                ],
                'response_data': {'data': []}
            }
        },
        {
            'channel_id': 'ch_openrouter_001', 
            'api_key': 'demo-pro-user-key-456',
            'provider': 'openrouter',
            'models_data': {
                'models': [
                    'mistralai/mistral-7b-instruct:free',
                    'meta-llama/llama-3-8b-instruct:free', 
                    'google/gemma-7b-it:free',
                    'openai/gpt-4o-mini',
                    'anthropic/claude-3-haiku',
                    'openai/gpt-4o'
                ],
                'response_data': {'data': []}
            }
        }
    ]
    
    # ä¿å­˜ä¸åŒAPI Keyçš„ç¼“å­˜
    for data in test_data:
        cache_manager.save_api_key_models(
            data['channel_id'],
            data['api_key'],
            data['models_data'],
            data['provider']
        )
    
    print("âœ… å·²ä¿å­˜API Keyçº§åˆ«ç¼“å­˜æ•°æ®")
    
    # éªŒè¯ç¼“å­˜æ•°æ®
    print("\nğŸ” éªŒè¯ç¼“å­˜æ•°æ®:")
    for data in test_data:
        cached_data = cache_manager.load_api_key_models(
            data['channel_id'],
            data['api_key']
        )
        
        if cached_data:
            user_type = "å…è´¹ç”¨æˆ·" if len(cached_data['models']) <= 3 else "ä»˜è´¹ç”¨æˆ·"
            api_key_preview = data['api_key'][:8] + "..."
            print(f"  ğŸ“‹ {api_key_preview} ({user_type}): {len(cached_data['models'])} ä¸ªæ¨¡å‹")
            print(f"     å¯ç”¨æ¨¡å‹: {list(cached_data['models'].keys())[:3]}...")
        else:
            print(f"  âŒ API Key {data['api_key'][:8]}... ç¼“å­˜åŠ è½½å¤±è´¥")
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    stats = cache_manager.get_cache_stats()
    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
    print(f"  æ€»ç¼“å­˜æ–‡ä»¶: {stats['total_cache_files']}")
    print(f"  å†…å­˜æ¡ç›®: {stats['memory_entries']}")
    print(f"  æœ‰æ•ˆå†…å­˜æ¡ç›®: {stats['valid_memory_entries']}")
    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}")

async def demo_integration():
    """æ¼”ç¤ºé›†æˆä½¿ç”¨åœºæ™¯"""
    print("\n\nğŸš€ é›†æˆä½¿ç”¨åœºæ™¯æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # å°è¯•åŠ è½½é…ç½®
        config_loader = get_yaml_config_loader()
        channels = config_loader.get_enabled_channels()
        
        print(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€:")
        print(f"  å¯ç”¨æ¸ é“: {len(channels)}")
        
        # æ¨¡æ‹Ÿä¸€ä¸ªå®é™…çš„APIè¯·æ±‚ä¼˜åŒ–è¿‡ç¨‹
        print(f"\nğŸ’¡ æ™ºèƒ½è·¯ç”±å»ºè®®:")
        print(f"  1. å¯¹äºç®€å•å¯¹è¯ï¼Œæ¨èä½¿ç”¨å…è´¹æ¨¡å‹ï¼ˆå¦‚Groq Llama3-8Bï¼‰")
        print(f"  2. å¯¹äºæ–‡æ¡£åˆ†æï¼Œæ¨èä½¿ç”¨æ€§ä»·æ¯”æ¨¡å‹ï¼ˆå¦‚GPT-4O-Miniï¼‰")
        print(f"  3. å¯¹äºä»£ç ç”Ÿæˆï¼Œæ¨èä½¿ç”¨ä¸“ä¸šæ¨¡å‹ï¼ˆå¦‚Claude-3-Haikuï¼‰")
        print(f"  4. å¯¹äºå¤æ‚åˆ†æï¼Œæ¨èä½¿ç”¨é¡¶çº§æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰")
        
    except Exception as e:
        print(f"âš ï¸  é…ç½®åŠ è½½å¤±è´¥: {e}")
        print("è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæ¼”ç¤ºç¯å¢ƒå¯èƒ½æ²¡æœ‰å®Œæ•´é…ç½®")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Smart AI Router - Tokené¢„ä¼°å’ŒAPI Keyçº§åˆ«ç¼“å­˜æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # 1. Tokené¢„ä¼°æ¼”ç¤º
        demo_token_estimation()
        
        # 2. æ¨¡å‹ä¼˜åŒ–æ¼”ç¤º
        demo_model_optimization()
        
        # 3. API Keyç¼“å­˜æ¼”ç¤º
        demo_api_key_cache()
        
        # 4. é›†æˆæ¼”ç¤º
        asyncio.run(demo_integration())
        
        print("\n\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print("  â€¢ Tokené¢„ä¼°å¯ä»¥å¸®åŠ©é€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œmax_tokensè®¾ç½®")
        print("  â€¢ æ¨¡å‹ä¼˜åŒ–å¯ä»¥æ ¹æ®ä»»åŠ¡ç±»å‹æ¨èæœ€ä½³æ¨¡å‹")
        print("  â€¢ API Keyçº§åˆ«ç¼“å­˜è§£å†³äº†ä¸åŒç”¨æˆ·çº§åˆ«çš„å®šä»·å·®å¼‚é—®é¢˜")
        print("  â€¢ æ–°çš„APIç«¯ç‚¹: /v1/token/estimate, /v1/token/pricing ç­‰")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()