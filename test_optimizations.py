#!/usr/bin/env python3
"""
æµ‹è¯•æ€§èƒ½ä¼˜åŒ–æ•ˆæœçš„è„šæœ¬
"""

import asyncio
import time
import httpx
import json
from typing import List, Dict, Any

# æµ‹è¯•é…ç½®
TEST_BASE_URL = "http://localhost:7601"
TEST_ENDPOINTS = {
    "health": f"{TEST_BASE_URL}/health",
    "models": f"{TEST_BASE_URL}/v1/models", 
    "chat": f"{TEST_BASE_URL}/v1/chat/completions"
}

async def test_health_check_speed():
    """æµ‹è¯•å¥åº·æ£€æŸ¥é€Ÿåº¦"""
    print("ğŸ” æµ‹è¯•å¥åº·æ£€æŸ¥é€Ÿåº¦...")
    
    start_time = time.time()
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(TEST_ENDPOINTS["health"])
            
        end_time = time.time()
        latency = (end_time - start_time) * 1000
        
        print(f"âœ… å¥åº·æ£€æŸ¥å»¶è¿Ÿ: {latency:.1f}ms")
        print(f"âœ… çŠ¶æ€ç : {response.status_code}")
        
        return latency, response.status_code == 200
        
    except Exception as e:
        print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return None, False

async def test_models_endpoint_speed():
    """æµ‹è¯•æ¨¡å‹åˆ—è¡¨è·å–é€Ÿåº¦"""
    print("ğŸ“‹ æµ‹è¯•æ¨¡å‹åˆ—è¡¨è·å–é€Ÿåº¦...")
    
    start_time = time.time()
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(TEST_ENDPOINTS["models"])
            
        end_time = time.time()
        latency = (end_time - start_time) * 1000
        
        if response.status_code == 200:
            data = response.json()
            model_count = len(data.get("data", []))
            print(f"âœ… æ¨¡å‹åˆ—è¡¨å»¶è¿Ÿ: {latency:.1f}ms")
            print(f"âœ… å‘ç°æ¨¡å‹æ•°é‡: {model_count}")
            return latency, True, model_count
        else:
            print(f"âŒ æ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥: {response.status_code}")
            return latency, False, 0
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ—è¡¨è·å–å¼‚å¸¸: {e}")
        return None, False, 0

async def test_chat_request_speed(model: str = "tag:gpt", test_message: str = "Hi"):
    """æµ‹è¯•èŠå¤©è¯·æ±‚é€Ÿåº¦"""
    print(f"ğŸ’¬ æµ‹è¯•èŠå¤©è¯·æ±‚é€Ÿåº¦ (æ¨¡å‹: {model})...")
    
    payload = {
        "model": model,
        "messages": [
            {"role": "user", "content": test_message}
        ],
        "max_tokens": 10,
        "temperature": 0.1
    }
    
    start_time = time.time()
    
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                TEST_ENDPOINTS["chat"],
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
        end_time = time.time()
        latency = (end_time - start_time) * 1000
        
        if response.status_code == 200:
            data = response.json()
            used_model = data.get("model", "unknown")
            usage = data.get("usage", {})
            
            print(f"âœ… èŠå¤©è¯·æ±‚å»¶è¿Ÿ: {latency:.1f}ms")
            print(f"âœ… ä½¿ç”¨çš„æ¨¡å‹: {used_model}")
            print(f"âœ… Tokenä½¿ç”¨: {usage}")
            
            # æ£€æŸ¥è°ƒè¯•å¤´
            debug_headers = {k: v for k, v in response.headers.items() if k.startswith("X-Router-")}
            if debug_headers:
                print(f"ğŸ” è·¯ç”±è°ƒè¯•ä¿¡æ¯:")
                for k, v in debug_headers.items():
                    print(f"   {k}: {v}")
            
            return latency, True, used_model
        else:
            print(f"âŒ èŠå¤©è¯·æ±‚å¤±è´¥: {response.status_code}")
            print(f"âŒ é”™è¯¯ä¿¡æ¯: {response.text[:200]}")
            return latency, False, None
            
    except Exception as e:
        print(f"âŒ èŠå¤©è¯·æ±‚å¼‚å¸¸: {e}")
        return None, False, None

async def test_concurrent_requests(count: int = 5):
    """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
    print(f"ğŸš€ æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½ ({count}ä¸ªå¹¶å‘)...")
    
    start_time = time.time()
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = []
    for i in range(count):
        task = test_chat_request_speed(
            model="tag:free", 
            test_message=f"Concurrent test {i+1}"
        )
        tasks.append(task)
    
    # æ‰§è¡Œå¹¶å‘è¯·æ±‚
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    end_time = time.time()
    total_time = (end_time - start_time) * 1000
    
    # åˆ†æç»“æœ
    successful_requests = 0
    total_latency = 0
    
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"âŒ å¹¶å‘è¯·æ±‚ {i+1} å¼‚å¸¸: {result}")
        else:
            latency, success, model = result
            if success and latency:
                successful_requests += 1
                total_latency += latency
                print(f"âœ… å¹¶å‘è¯·æ±‚ {i+1} æˆåŠŸ: {latency:.1f}ms (æ¨¡å‹: {model})")
            else:
                print(f"âŒ å¹¶å‘è¯·æ±‚ {i+1} å¤±è´¥")
    
    if successful_requests > 0:
        avg_latency = total_latency / successful_requests
        print(f"ğŸ“Š å¹¶å‘æµ‹è¯•ç»“æœ:")
        print(f"   æ€»æ—¶é—´: {total_time:.1f}ms")
        print(f"   æˆåŠŸè¯·æ±‚: {successful_requests}/{count}")
        print(f"   å¹³å‡å•è¯·æ±‚å»¶è¿Ÿ: {avg_latency:.1f}ms")
        print(f"   æˆåŠŸç‡: {successful_requests/count*100:.1f}%")
    else:
        print(f"âŒ æ‰€æœ‰å¹¶å‘è¯·æ±‚éƒ½å¤±è´¥äº†")

async def test_negative_tag_filtering():
    """æµ‹è¯•è´Ÿæ ‡ç­¾è¿‡æ»¤åŠŸèƒ½"""
    print("ğŸš« æµ‹è¯•è´Ÿæ ‡ç­¾è¿‡æ»¤åŠŸèƒ½...")
    
    test_cases = [
        ("tag:gpt,!free", "æŸ¥è¯¢GPTæ¨¡å‹ä½†æ’é™¤å…è´¹ç‰ˆ"),
        ("tag:free,!local", "æŸ¥è¯¢å…è´¹æ¨¡å‹ä½†æ’é™¤æœ¬åœ°æ¨¡å‹"), 
        ("tag:qwen3,!embedding", "æŸ¥è¯¢qwen3ä½†æ’é™¤embeddingæ¨¡å‹")
    ]
    
    for model_query, description in test_cases:
        print(f"\nğŸ” æµ‹è¯•ç”¨ä¾‹: {description}")
        print(f"   æŸ¥è¯¢: {model_query}")
        
        latency, success, used_model = await test_chat_request_speed(
            model=model_query,
            test_message="æµ‹è¯•è´Ÿæ ‡ç­¾è¿‡æ»¤"
        )
        
        if success:
            print(f"âœ… è´Ÿæ ‡ç­¾è¿‡æ»¤æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {used_model}")
        else:
            print(f"âŒ è´Ÿæ ‡ç­¾è¿‡æ»¤æµ‹è¯•å¤±è´¥")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("Smart AI Router - Performance Optimization Test")
    print("=" * 50)
    
    # æµ‹è¯•åŸºç¡€åŠŸèƒ½
    print("\nBasic Function Tests")
    await test_health_check_speed()
    await test_models_endpoint_speed()
    
    # æµ‹è¯•å•ä¸ªè¯·æ±‚
    print("\nSingle Request Test")
    await test_chat_request_speed("tag:free", "Hello optimization test!")
    
    # æµ‹è¯•è´Ÿæ ‡ç­¾è¿‡æ»¤
    print("\nNegative Tag Filtering Test")
    await test_negative_tag_filtering()
    
    # æµ‹è¯•å¹¶å‘æ€§èƒ½
    print("\nConcurrent Performance Test")
    await test_concurrent_requests(3)
    await test_concurrent_requests(5)
    
    print("\nAll tests completed!")
    print("\nOptimization Analysis:")
    print("1. Health check latency < 100ms indicates /models endpoint optimization works")
    print("2. Low chat request latency indicates fast-fail detection works")
    print("3. High concurrent success rate indicates smart channel pre-check works")
    print("4. Check X-Router-* headers for routing decision details")

if __name__ == "__main__":
    asyncio.run(main())