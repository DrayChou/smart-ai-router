"""
å†…å­˜ç´¢å¼•ç³»ç»Ÿ - æ¶ˆé™¤æ–‡ä»¶I/Oæ€§èƒ½ç“¶é¢ˆ
æ„å»ºé«˜æ€§èƒ½çš„æ ‡ç­¾â†’æ¨¡å‹æ˜ å°„ç´¢å¼•
"""

import logging
import time
from collections import defaultdict
from dataclasses import dataclass
from typing import Dict, List, Set, Tuple, Optional
import threading

logger = logging.getLogger(__name__)

@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    channel_id: str
    model_name: str
    provider: str
    tags: Set[str]
    pricing: Optional[Dict] = None
    capabilities: Optional[Dict] = None
    specs: Optional[Dict] = None  # æ¨¡å‹è§„æ ¼ä¿¡æ¯ï¼ˆå‚æ•°é‡ã€ä¸Šä¸‹æ–‡é•¿åº¦ç­‰ï¼‰
    # å¥åº·çŠ¶æ€ç¼“å­˜
    health_score: Optional[float] = None
    health_cached_at: Optional[float] = None  # Unixæ—¶é—´æˆ³

@dataclass 
class IndexStats:
    """ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
    total_models: int
    total_channels: int
    total_tags: int
    build_time_ms: float
    memory_usage_mb: float

class MemoryModelIndex:
    """å†…å­˜æ¨¡å‹ç´¢å¼• - é«˜æ€§èƒ½æ ‡ç­¾æŸ¥è¯¢"""
    
    def __init__(self):
        self._lock = threading.RLock()
        
        # æ ¸å¿ƒç´¢å¼•ç»“æ„
        self._tag_to_models: Dict[str, Set[Tuple[str, str]]] = defaultdict(set)  # tag -> {(channel_id, model_name)}
        self._channel_to_models: Dict[str, Set[str]] = defaultdict(set)  # channel_id -> {model_names}
        self._model_info: Dict[Tuple[str, str], ModelInfo] = {}  # (channel_id, model_name) -> ModelInfo
        
        # æ€§èƒ½ç»Ÿè®¡
        self._stats = IndexStats(0, 0, 0, 0.0, 0.0)
        self._last_build_time = 0.0
        
        logger.info("ğŸš€ MemoryModelIndex initialized - ready for high-performance tag queries")
    
    def build_index_from_cache(self, model_cache: Dict[str, Dict]) -> IndexStats:
        """
        ä»æ¨¡å‹ç¼“å­˜æ„å»ºå†…å­˜ç´¢å¼•
        
        Args:
            model_cache: åŸå§‹æ¨¡å‹ç¼“å­˜æ•°æ®
            
        Returns:
            ç´¢å¼•æ„å»ºç»Ÿè®¡ä¿¡æ¯
        """
        start_time = time.time()
        
        with self._lock:
            # æ¸…ç©ºç°æœ‰ç´¢å¼•
            self._tag_to_models.clear()
            self._channel_to_models.clear()
            self._model_info.clear()
            
            logger.info(f"ğŸ”¨ INDEX BUILD: Processing {len(model_cache)} cache entries...")
            
            total_models = 0
            processed_channels = set()
            
            for cache_key, cache_data in model_cache.items():
                if not isinstance(cache_data, dict):
                    continue
                
                # æå–æ¸ é“IDï¼ˆå¤„ç†API Keyçº§åˆ«çš„ç¼“å­˜é”®ï¼‰
                channel_id = self._extract_channel_id(cache_key)
                if not channel_id:
                    continue
                    
                processed_channels.add(channel_id)
                
                # å¤„ç†æ¨¡å‹åˆ—è¡¨
                models = cache_data.get("models", [])
                models_data = cache_data.get("models_data", {})  # æ¨¡å‹è¯¦ç»†è§„æ ¼æ•°æ®
                provider = cache_data.get("provider", "unknown")
                
                for model_name in models:
                    if not isinstance(model_name, str):
                        continue
                    
                    # ç”Ÿæˆæ ‡ç­¾
                    tags = self._generate_model_tags(model_name, provider)
                    
                    # è·å–æ¨¡å‹è¯¦ç»†è§„æ ¼
                    model_specs = models_data.get(model_name, {}) if models_data else {}
                    
                    # åˆ›å»ºæ¨¡å‹ä¿¡æ¯
                    model_key = (channel_id, model_name)
                    model_info = ModelInfo(
                        channel_id=channel_id,
                        model_name=model_name,
                        provider=provider,
                        tags=tags,
                        pricing=cache_data.get("models_pricing", {}).get(model_name),
                        capabilities=self._extract_capabilities(cache_data, model_name)
                    )
                    
                    # æ·»åŠ æ¨¡å‹è§„æ ¼åˆ°ModelInfoï¼ˆæ‰©å±•ç”¨äºå¥åº·æ£€æŸ¥ä¼˜åŒ–ï¼‰
                    if model_specs:
                        model_info.specs = model_specs
                    
                    # æ›´æ–°ç´¢å¼•
                    self._model_info[model_key] = model_info
                    self._channel_to_models[channel_id].add(model_name)
                    
                    # æ›´æ–°æ ‡ç­¾ç´¢å¼•
                    for tag in tags:
                        self._tag_to_models[tag].add(model_key)
                    
                    total_models += 1
            
            # æ„å»ºç»Ÿè®¡ä¿¡æ¯
            build_time_ms = (time.time() - start_time) * 1000
            self._stats = IndexStats(
                total_models=total_models,
                total_channels=len(processed_channels),
                total_tags=len(self._tag_to_models),
                build_time_ms=build_time_ms,
                memory_usage_mb=self._estimate_memory_usage()
            )
            
            self._last_build_time = time.time()
            
            # ä¿å­˜ç¼“å­˜å“ˆå¸Œå€¼ä»¥é¿å…é‡å¤é‡å»º
            try:
                import hashlib
                # ğŸš€ ä¼˜åŒ–ï¼šåªå¯¹å‰50ä¸ªç¼“å­˜é”®è®¡ç®—å“ˆå¸Œï¼Œä¸æ£€æŸ¥é€»è¾‘ä¿æŒä¸€è‡´
                sorted_keys = sorted(model_cache.keys())[:50]
                self._cache_hash = hashlib.md5(str(sorted_keys).encode()).hexdigest()
            except:
                self._cache_hash = None
            
            logger.info(f"âœ… INDEX BUILT: {total_models} models, {len(processed_channels)} channels, "
                       f"{len(self._tag_to_models)} tags in {build_time_ms:.1f}ms")
            
            return self._stats
    
    def find_models_by_tags(self, include_tags: List[str], exclude_tags: List[str] = None) -> List[Tuple[str, str]]:
        """
        æ ¹æ®æ ‡ç­¾æŸ¥æ‰¾æ¨¡å‹ï¼ˆè¶…é«˜é€Ÿï¼‰
        
        Args:
            include_tags: å¿…é¡»åŒ…å«çš„æ ‡ç­¾
            exclude_tags: å¿…é¡»æ’é™¤çš„æ ‡ç­¾
            
        Returns:
            åŒ¹é…çš„æ¨¡å‹åˆ—è¡¨ [(channel_id, model_name), ...]
        """
        if not include_tags:
            return []
        
        with self._lock:
            # å¼€å§‹ä»æœ€å°çš„æ ‡ç­¾é›†åˆè¿›è¡Œäº¤é›†è¿ç®—
            tag_sets = [self._tag_to_models.get(tag, set()) for tag in include_tags]
            if not all(tag_sets):
                return []  # å¦‚æœä»»ä½•æ ‡ç­¾éƒ½æ²¡æœ‰åŒ¹é…ï¼Œç›´æ¥è¿”å›ç©º
            
            # æ‰¾åˆ°æœ€å°çš„é›†åˆä½œä¸ºèµ·ç‚¹ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
            result_set = min(tag_sets, key=len)
            
            # è®¡ç®—äº¤é›†
            for tag_set in tag_sets:
                if tag_set is not result_set:  # é¿å…è‡ªå·±å’Œè‡ªå·±æ±‚äº¤é›†
                    result_set = result_set.intersection(tag_set)
                    if not result_set:  # å¦‚æœäº¤é›†ä¸ºç©ºï¼Œæå‰é€€å‡º
                        break
            
            # æ’é™¤ä¸éœ€è¦çš„æ ‡ç­¾
            if exclude_tags and result_set:
                exclude_set = set()
                for tag in exclude_tags:
                    exclude_set.update(self._tag_to_models.get(tag, set()))
                result_set = result_set - exclude_set
            
            return list(result_set)
    
    def get_model_info(self, channel_id: str, model_name: str) -> Optional[ModelInfo]:
        """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
        with self._lock:
            return self._model_info.get((channel_id, model_name))
    
    def get_model_specs(self, channel_id: str, model_name: str) -> Optional[Dict]:
        """è·å–æ¨¡å‹è§„æ ¼ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæ›¿ä»£æ–‡ä»¶I/Oï¼‰"""
        with self._lock:
            model_info = self._model_info.get((channel_id, model_name))
            return model_info.specs if model_info else None
    
    def get_channel_models(self, channel_id: str) -> Set[str]:
        """è·å–æ¸ é“ä¸‹çš„æ‰€æœ‰æ¨¡å‹"""
        with self._lock:
            return self._channel_to_models.get(channel_id, set()).copy()
    
    def get_health_score(self, channel_id: str, cache_ttl: float = 60.0) -> Optional[float]:
        """è·å–å¥åº·è¯„åˆ†ï¼ˆå¸¦ç¼“å­˜TTLæ£€æŸ¥ï¼‰"""
        current_time = time.time()
        
        with self._lock:
            # æŸ¥æ‰¾è¯¥æ¸ é“ä¸‹çš„ä»»æ„æ¨¡å‹æ¥è·å–å¥åº·çŠ¶æ€
            models = self._channel_to_models.get(channel_id, set())
            if not models:
                return None
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„å¥åº·çŠ¶æ€ï¼ˆæ¸ é“çº§åˆ«ï¼‰
            first_model = next(iter(models))
            model_info = self._model_info.get((channel_id, first_model))
            
            if (model_info and 
                model_info.health_score is not None and 
                model_info.health_cached_at is not None and
                (current_time - model_info.health_cached_at) < cache_ttl):
                return model_info.health_score
            
            return None
    
    def set_health_score(self, channel_id: str, health_score: float):
        """è®¾ç½®å¥åº·è¯„åˆ†ï¼ˆæ›´æ–°æ‰€æœ‰è¯¥æ¸ é“ä¸‹çš„æ¨¡å‹ï¼‰"""
        current_time = time.time()
        
        with self._lock:
            models = self._channel_to_models.get(channel_id, set())
            for model_name in models:
                model_info = self._model_info.get((channel_id, model_name))
                if model_info:
                    model_info.health_score = health_score
                    model_info.health_cached_at = current_time
    
    def get_tag_stats(self) -> Dict[str, int]:
        """è·å–æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            return {tag: len(models) for tag, models in self._tag_to_models.items()}
    
    def get_stats(self) -> IndexStats:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            return self._stats
    
    def is_stale(self, cache_update_time: float) -> bool:
        """æ£€æŸ¥ç´¢å¼•æ˜¯å¦è¿‡æœŸ"""
        return cache_update_time > self._last_build_time
    
    def needs_rebuild(self, model_cache: Dict[str, Dict]) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•"""
        # ğŸš€ ä¿®å¤ï¼šåªæœ‰åœ¨ç´¢å¼•ä¸ºç©ºæ—¶æ‰å¼ºåˆ¶é‡å»º
        if self._stats.total_models == 0:
            logger.debug("INDEX REBUILD: Index is empty, needs rebuild")
            return True
        
        # ğŸš€ ä¿®å¤ï¼šæ›´æ™ºèƒ½çš„ç¼“å­˜å¤§å°æ£€æŸ¥
        current_cache_size = len(model_cache)
        cached_channels = self._stats.total_channels
        
        # ğŸš€ å®¹å¿ç¼“å­˜è¿ç§»é€ æˆçš„å¤§å°å˜åŒ–ï¼ˆä»legacyæ ¼å¼åˆ°API Keyæ ¼å¼ï¼‰
        if current_cache_size < cached_channels:
            # å¦‚æœç¼“å­˜å‡å°‘è¶…è¿‡30%ï¼Œå¯èƒ½æ˜¯è¿ç§»å¯¼è‡´çš„æ¸…ç†ï¼Œéœ€è¦é‡å»º
            reduction_ratio = current_cache_size / cached_channels
            if reduction_ratio < 0.7:  # å‡å°‘è¶…è¿‡30%
                logger.debug(f"INDEX REBUILD: Cache size reduced significantly ({current_cache_size} vs {cached_channels})")
                return True
        elif current_cache_size > cached_channels:
            # ğŸš€ ç¼“å­˜å¢åŠ äº†ï¼Œä»…å½“å¢åŠ è¶…è¿‡50%æ—¶æ‰é‡å»ºï¼ˆæ›´å®½æ¾ï¼‰
            growth_ratio = current_cache_size / cached_channels
            if growth_ratio > 1.5:  # å¢é•¿è¶…è¿‡50%æ‰é‡å»º
                logger.debug(f"INDEX REBUILD: Cache size increased significantly ({current_cache_size} vs {cached_channels})")
                return True
        
        # ğŸš€ æ›´ç¨³å®šçš„å“ˆå¸Œæ£€æŸ¥ï¼šåŸºäºæ¨¡å‹æ€»æ•°è€Œéç¼“å­˜é”®ç»“æ„
        try:
            total_models_in_cache = sum(len(cache_data.get('models', [])) for cache_data in model_cache.values() if isinstance(cache_data, dict))
            if abs(total_models_in_cache - self._stats.total_models) > self._stats.total_models * 0.1:  # æ¨¡å‹æ•°é‡å˜åŒ–è¶…è¿‡10%
                logger.debug(f"INDEX REBUILD: Model count changed significantly ({total_models_in_cache} vs {self._stats.total_models})")
                return True
        except Exception as e:
            logger.debug(f"INDEX REBUILD: Error checking model count: {e}")
            pass
        
        logger.debug(f"INDEX REBUILD: No rebuild needed (cache: {current_cache_size}, indexed: {cached_channels})")
        return False  # é»˜è®¤ä¸é‡å»º
    
    def _extract_channel_id(self, cache_key: str) -> Optional[str]:
        """ä»ç¼“å­˜é”®æå–æ¸ é“ID"""
        if not cache_key:
            return None
            
        # å¤„ç†API Keyçº§åˆ«çš„ç¼“å­˜é”®æ ¼å¼: "channel_id_apikeyash"
        if '_' in cache_key:
            parts = cache_key.split('_')
            if len(parts) >= 2:
                # æ£€æŸ¥æœ€åä¸€éƒ¨åˆ†æ˜¯å¦ä¸ºhashæ ¼å¼ï¼ˆ8ä½åå…­è¿›åˆ¶ï¼‰
                potential_hash = parts[-1]
                if len(potential_hash) == 8 and all(c in '0123456789abcdef' for c in potential_hash.lower()):
                    return '_'.join(parts[:-1])
        
        # ç›´æ¥è¿”å›ä½œä¸ºæ¸ é“ID
        return cache_key
    
    def _generate_model_tags(self, model_name: str, provider: str) -> Set[str]:
        """ä¸ºæ¨¡å‹ç”Ÿæˆæ ‡ç­¾é›†åˆ"""
        tags = set()
        
        if not model_name:
            return tags
        
        model_lower = model_name.lower()
        
        # æ·»åŠ æä¾›å•†æ ‡ç­¾
        if provider:
            tags.add(provider.lower())
        
        # ä½¿ç”¨å¤šç§åˆ†éš”ç¬¦åˆ†å‰²æ¨¡å‹åç§°ç”Ÿæˆæ ‡ç­¾
        separators = [':', '/', '@', '-', '_', ',', '.', ' ']
        tokens = [model_lower]
        
        for sep in separators:
            new_tokens = []
            for token in tokens:
                new_tokens.extend(token.split(sep))
            tokens = [t.strip() for t in new_tokens if t.strip()]
        
        # æ·»åŠ æ‰€æœ‰æœ‰æ•ˆçš„tokenä½œä¸ºæ ‡ç­¾
        for token in tokens:
            if len(token) >= 2:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡ç­¾
                tags.add(token)
        
        # ç‰¹æ®Šæ ‡ç­¾è¯†åˆ«
        if any(free_indicator in model_lower for free_indicator in ['free', 'gratis', 'å…è´¹']):
            tags.add('free')
        
        if any(vision_indicator in model_lower for vision_indicator in ['vision', 'visual', 'image', 'multimodal']):
            tags.add('vision')
            
        if any(code_indicator in model_lower for code_indicator in ['code', 'coder', 'coding', 'program']):
            tags.add('code')
        
        return tags
    
    def _extract_capabilities(self, cache_data: Dict, model_name: str) -> Optional[Dict]:
        """æå–æ¨¡å‹èƒ½åŠ›ä¿¡æ¯"""
        capabilities_data = cache_data.get("models_capabilities", {})
        return capabilities_data.get(model_name) if capabilities_data else None
    
    def _estimate_memory_usage(self) -> float:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        import sys
        
        total_size = 0
        total_size += sys.getsizeof(self._tag_to_models)
        total_size += sys.getsizeof(self._channel_to_models)
        total_size += sys.getsizeof(self._model_info)
        
        # ä¼°ç®—å†…éƒ¨æ•°æ®ç»“æ„çš„å¤§å°
        for tag_set in self._tag_to_models.values():
            total_size += sys.getsizeof(tag_set)
        
        for model_set in self._channel_to_models.values():
            total_size += sys.getsizeof(model_set)
            
        return total_size / 1024 / 1024


# å…¨å±€ç´¢å¼•å®ä¾‹
_memory_index: Optional[MemoryModelIndex] = None

def get_memory_index() -> MemoryModelIndex:
    """è·å–å…¨å±€å†…å­˜ç´¢å¼•å®ä¾‹"""
    global _memory_index
    if _memory_index is None:
        _memory_index = MemoryModelIndex()
    return _memory_index

def rebuild_index_if_needed(model_cache: Dict[str, Dict], force_rebuild: bool = False) -> IndexStats:
    """æŒ‰éœ€é‡å»ºç´¢å¼•"""
    index = get_memory_index()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºï¼ˆè¿™é‡Œå¯ä»¥åŸºäºç¼“å­˜æ›´æ–°æ—¶é—´ç­‰æ¡ä»¶ï¼‰
    if force_rebuild:
        return index.build_index_from_cache(model_cache)
    
    return index.get_stats()