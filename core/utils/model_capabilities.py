"""
æ¨¡å‹èƒ½åŠ›æ£€æµ‹æ¨¡å—
ä½¿ç”¨OpenRouteræ•°æ®åº“ä½œä¸ºé€šç”¨æ¨¡å‹èƒ½åŠ›å‚è€ƒ
"""

import json
import logging
from functools import lru_cache
from pathlib import Path
from typing import Dict, List, Tuple, Optional

logger = logging.getLogger(__name__)

# æ–‡ä»¶å†…å®¹ç¼“å­˜
_cache_content: Optional[Dict] = None
_cache_file_path = Path("cache/channels/openrouter.free.json")


def _load_openrouter_cache() -> Dict:
    """åŠ è½½OpenRouterç¼“å­˜æ•°æ®ï¼ˆå¸¦å†…å­˜ç¼“å­˜ï¼‰"""
    global _cache_content
    
    if _cache_content is not None:
        return _cache_content
    
    try:
        if not _cache_file_path.exists():
            logger.warning(f"OpenRouteræ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {_cache_file_path}")
            _cache_content = {}
            return _cache_content
        
        with open(_cache_file_path, 'r', encoding='utf-8') as f:
            _cache_content = json.load(f)
        
        logger.debug(f"OpenRouterç¼“å­˜å·²åŠ è½½: {len(_cache_content.get('models', {}))} ä¸ªæ¨¡å‹")
        return _cache_content
        
    except (FileNotFoundError, json.JSONDecodeError, PermissionError) as e:
        logger.warning(f"è¯»å–OpenRouteræ•°æ®å¤±è´¥: {type(e).__name__}: {e}")
        _cache_content = {}
        return _cache_content


def _extract_capabilities_from_data(raw_data: Dict) -> List[str]:
    """ä»OpenRouteråŸå§‹æ•°æ®ä¸­æå–èƒ½åŠ›ä¿¡æ¯"""
    capabilities = ["text"]  # é»˜è®¤æ”¯æŒæ–‡æœ¬
    
    # ä»architecture.input_modalitiesè·å–å¤šæ¨¡æ€èƒ½åŠ›
    architecture = raw_data.get('architecture', {})
    input_modalities = architecture.get('input_modalities', [])
    
    if "image" in input_modalities:
        capabilities.append("vision")
    
    # ä»supported_parametersè·å–é«˜çº§èƒ½åŠ›
    supported_params = raw_data.get('supported_parameters', [])
    
    if any(param in supported_params for param in ["tools", "tool_choice"]):
        capabilities.append("function_calling")
    
    if any(param in supported_params for param in ["response_format", "structured_outputs"]):
        capabilities.append("json_mode")
    
    return capabilities


def _extract_context_length(raw_data: Dict) -> int:
    """ä»OpenRouteråŸå§‹æ•°æ®ä¸­æå–ä¸Šä¸‹æ–‡é•¿åº¦"""
    return raw_data.get('context_length', 0)


def _models_match(model_name_1: str, model_name_2: str) -> bool:
    """
    æ£€æŸ¥ä¸¤ä¸ªæ¨¡å‹åç§°æ˜¯å¦æŒ‡å‘åŒä¸€ä¸ªæ¨¡å‹ï¼ˆæ”¯æŒä¸åŒproviderå‰ç¼€ï¼‰
    ä¾‹å¦‚: gpt-4o-mini åŒ¹é… openai/gpt-4o-mini
    """
    # ç§»é™¤providerå‰ç¼€
    clean_1 = model_name_1.split('/')[-1].lower()
    clean_2 = model_name_2.split('/')[-1].lower()
    
    # ç²¾ç¡®åŒ¹é…
    if clean_1 == clean_2:
        return True
    
    # æ¨¡ç³ŠåŒ¹é…ï¼šæ£€æŸ¥æ ¸å¿ƒæ¨¡å‹åç§°
    # ç§»é™¤å¸¸è§çš„å˜ä½“åç¼€
    variants = ['-instruct', '-chat', '-v1', '-v2', '-v3', '-latest', ':free', ':beta']
    
    for variant in variants:
        clean_1 = clean_1.replace(variant, '')
        clean_2 = clean_2.replace(variant, '')
    
    return clean_1 == clean_2


@lru_cache(maxsize=1000)
def get_model_capabilities_from_openrouter(model_name: str) -> Tuple[List[str], int]:
    """
    ä½¿ç”¨OpenRouteræ•°æ®åº“ä½œä¸ºé€šç”¨æ¨¡å‹èƒ½åŠ›å‚è€ƒ
    æ‰€æœ‰æ¸ é“ï¼ˆOpenRouterã€OpenAIã€Anthropicç­‰ï¼‰çš„æ¨¡å‹èƒ½åŠ›éƒ½å‚è€ƒOpenRouterçš„æ¨¡å‹åˆ—è¡¨
    
    Args:
        model_name: æ¨¡å‹åç§°
        
    Returns:
        tuple[capabilities, context_length]: æ¨¡å‹èƒ½åŠ›åˆ—è¡¨å’Œä¸Šä¸‹æ–‡é•¿åº¦
    """
    capabilities = ["text"]  # é»˜è®¤æ”¯æŒæ–‡æœ¬
    context_length = 0
    
    cache_data = _load_openrouter_cache()
    models_data = cache_data.get("models", {})
    
    # ç›´æ¥æŸ¥æ‰¾æ¨¡å‹
    if model_name in models_data:
        model_info = models_data[model_name]
        raw_data = model_info.get("raw_data", {})
        
        capabilities = _extract_capabilities_from_data(raw_data)
        context_length = _extract_context_length(raw_data)
        
        logger.debug(f"ğŸ” ä»OpenRouteræ•°æ®åº“è·å– {model_name}: {capabilities}, context: {context_length}")
        return capabilities, context_length
    
    # å¦‚æœç›´æ¥æŸ¥æ‰¾å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†ä¸åŒproviderçš„ç›¸åŒæ¨¡å‹ï¼‰
    for openrouter_model, model_info in models_data.items():
        if _models_match(model_name, openrouter_model):
            raw_data = model_info.get("raw_data", {})
            
            capabilities = _extract_capabilities_from_data(raw_data)
            context_length = _extract_context_length(raw_data)
            
            logger.debug(f"ğŸ” é€šè¿‡æ¨¡ç³ŠåŒ¹é…è·å– {model_name} -> {openrouter_model}: {capabilities}, context: {context_length}")
            return capabilities, context_length
    
    # å¦‚æœOpenRouteræ•°æ®ä¸å¯ç”¨ï¼Œåªæä¾›åŸºç¡€æ–‡æœ¬èƒ½åŠ›
    logger.debug(f"âš ï¸ æœªæ‰¾åˆ° {model_name} çš„OpenRouteræ•°æ®ï¼Œä½¿ç”¨é»˜è®¤èƒ½åŠ›")
    return capabilities, context_length


def clear_cache() -> None:
    """æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½æ•°æ®"""
    global _cache_content
    _cache_content = None
    # æ¸…é™¤LRUç¼“å­˜
    get_model_capabilities_from_openrouter.cache_clear()
    logger.debug("æ¨¡å‹èƒ½åŠ›ç¼“å­˜å·²æ¸…é™¤")


def get_cache_stats() -> Dict:
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    cache_info = get_model_capabilities_from_openrouter.cache_info()
    return {
        "lru_hits": cache_info.hits,
        "lru_misses": cache_info.misses,
        "lru_current_size": cache_info.currsize,
        "lru_max_size": cache_info.maxsize,
        "file_cached": _cache_content is not None,
        "total_models": len(_cache_content.get("models", {})) if _cache_content else 0
    }