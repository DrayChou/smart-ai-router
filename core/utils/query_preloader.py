#!/usr/bin/env python3
"""
æŸ¥è¯¢é¢„åŠ è½½å™¨ - é¢„åŠ è½½çƒ­ç‚¹æŸ¥è¯¢ä»¥æå‡APIå“åº”é€Ÿåº¦
åŸºäºKISSåŸåˆ™è®¾è®¡ï¼Œä¼˜å…ˆä½¿ç”¨å‡½æ•°è€Œéå¤æ‚ç±»ç»“æ„
"""

import asyncio
import time
import logging
from typing import List, Dict, Any, Optional, Callable
from collections import defaultdict
from dataclasses import dataclass

from .smart_cache import get_smart_cache, cache_set, cache_get

logger = logging.getLogger(__name__)

@dataclass
class HotQuery:
    """çƒ­ç‚¹æŸ¥è¯¢æ¨¡å¼"""
    pattern: str
    frequency: int
    last_accessed: float
    avg_response_time: float

# çƒ­ç‚¹æŸ¥è¯¢ç»Ÿè®¡ - ä½¿ç”¨ç®€å•çš„å…¨å±€å­—å…¸è€Œéç±»
_query_stats: Dict[str, HotQuery] = {}
_preload_tasks: Dict[str, asyncio.Task] = {}

def record_query_access(query_pattern: str, response_time_ms: float):
    """è®°å½•æŸ¥è¯¢è®¿é—®ï¼Œç”¨äºçƒ­ç‚¹æ£€æµ‹"""
    current_time = time.time()
    
    if query_pattern in _query_stats:
        stats = _query_stats[query_pattern]
        stats.frequency += 1
        stats.last_accessed = current_time
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        stats.avg_response_time = (stats.avg_response_time + response_time_ms) / 2
    else:
        _query_stats[query_pattern] = HotQuery(
            pattern=query_pattern,
            frequency=1,
            last_accessed=current_time,
            avg_response_time=response_time_ms
        )

def get_hot_queries(min_frequency: int = 3, max_age_hours: float = 24) -> List[HotQuery]:
    """è·å–çƒ­ç‚¹æŸ¥è¯¢åˆ—è¡¨"""
    current_time = time.time()
    max_age_seconds = max_age_hours * 3600
    
    hot_queries = []
    for query in _query_stats.values():
        # è¿‡æ»¤ï¼šé¢‘ç‡è¶³å¤Ÿé«˜ä¸”æœ€è¿‘è¢«è®¿é—®è¿‡
        if (query.frequency >= min_frequency and 
            current_time - query.last_accessed < max_age_seconds):
            hot_queries.append(query)
    
    # æŒ‰é¢‘ç‡é™åºæ’åº
    return sorted(hot_queries, key=lambda q: q.frequency, reverse=True)

async def preload_query_result(query_pattern: str, preload_func: Callable) -> bool:
    """é¢„åŠ è½½å•ä¸ªæŸ¥è¯¢ç»“æœ"""
    try:
        cache = get_smart_cache()
        cache_key = f"preload_{query_pattern}"
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»é¢„åŠ è½½
        existing = await cache.get("hot_queries", cache_key)
        if existing:
            logger.debug(f"Query already preloaded: {query_pattern}")
            return True
        
        # æ‰§è¡Œé¢„åŠ è½½å‡½æ•°
        start_time = time.time()
        if asyncio.iscoroutinefunction(preload_func):
            result = await preload_func()
        else:
            result = preload_func()
        
        elapsed_ms = (time.time() - start_time) * 1000
        
        # ç¼“å­˜é¢„åŠ è½½ç»“æœ
        await cache.set("hot_queries", cache_key, {
            "result": result,
            "preloaded_at": time.time(),
            "preload_time_ms": elapsed_ms,
            "pattern": query_pattern
        })
        
        logger.info(f"ğŸ”¥ PRELOADED: {query_pattern} in {elapsed_ms:.1f}ms")
        return True
        
    except Exception as e:
        logger.error(f"Failed to preload query {query_pattern}: {e}")
        return False

async def batch_preload_hot_queries(router_instance, max_concurrent: int = 3):
    """æ‰¹é‡é¢„åŠ è½½çƒ­ç‚¹æŸ¥è¯¢"""
    hot_queries = get_hot_queries(min_frequency=5)  # è‡³å°‘è®¿é—®5æ¬¡æ‰é¢„åŠ è½½
    
    if not hot_queries:
        logger.info("No hot queries to preload")
        return
    
    logger.info(f"ğŸ”¥ PRELOADING: {len(hot_queries)} hot queries")
    
    # å®šä¹‰å¸¸è§æŸ¥è¯¢æ¨¡å¼çš„é¢„åŠ è½½å‡½æ•°
    preload_functions = {
        "tag:gpt": lambda: _preload_tag_query(router_instance, ["gpt"]),
        "tag:claude": lambda: _preload_tag_query(router_instance, ["claude"]),
        "tag:free": lambda: _preload_tag_query(router_instance, ["free"]),
        "tag:local": lambda: _preload_tag_query(router_instance, ["local"]),
        "tag:fast": lambda: _preload_tag_query(router_instance, ["fast"]),
        "tag:cheap": lambda: _preload_tag_query(router_instance, ["cheap"]),
    }
    
    # åˆ›å»ºé¢„åŠ è½½ä»»åŠ¡
    tasks = []
    for query in hot_queries[:max_concurrent]:  # é™åˆ¶å¹¶å‘æ•°
        pattern = query.pattern
        if pattern in preload_functions:
            preload_func = preload_functions[pattern]
            task = asyncio.create_task(preload_query_result(pattern, preload_func))
            tasks.append((pattern, task))
    
    # ç­‰å¾…æ‰€æœ‰é¢„åŠ è½½ä»»åŠ¡å®Œæˆ
    results = []
    for pattern, task in tasks:
        try:
            success = await task
            results.append((pattern, success))
        except Exception as e:
            logger.error(f"Preload task failed for {pattern}: {e}")
            results.append((pattern, False))
    
    # ç»Ÿè®¡ç»“æœ
    successful = sum(1 for _, success in results if success)
    logger.info(f"ğŸ”¥ PRELOAD COMPLETE: {successful}/{len(results)} queries preloaded successfully")

async def _preload_tag_query(router_instance, tags: List[str]):
    """é¢„åŠ è½½æ ‡ç­¾æŸ¥è¯¢ï¼ˆè¾…åŠ©å‡½æ•°ï¼‰"""
    from ..json_router import RoutingRequest
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¯·æ±‚
    request = RoutingRequest(
        model=f"tag:{','.join(tags)}",
        messages=[{"role": "user", "content": "test"}],
        temperature=0.7
    )
    
    # æ‰§è¡Œè·¯ç”±æŸ¥è¯¢ï¼ˆè¿™ä¼šå¡«å……ç¼“å­˜ï¼‰
    candidates = await router_instance.route_request(request)
    return {"candidates_count": len(candidates), "tags": tags}

def get_preload_stats() -> Dict[str, Any]:
    """è·å–é¢„åŠ è½½ç»Ÿè®¡ä¿¡æ¯"""
    hot_queries = get_hot_queries()
    
    return {
        "total_tracked_queries": len(_query_stats),
        "hot_queries_count": len(hot_queries),
        "active_preload_tasks": len(_preload_tasks),
        "top_queries": [
            {
                "pattern": q.pattern,
                "frequency": q.frequency,
                "avg_response_time_ms": q.avg_response_time
            }
            for q in hot_queries[:10]
        ]
    }

def cleanup_old_queries(max_age_days: int = 7):
    """æ¸…ç†è¿‡æœŸçš„æŸ¥è¯¢ç»Ÿè®¡"""
    current_time = time.time()
    max_age_seconds = max_age_days * 24 * 3600
    
    expired_patterns = []
    for pattern, query in _query_stats.items():
        if current_time - query.last_accessed > max_age_seconds:
            expired_patterns.append(pattern)
    
    for pattern in expired_patterns:
        del _query_stats[pattern]
    
    if expired_patterns:
        logger.info(f"ğŸ§¹ CLEANUP: Removed {len(expired_patterns)} expired query patterns")

# å¯åŠ¨é¢„åŠ è½½ä»»åŠ¡çš„ä¾¿æ·å‡½æ•°
async def start_preload_scheduler(router_instance, interval_minutes: int = 30):
    """å¯åŠ¨é¢„åŠ è½½è°ƒåº¦å™¨"""
    while True:
        try:
            await batch_preload_hot_queries(router_instance)
            cleanup_old_queries()
            await asyncio.sleep(interval_minutes * 60)
        except Exception as e:
            logger.error(f"Preload scheduler error: {e}")
            await asyncio.sleep(300)  # å‡ºé”™æ—¶ç­‰å¾…5åˆ†é’Ÿ

# ç®€å•çš„è£…é¥°å™¨å‡½æ•°ç”¨äºè‡ªåŠ¨è®°å½•æŸ¥è¯¢
def track_query_performance(query_pattern: str):
    """è£…é¥°å™¨ï¼šè‡ªåŠ¨è·Ÿè¸ªæŸ¥è¯¢æ€§èƒ½"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                elapsed_ms = (time.time() - start_time) * 1000
                record_query_access(query_pattern, elapsed_ms)
                return result
            except Exception as e:
                elapsed_ms = (time.time() - start_time) * 1000
                record_query_access(query_pattern, elapsed_ms)
                raise
        return wrapper
    return decorator