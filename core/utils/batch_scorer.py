"""
æ‰¹é‡è¯„åˆ†å™¨ - ä¼˜åŒ–æ¨¡å‹ç­›é€‰æ€§èƒ½
ç”¨äºå¤§å¹…å‡å°‘å•ä¸ªæ¨¡å‹è¯„åˆ†æ—¶é—´ï¼Œä»70-80msé™ä½åˆ°10msä»¥ä¸‹
"""

import asyncio
import logging
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Any, Optional, cast

from core.json_router import ChannelCandidate

logger = logging.getLogger(__name__)


@dataclass
class BatchedScoreComponents:
    """æ‰¹é‡è¯„åˆ†ç»“æœ"""

    cost_scores: dict[str, float]
    speed_scores: dict[str, float]
    quality_scores: dict[str, float]
    reliability_scores: dict[str, float]
    parameter_scores: dict[str, float]
    context_scores: dict[str, float]
    free_scores: dict[str, float]
    local_scores: dict[str, float]
    computation_time_ms: float


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½ç›‘æ§æŒ‡æ ‡"""

    channel_count: int
    total_time_ms: float
    avg_time_per_channel: float
    cache_hit: bool
    slow_threshold_exceeded: bool = False
    optimization_applied: str = "none"


class BatchScorer:
    """æ‰¹é‡è¯„åˆ†å™¨ - ä¼˜åŒ–æ¨¡å‹ç­›é€‰æ€§èƒ½"""

    def __init__(self, router):
        """åˆå§‹åŒ–æ‰¹é‡è¯„åˆ†å™¨"""
        self.router = router
        self.cache = {}
        self.cache_timeout = 300  # 5åˆ†é’Ÿç¼“å­˜
        self.thread_pool = ThreadPoolExecutor(max_workers=4)

        # æ€§èƒ½ç›‘æ§é…ç½®
        self.slow_threshold_ms = 1000  # æ…¢æŸ¥è¯¢é˜ˆå€¼ï¼š1ç§’
        self.performance_stats = {
            "total_requests": 0,
            "slow_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "cache_hit_rate": 0.0,
            "optimizations_applied": defaultdict(int),
        }

        # [BOOST] æ™ºèƒ½ç¼“å­˜é…ç½®
        self.min_cache_timeout = 60  # 1åˆ†é’Ÿæœ€å°ç¼“å­˜
        self.max_cache_timeout = 600  # 10åˆ†é’Ÿæœ€å¤§ç¼“å­˜
        self.adaptive_cache_timeout = self.cache_timeout  # åˆå§‹å€¼

    def _get_cache_key(self, channels: list[ChannelCandidate], request) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ˆä¼˜åŒ–ç‰ˆï¼šåŒ…å«æ›´å¤šè¯·æ±‚ä¸Šä¸‹æ–‡ï¼‰"""
        channel_ids = sorted([c.channel.id for c in channels])
        model_name = getattr(request, "model", "unknown")

        # [BOOST] æ·»åŠ è¯·æ±‚ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥æé«˜ç¼“å­˜å‘½ä¸­ç‡
        request_context = {
            "model": model_name,
            "channels": channel_ids,
            "timestamp": int(time.time() / 60),  # æ¯åˆ†é’Ÿå˜åŒ–ä¸€æ¬¡
        }

        # ä½¿ç”¨æ›´ç¨³å®šçš„å“ˆå¸Œç®—æ³•
        import hashlib

        key_data = str(request_context).encode("utf-8")
        cache_hash = hashlib.sha256(key_data).hexdigest()[:12]

        return f"batch_score_{cache_hash}"

    def _check_cache(self, cache_key: str) -> Optional[BatchedScoreComponents]:
        """æ£€æŸ¥ç¼“å­˜ï¼ˆæ”¯æŒè‡ªé€‚åº”è¶…æ—¶ï¼‰"""
        if cache_key in self.cache:
            cached_time, cached_result = self.cache[cache_key]

            # [BOOST] ä½¿ç”¨è‡ªé€‚åº”ç¼“å­˜è¶…æ—¶
            current_timeout = self.adaptive_cache_timeout
            if (time.time() - cached_time) < current_timeout:
                logger.debug(
                    f"BATCH_SCORER: Cache hit for {cache_key} (timeout: {current_timeout}s)"
                )
                self.performance_stats["cache_hits"] += 1
                return cast(Optional[BatchedScoreComponents], cached_result)
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œç§»é™¤
                del self.cache[cache_key]

        self.performance_stats["cache_misses"] += 1
        return None

    def _store_cache(self, cache_key: str, result: BatchedScoreComponents):
        """å­˜å‚¨åˆ°ç¼“å­˜ï¼ˆæ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼‰"""
        self.cache[cache_key] = (time.time(), result)

        # [BOOST] æ›´æ–°ç¼“å­˜ç»Ÿè®¡å’Œè‡ªé€‚åº”è¶…æ—¶
        self._update_cache_statistics()

    def _update_cache_statistics(self):
        """æ›´æ–°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯å’Œè‡ªé€‚åº”è¶…æ—¶"""
        total = (
            self.performance_stats["cache_hits"]
            + self.performance_stats["cache_misses"]
        )
        if total > 0:
            hit_rate = self.performance_stats["cache_hits"] / total
            self.performance_stats["cache_hit_rate"] = hit_rate

            # [BOOST] è‡ªé€‚åº”è°ƒæ•´ç¼“å­˜è¶…æ—¶ï¼šå‘½ä¸­ç‡é«˜åˆ™å»¶é•¿è¶…æ—¶
            if hit_rate > 0.7:  # å‘½ä¸­ç‡è¶…è¿‡70%
                self.adaptive_cache_timeout = min(
                    self.max_cache_timeout, self.adaptive_cache_timeout * 1.2
                )
            elif hit_rate < 0.3:  # å‘½ä¸­ç‡ä½äº30%
                self.adaptive_cache_timeout = max(
                    self.min_cache_timeout, self.adaptive_cache_timeout * 0.8
                )

            logger.debug(
                f"CACHE STATS: Hit rate {hit_rate:.1%}, timeout: {self.adaptive_cache_timeout}s"
            )

    def _batch_get_model_specs(
        self, channels: list[ChannelCandidate]
    ) -> dict[str, dict[str, Any]]:
        """æ‰¹é‡è·å–æ¨¡å‹è§„æ ¼ä¿¡æ¯ï¼ˆå†…å­˜ç´¢å¼•ä¼˜åŒ–ç‰ˆï¼‰"""
        model_specs = {}

        # [BOOST] æ€§èƒ½ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨å†…å­˜ç´¢å¼•ï¼Œé¿å…æ–‡ä»¶I/O
        try:
            from core.utils.memory_index import get_memory_index

            memory_index = get_memory_index()

            for candidate in channels:
                if candidate.matched_model:
                    key = f"{candidate.channel.id}_{candidate.matched_model}"

                    # é¦–å…ˆå°è¯•ä»å†…å­˜ç´¢å¼•è·å–
                    specs = memory_index.get_model_specs(
                        candidate.channel.id, candidate.matched_model
                    )
                    if specs:
                        model_specs[key] = specs
                        continue

                    # å›é€€åˆ°æ–‡ä»¶ç¼“å­˜
                    try:
                        channel_cache = self.router.cache_manager.load_channel_models(
                            candidate.channel.id
                        )
                        if channel_cache and "models" in channel_cache:
                            cached_spec = channel_cache["models"].get(
                                candidate.matched_model
                            )
                            if cached_spec:
                                model_specs[key] = cached_spec
                                continue
                    except Exception:
                        pass

                    # æœ€åä½¿ç”¨åˆ†æå™¨
                    try:
                        analyzed_specs = self.router.model_analyzer.analyze_model(
                            candidate.matched_model
                        )
                        model_specs[key] = {
                            "parameter_count": analyzed_specs.parameter_count,
                            "context_length": analyzed_specs.context_length,
                        }
                    except (AttributeError, ValueError):
                        model_specs[key] = {"parameter_count": 0, "context_length": 0}

        except Exception as e:
            logger.warning(
                f"BATCH_SCORER: Memory index lookup failed, falling back to file cache: {e}"
            )
            # å›é€€åˆ°åŸå§‹æ–‡ä»¶ç¼“å­˜æ–¹æ³•
            specs_by_channel = defaultdict(list)
            for candidate in channels:
                if candidate.matched_model:
                    specs_by_channel[candidate.channel.id].append(
                        candidate.matched_model
                    )

            for channel_id, models in specs_by_channel.items():
                try:
                    channel_cache = self.router.cache_manager.load_channel_models(
                        channel_id
                    )
                    if channel_cache and "models" in channel_cache:
                        for model_name in models:
                            key = f"{channel_id}_{model_name}"
                            cached_spec = channel_cache["models"].get(model_name)
                            if cached_spec:
                                model_specs[key] = cached_spec
                            else:
                                analyzed_specs = (
                                    self.router.model_analyzer.analyze_model(model_name)
                                )
                                model_specs[key] = {
                                    "parameter_count": analyzed_specs.parameter_count,
                                    "context_length": analyzed_specs.context_length,
                                }
                except Exception as e2:
                    logger.debug(
                        f"BATCH_SCORER: Failed to load specs for channel {channel_id}: {e2}"
                    )
                    for model_name in models:
                        key = f"{channel_id}_{model_name}"
                        try:
                            analyzed_specs = self.router.model_analyzer.analyze_model(
                                model_name
                            )
                            model_specs[key] = {
                                "parameter_count": analyzed_specs.parameter_count,
                                "context_length": analyzed_specs.context_length,
                            }
                        except (AttributeError, ValueError):
                            model_specs[key] = {
                                "parameter_count": 0,
                                "context_length": 0,
                            }

        return model_specs

    def _batch_calculate_parameter_scores(
        self, channels: list[ChannelCandidate], model_specs: dict[str, dict[str, Any]]
    ) -> dict[str, float]:
        """æ‰¹é‡è®¡ç®—å‚æ•°æ•°é‡è¯„åˆ†"""
        parameter_scores = {}

        for candidate in channels:
            channel_id = candidate.channel.id
            model_name = candidate.matched_model
            key = f"{channel_id}_{model_name}" if model_name else channel_id

            if model_name:
                spec_key = f"{channel_id}_{model_name}"
                specs = model_specs.get(spec_key, {})
                param_count = specs.get("parameter_count", 0)
            else:
                param_count = 0

            # ç¡®ä¿param_countä¸ä¸ºNone
            if param_count is None:
                param_count = 0

            # å‚æ•°æ•°é‡è¯„åˆ†é€»è¾‘
            if param_count >= 70000:  # 70B+
                score = 1.0
            elif param_count >= 30000:  # 30B+
                score = 0.9
            elif param_count >= 8000:  # 8B+
                score = 0.8
            elif param_count >= 3000:  # 3B+
                score = 0.7
            elif param_count >= 1000:  # 1B+
                score = 0.6
            elif param_count >= 500:  # 500M+
                score = 0.5
            elif param_count >= 100:  # 100M+
                score = 0.4
            else:  # <100M or unknown
                score = 0.3

            parameter_scores[key] = score

        return parameter_scores

    def _batch_calculate_context_scores(
        self, channels: list[ChannelCandidate], model_specs: dict[str, dict[str, Any]]
    ) -> dict[str, float]:
        """æ‰¹é‡è®¡ç®—ä¸Šä¸‹æ–‡é•¿åº¦è¯„åˆ†"""
        context_scores = {}

        for candidate in channels:
            channel_id = candidate.channel.id
            model_name = candidate.matched_model
            key = f"{channel_id}_{model_name}" if model_name else channel_id

            if model_name:
                spec_key = f"{channel_id}_{model_name}"
                specs = model_specs.get(spec_key, {})
                context_length = specs.get("context_length", 0)
            else:
                context_length = 0

            # ç¡®ä¿context_lengthä¸ä¸ºNone
            if context_length is None:
                context_length = 0

            # ä¸Šä¸‹æ–‡é•¿åº¦è¯„åˆ†é€»è¾‘
            if context_length >= 1000000:  # 1M+
                score = 1.0
            elif context_length >= 200000:  # 200k+
                score = 0.9
            elif context_length >= 32000:  # 32k+
                score = 0.8
            elif context_length >= 16000:  # 16k+
                score = 0.7
            elif context_length >= 8000:  # 8k+
                score = 0.6
            elif context_length >= 4000:  # 4k+
                score = 0.5
            else:  # <4k
                score = 0.3

            context_scores[key] = score

        return context_scores

    def _batch_calculate_free_scores(
        self, channels: list[ChannelCandidate], model_specs: dict[str, dict[str, Any]]
    ) -> dict[str, float]:
        """æ‰¹é‡è®¡ç®—å…è´¹ä¼˜å…ˆè¯„åˆ†"""
        free_scores = {}
        free_tags = {"free", "å…è´¹", "0cost", "nocost", "trial"}

        for candidate in channels:
            channel = candidate.channel
            model_name = candidate.matched_model
            key = f"{channel.id}_{model_name}" if model_name else channel.id
            score = 0.1  # é»˜è®¤è¯„åˆ†

            # æ£€æŸ¥æ¨¡å‹çº§åˆ«å®šä»·ä¿¡æ¯
            if model_name:
                spec_key = f"{channel.id}_{model_name}"
                specs = model_specs.get(spec_key, {})
                if "raw_data" in specs and "pricing" in specs["raw_data"]:
                    pricing = specs["raw_data"]["pricing"]
                    prompt_cost = float(pricing.get("prompt", "0"))
                    completion_cost = float(pricing.get("completion", "0"))
                    if prompt_cost == 0.0 and completion_cost == 0.0:
                        score = 1.0
                    else:
                        score = 0.1
                # æ£€æŸ¥æ¨¡å‹åç§°ä¸­çš„å…è´¹æ ‡è¯†
                elif any(
                    tag in model_name.lower() for tag in [":free", "-free", "_free"]
                ):
                    score = 1.0

            # å¦‚æœæ¨¡å‹çº§åˆ«æ²¡æœ‰æ˜ç¡®è¯æ®ï¼Œæ£€æŸ¥æ¸ é“çº§åˆ«
            if score == 0.1:
                # æ£€æŸ¥æ¸ é“æˆæœ¬é…ç½®
                cost_per_token = getattr(channel, "cost_per_token", None)
                if cost_per_token:
                    input_cost = cost_per_token.get("input", 0.0)
                    output_cost = cost_per_token.get("output", 0.0)
                    if input_cost <= 0.0 and output_cost <= 0.0:
                        score = 1.0
                    elif model_name and any(
                        tag in model_name.lower() for tag in [":free", "-free"]
                    ):
                        score = 1.0
                    else:
                        score = 0.1

                # æ£€æŸ¥ä¼ ç»Ÿpricingé…ç½®
                elif hasattr(channel, "pricing"):
                    pricing = channel.pricing
                    input_cost = pricing.get("input_cost_per_1k", 0.001)
                    output_cost = pricing.get("output_cost_per_1k", 0.002)
                    avg_cost = (input_cost + output_cost) / 2
                    if avg_cost <= 0.0001:
                        score = 0.9
                    elif avg_cost <= 0.001:
                        score = 0.7
                    elif model_name and any(
                        tag in model_name.lower() for tag in [":free", "-free"]
                    ):
                        score = 1.0
                    else:
                        score = 0.1

                # æ£€æŸ¥æ¸ é“æ ‡ç­¾
                elif any(
                    tag.lower() in free_tags for tag in getattr(channel, "tags", [])
                ):
                    score = 1.0

            free_scores[key] = score

        return free_scores

    def _batch_calculate_basic_scores(
        self, channels: list[ChannelCandidate], request
    ) -> tuple[
        dict[str, float],
        dict[str, float],
        dict[str, float],
        dict[str, float],
        dict[str, float],
    ]:
        """æ‰¹é‡è®¡ç®—åŸºç¡€è¯„åˆ†ï¼ˆæˆæœ¬ã€é€Ÿåº¦ã€è´¨é‡ã€å¯é æ€§ã€æœ¬åœ°ï¼‰"""
        cost_scores = {}
        speed_scores = {}
        quality_scores = {}
        reliability_scores = {}
        local_scores = {}

        # [BOOST] æ‰¹é‡è·å–è¿è¡Œæ—¶çŠ¶æ€ - ä¼˜åŒ–å¥åº·è¯„åˆ†è·å–
        runtime_state = self.router.config_loader.runtime_state
        channel_stats = runtime_state.channel_stats

        # [BOOST] ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨å†…å­˜ç´¢å¼•çš„å¥åº·ç¼“å­˜ï¼Œå‡å°‘å®æ—¶è®¡ç®—
        health_scores_dict = {}
        health_cache_hits = 0
        health_cache_misses = 0

        try:
            from core.utils.memory_index import get_memory_index

            memory_index = get_memory_index()
            for candidate in channels:
                cached_health = memory_index.get_health_score(
                    candidate.channel.id, cache_ttl=600.0
                )  # 10åˆ†é’ŸTTL
                if cached_health is not None:
                    health_scores_dict[candidate.channel.id] = cached_health
                    health_cache_hits += 1
                else:
                    # å›é€€åˆ°è¿è¡Œæ—¶çŠ¶æ€
                    health_scores_dict[candidate.channel.id] = (
                        runtime_state.health_scores.get(candidate.channel.id, 1.0)
                    )
                    health_cache_misses += 1

            if health_cache_hits + health_cache_misses > 0:
                hit_rate = (
                    health_cache_hits / (health_cache_hits + health_cache_misses) * 100
                )
                logger.debug(
                    f"ğŸ¥ HEALTH CACHE: {health_cache_hits}/{health_cache_hits + health_cache_misses} hits ({hit_rate:.1f}%)"
                )

        except Exception as e:
            # å¦‚æœå†…å­˜ç´¢å¼•å¤±è´¥ï¼Œä½¿ç”¨è¿è¡Œæ—¶çŠ¶æ€
            logger.warning(
                f"ğŸ¥ HEALTH CACHE: Failed to access memory index, using runtime state: {e}"
            )
            health_scores_dict = runtime_state.health_scores

        local_tags = {"local", "æœ¬åœ°", "localhost", "127.0.0.1", "offline", "edge"}

        for candidate in channels:
            channel = candidate.channel
            model_name = candidate.matched_model
            key = f"{channel.id}_{model_name}" if model_name else channel.id

            # æˆæœ¬è¯„åˆ†
            cost_scores[key] = self.router._calculate_cost_score(channel, request)

            # é€Ÿåº¦è¯„åˆ† - ä¼˜åŒ–ç‰ˆæœ¬
            stats = channel_stats.get(channel.id)
            if stats and "avg_latency_ms" in stats:
                avg_latency = stats["avg_latency_ms"]
                base_latency = 2000.0
                score = max(0.0, 1.0 - (avg_latency / base_latency))
                speed_scores[key] = 0.1 + score * 0.9
            else:
                speed_scores[key] = channel.performance.get("speed_score", 0.8)

            # è´¨é‡è¯„åˆ† - ç›´æ¥è®¡ç®—
            quality_scores[key] = self.router._calculate_quality_score(
                channel, model_name
            )

            # å¯é æ€§è¯„åˆ†
            reliability_scores[key] = health_scores_dict.get(channel.id, 1.0)

            # æœ¬åœ°è¯„åˆ†
            score = 0.1
            channel_tags_lower = [tag.lower() for tag in channel.tags]
            if any(tag in local_tags for tag in channel_tags_lower):
                score = 1.0
            elif model_name:
                model_tags = self.router._extract_tags_from_model_name(model_name)
                model_tags_lower = [tag.lower() for tag in model_tags]
                if any(tag in local_tags for tag in model_tags_lower):
                    score = 1.0

            # æ£€æŸ¥base_url
            if score == 0.1:
                base_url = getattr(channel, "base_url", None)
                if base_url:
                    base_url_lower = base_url.lower()
                    local_indicators = ["localhost", "127.0.0.1", "0.0.0.0", "::1"]
                    if any(
                        indicator in base_url_lower for indicator in local_indicators
                    ):
                        score = 1.0

            local_scores[key] = score

        return (
            cost_scores,
            speed_scores,
            quality_scores,
            reliability_scores,
            local_scores,
        )

    async def batch_score_channels(
        self, channels: list[ChannelCandidate], request
    ) -> tuple[BatchedScoreComponents, PerformanceMetrics]:
        """æ‰¹é‡è¯„åˆ†æ¸ é“åˆ—è¡¨ï¼Œè¿”å›è¯„åˆ†ç»“æœå’Œæ€§èƒ½æŒ‡æ ‡"""
        start_time = time.time()
        channel_count = len(channels)
        self.performance_stats["total_requests"] += 1

        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(channels, request)
        cached_result = self._check_cache(cache_key)
        if cached_result:
            elapsed_ms = (time.time() - start_time) * 1000
            self.performance_stats["cache_hits"] += 1
            logger.info(
                f"BATCH_SCORER: Using cached scores for {channel_count} channels ({elapsed_ms:.1f}ms)"
            )

            metrics = PerformanceMetrics(
                channel_count=channel_count,
                total_time_ms=elapsed_ms,
                avg_time_per_channel=elapsed_ms / max(channel_count, 1),
                cache_hit=True,
                optimization_applied="cache_hit",
            )
            return cached_result, metrics

        logger.info(f"BATCH_SCORER: Computing scores for {len(channels)} channels")

        # æ‰¹é‡è·å–æ¨¡å‹è§„æ ¼ï¼ˆæœ€è€—æ—¶çš„æ“ä½œï¼‰
        model_specs = self._batch_get_model_specs(channels)

        # å¹¶è¡Œè®¡ç®—å„ç§è¯„åˆ†
        async def compute_scores():
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè®¡ç®—å¯†é›†å‹ä»»åŠ¡
            loop = asyncio.get_event_loop()

            # åˆ†æ‰¹å¹¶è¡Œæ‰§è¡Œè¯„åˆ†è®¡ç®—
            basic_scores_future = loop.run_in_executor(
                self.thread_pool, self._batch_calculate_basic_scores, channels, request
            )

            parameter_scores_future = loop.run_in_executor(
                self.thread_pool,
                self._batch_calculate_parameter_scores,
                channels,
                model_specs,
            )

            context_scores_future = loop.run_in_executor(
                self.thread_pool,
                self._batch_calculate_context_scores,
                channels,
                model_specs,
            )

            free_scores_future = loop.run_in_executor(
                self.thread_pool,
                self._batch_calculate_free_scores,
                channels,
                model_specs,
            )

            # ç­‰å¾…æ‰€æœ‰è®¡ç®—å®Œæˆ
            basic_scores, parameter_scores, context_scores, free_scores = (
                await asyncio.gather(
                    basic_scores_future,
                    parameter_scores_future,
                    context_scores_future,
                    free_scores_future,
                )
            )

            (
                cost_scores,
                speed_scores,
                quality_scores,
                reliability_scores,
                local_scores,
            ) = basic_scores

            return BatchedScoreComponents(
                cost_scores=cost_scores,
                speed_scores=speed_scores,
                quality_scores=quality_scores,
                reliability_scores=reliability_scores,
                parameter_scores=parameter_scores,
                context_scores=context_scores,
                free_scores=free_scores,
                local_scores=local_scores,
                computation_time_ms=(time.time() - start_time) * 1000,
            )

        result = await compute_scores()

        # ç¼“å­˜ç»“æœ
        self._store_cache(cache_key, result)

        # æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–å»ºè®®
        elapsed_ms = (time.time() - start_time) * 1000
        avg_time_per_channel = elapsed_ms / max(channel_count, 1)
        is_slow = elapsed_ms > self.slow_threshold_ms

        if is_slow:
            self.performance_stats["slow_requests"] += 1
            logger.warning(
                f"[WARNING] SLOW BATCH SCORING: {channel_count} channels took {elapsed_ms:.1f}ms (avg: {avg_time_per_channel:.1f}ms/channel)"
            )

        # åº”ç”¨ä¼˜åŒ–ç­–ç•¥
        optimization = self._determine_optimization(channel_count, elapsed_ms)
        if optimization != "none":
            self.performance_stats["optimizations_applied"][optimization] += 1

        logger.info(
            f"BATCH_SCORER: Computed {channel_count} scores in {elapsed_ms:.1f}ms (avg: {avg_time_per_channel:.1f}ms/channel)"
        )

        metrics = PerformanceMetrics(
            channel_count=channel_count,
            total_time_ms=elapsed_ms,
            avg_time_per_channel=avg_time_per_channel,
            cache_hit=False,
            slow_threshold_exceeded=is_slow,
            optimization_applied=optimization,
        )

        return result, metrics

    def get_score_for_channel(
        self, batch_result: BatchedScoreComponents, candidate: ChannelCandidate
    ) -> dict[str, float]:
        """ä»æ‰¹é‡ç»“æœä¸­è·å–ç‰¹å®šæ¸ é“çš„è¯„åˆ†"""
        channel_id = candidate.channel.id
        model_name = candidate.matched_model
        key = f"{channel_id}_{model_name}" if model_name else channel_id

        return {
            "cost_score": batch_result.cost_scores.get(key, 0.5),
            "speed_score": batch_result.speed_scores.get(key, 0.5),
            "quality_score": batch_result.quality_scores.get(key, 0.5),
            "reliability_score": batch_result.reliability_scores.get(key, 0.5),
            "parameter_score": batch_result.parameter_scores.get(key, 0.5),
            "context_score": batch_result.context_scores.get(key, 0.5),
            "free_score": batch_result.free_scores.get(key, 0.1),
            "local_score": batch_result.local_scores.get(key, 0.1),
        }

    def _determine_optimization(self, channel_count: int, elapsed_ms: float) -> str:
        """ç¡®å®šåº”ç”¨çš„ä¼˜åŒ–ç­–ç•¥"""
        if channel_count > 50 and elapsed_ms > 2000:
            return "large_batch_optimization"
        elif elapsed_ms > 1000:
            return "slow_query_optimization"
        elif channel_count > 20 and elapsed_ms > 500:
            return "medium_batch_optimization"
        return "none"

    def get_performance_stats(self) -> dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.performance_stats["total_requests"]
        if total_requests == 0:
            return {"message": "No requests processed yet"}

        cache_hit_rate = (self.performance_stats["cache_hits"] / total_requests) * 100
        slow_request_rate = (
            self.performance_stats["slow_requests"] / total_requests
        ) * 100

        return {
            "total_requests": total_requests,
            "cache_hit_rate": f"{cache_hit_rate:.1f}%",
            "slow_request_rate": f"{slow_request_rate:.1f}%",
            "slow_threshold_ms": self.slow_threshold_ms,
            "optimizations_applied": dict(
                self.performance_stats["optimizations_applied"]
            ),
            "recommendations": self._generate_performance_recommendations(),
        }

    def _generate_performance_recommendations(self) -> list[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        stats = self.performance_stats

        cache_hit_rate = stats["cache_hits"] / max(stats["total_requests"], 1)
        slow_rate = stats["slow_requests"] / max(stats["total_requests"], 1)

        if cache_hit_rate < 0.5:
            recommendations.append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œè€ƒè™‘å¢åŠ ç¼“å­˜TTLæˆ–ä¼˜åŒ–ç¼“å­˜é”®ç­–ç•¥")

        if slow_rate > 0.1:
            recommendations.append("æ…¢æŸ¥è¯¢æ¯”ä¾‹è¾ƒé«˜ï¼Œè€ƒè™‘å®æ–½æ¸ é“é¢„è¿‡æ»¤æˆ–åˆ†æ‰¹å¤„ç†")

        optimizations_applied = stats["optimizations_applied"]
        if (
            isinstance(optimizations_applied, dict)
            and optimizations_applied.get("large_batch_optimization", 0) > 5
        ):
            recommendations.append("å¤§æ‰¹é‡æŸ¥è¯¢é¢‘ç¹ï¼Œå»ºè®®å®æ–½åˆ†å±‚ç¼“å­˜ç­–ç•¥")

        if len(recommendations) == 0:
            recommendations.append("æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–")

        return recommendations

    def cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆä½¿ç”¨è‡ªé€‚åº”è¶…æ—¶ï¼‰"""
        current_time = time.time()
        expired_keys = []

        for key, (cached_time, _) in self.cache.items():
            if (current_time - cached_time) > self.adaptive_cache_timeout:
                expired_keys.append(key)

        for key in expired_keys:
            del self.cache[key]

        if expired_keys:
            logger.debug(
                f"BATCH_SCORER: Cleaned up {len(expired_keys)} expired cache entries (timeout: {self.adaptive_cache_timeout}s)"
            )
