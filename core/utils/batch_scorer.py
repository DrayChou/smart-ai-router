# -*- coding: utf-8 -*-
"""
æ‰¹é‡è¯„åˆ†å™¨ - ä¼˜åŒ–æ¨¡å‹ç­›é€‰æ€§èƒ½
ç”¨äºå¤§å¹…å‡å°‘å•ä¸ªæ¨¡å‹è¯„åˆ†æ—¶é—´ï¼Œä»70-80msé™ä½åˆ°10msä»¥ä¸‹
"""

import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
import logging
import time
from concurrent.futures import ThreadPoolExecutor

from core.config_models import Channel
from core.json_router import ChannelCandidate

logger = logging.getLogger(__name__)


@dataclass
class BatchedScoreComponents:
    """æ‰¹é‡è¯„åˆ†ç»“æœ"""
    cost_scores: Dict[str, float]
    speed_scores: Dict[str, float] 
    quality_scores: Dict[str, float]
    reliability_scores: Dict[str, float]
    parameter_scores: Dict[str, float]
    context_scores: Dict[str, float]
    free_scores: Dict[str, float]
    local_scores: Dict[str, float]
    computation_time_ms: float


class BatchScorer:
    """æ‰¹é‡è¯„åˆ†å™¨ - ä¼˜åŒ–æ¨¡å‹ç­›é€‰æ€§èƒ½"""
    
    def __init__(self, router):
        """åˆå§‹åŒ–æ‰¹é‡è¯„åˆ†å™¨"""
        self.router = router
        self.cache = {}
        self.cache_timeout = 300  # 5åˆ†é’Ÿç¼“å­˜
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
    
    def _get_cache_key(self, channels: List[ChannelCandidate], request) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        channel_ids = sorted([c.channel.id for c in channels])
        model_name = getattr(request, 'model', 'unknown')
        return f"batch_score_{hash(tuple(channel_ids))}_{model_name}"
    
    def _check_cache(self, cache_key: str) -> Optional[BatchedScoreComponents]:
        """æ£€æŸ¥ç¼“å­˜"""
        if cache_key in self.cache:
            cached_time, cached_result = self.cache[cache_key]
            if (time.time() - cached_time) < self.cache_timeout:
                logger.debug(f"BATCH_SCORER: Cache hit for {cache_key}")
                return cached_result
        return None
    
    def _store_cache(self, cache_key: str, result: BatchedScoreComponents):
        """å­˜å‚¨åˆ°ç¼“å­˜"""
        self.cache[cache_key] = (time.time(), result)
    
    def _batch_get_model_specs(self, channels: List[ChannelCandidate]) -> Dict[str, Dict[str, Any]]:
        """æ‰¹é‡è·å–æ¨¡å‹è§„æ ¼ä¿¡æ¯"""
        model_specs = {}
        
        # æŒ‰æ¸ é“åˆ†ç»„ä»¥ä¼˜åŒ–ç¼“å­˜è®¿é—®
        specs_by_channel = defaultdict(list)
        for candidate in channels:
            if candidate.matched_model:
                specs_by_channel[candidate.channel.id].append(candidate.matched_model)
        
        # æ‰¹é‡åŠ è½½æ¯ä¸ªæ¸ é“çš„æ¨¡å‹ç¼“å­˜
        for channel_id, models in specs_by_channel.items():
            try:
                channel_cache = self.router.cache_manager.load_channel_models(channel_id)
                if channel_cache and 'models' in channel_cache:
                    for model_name in models:
                        key = f"{channel_id}_{model_name}"
                        cached_spec = channel_cache['models'].get(model_name)
                        if cached_spec:
                            model_specs[key] = cached_spec
                        else:
                            # å›é€€åˆ°åˆ†æå™¨
                            analyzed_specs = self.router.model_analyzer.analyze_model(model_name)
                            model_specs[key] = {
                                'parameter_count': analyzed_specs.parameter_count,
                                'context_length': analyzed_specs.context_length
                            }
            except Exception as e:
                logger.debug(f"BATCH_SCORER: Failed to load specs for channel {channel_id}: {e}")
                # å¯¹è¯¥æ¸ é“çš„æ‰€æœ‰æ¨¡å‹ä½¿ç”¨åˆ†æå™¨
                for model_name in models:
                    key = f"{channel_id}_{model_name}"
                    try:
                        analyzed_specs = self.router.model_analyzer.analyze_model(model_name)
                        model_specs[key] = {
                            'parameter_count': analyzed_specs.parameter_count,
                            'context_length': analyzed_specs.context_length
                        }
                    except:
                        model_specs[key] = {'parameter_count': 0, 'context_length': 0}
        
        return model_specs
    
    def _batch_calculate_parameter_scores(
        self, 
        channels: List[ChannelCandidate],
        model_specs: Dict[str, Dict[str, Any]]
    ) -> Dict[str, float]:
        """æ‰¹é‡è®¡ç®—å‚æ•°æ•°é‡è¯„åˆ†"""
        parameter_scores = {}
        
        for candidate in channels:
            channel_id = candidate.channel.id
            model_name = candidate.matched_model
            key = f"{channel_id}_{model_name}" if model_name else channel_id
            
            if model_name:
                spec_key = f"{channel_id}_{model_name}"
                specs = model_specs.get(spec_key, {})
                param_count = specs.get('parameter_count', 0)
            else:
                param_count = 0
            
            # ç¡®ä¿param_countä¸ä¸ºNone
            if param_count is None:
                param_count = 0
            
            # å‚æ•°æ•°é‡è¯„åˆ†é€»è¾‘
            if param_count >= 70000:         # 70B+
                score = 1.0
            elif param_count >= 30000:       # 30B+
                score = 0.9
            elif param_count >= 8000:        # 8B+
                score = 0.8
            elif param_count >= 3000:        # 3B+
                score = 0.7
            elif param_count >= 1000:        # 1B+
                score = 0.6
            elif param_count >= 500:         # 500M+
                score = 0.5
            elif param_count >= 100:         # 100M+
                score = 0.4
            else:                            # <100M or unknown
                score = 0.3
            
            parameter_scores[key] = score
        
        return parameter_scores
    
    def _batch_calculate_context_scores(
        self,
        channels: List[ChannelCandidate], 
        model_specs: Dict[str, Dict[str, Any]]
    ) -> Dict[str, float]:
        """æ‰¹é‡è®¡ç®—ä¸Šä¸‹æ–‡é•¿åº¦è¯„åˆ†"""
        context_scores = {}
        
        for candidate in channels:
            channel_id = candidate.channel.id
            model_name = candidate.matched_model
            key = f"{channel_id}_{model_name}" if model_name else channel_id
            
            if model_name:
                spec_key = f"{channel_id}_{model_name}"
                specs = model_specs.get(spec_key, {})
                context_length = specs.get('context_length', 0)
            else:
                context_length = 0
            
            # ç¡®ä¿context_lengthä¸ä¸ºNone
            if context_length is None:
                context_length = 0
            
            # ä¸Šä¸‹æ–‡é•¿åº¦è¯„åˆ†é€»è¾‘
            if context_length >= 1000000:       # 1M+
                score = 1.0
            elif context_length >= 200000:      # 200k+
                score = 0.9
            elif context_length >= 32000:       # 32k+
                score = 0.8
            elif context_length >= 16000:       # 16k+
                score = 0.7
            elif context_length >= 8000:        # 8k+
                score = 0.6
            elif context_length >= 4000:        # 4k+
                score = 0.5
            else:                               # <4k
                score = 0.3
            
            context_scores[key] = score
        
        return context_scores
    
    def _batch_calculate_free_scores(
        self,
        channels: List[ChannelCandidate],
        model_specs: Dict[str, Dict[str, Any]]
    ) -> Dict[str, float]:
        """æ‰¹é‡è®¡ç®—å…è´¹ä¼˜å…ˆè¯„åˆ†"""
        free_scores = {}
        free_tags = {"free", "å…è´¹", "0cost", "nocost", "trial"}
        
        for candidate in channels:
            channel = candidate.channel
            model_name = candidate.matched_model
            key = f"{channel.id}_{model_name}" if model_name else channel.id
            score = 0.1  # é»˜è®¤è¯„åˆ†
            
            # æ£€æŸ¥æ¨¡å‹çº§åˆ«å®šä»·ä¿¡æ¯
            if model_name:
                spec_key = f"{channel.id}_{model_name}"
                specs = model_specs.get(spec_key, {})
                if 'raw_data' in specs and 'pricing' in specs['raw_data']:
                    pricing = specs['raw_data']['pricing']
                    prompt_cost = float(pricing.get('prompt', '0'))
                    completion_cost = float(pricing.get('completion', '0'))
                    if prompt_cost == 0.0 and completion_cost == 0.0:
                        score = 1.0
                    else:
                        score = 0.1
                # æ£€æŸ¥æ¨¡å‹åç§°ä¸­çš„å…è´¹æ ‡è¯†
                elif any(tag in model_name.lower() for tag in [":free", "-free", "_free"]):
                    score = 1.0
            
            # å¦‚æœæ¨¡å‹çº§åˆ«æ²¡æœ‰æ˜ç¡®è¯æ®ï¼Œæ£€æŸ¥æ¸ é“çº§åˆ«
            if score == 0.1:
                # æ£€æŸ¥æ¸ é“æˆæœ¬é…ç½®
                cost_per_token = getattr(channel, 'cost_per_token', None)
                if cost_per_token:
                    input_cost = cost_per_token.get("input", 0.0)
                    output_cost = cost_per_token.get("output", 0.0)
                    if input_cost <= 0.0 and output_cost <= 0.0:
                        score = 1.0
                    elif model_name and any(tag in model_name.lower() for tag in [":free", "-free"]):
                        score = 1.0
                    else:
                        score = 0.1
                
                # æ£€æŸ¥ä¼ ç»Ÿpricingé…ç½®
                elif hasattr(channel, 'pricing'):
                    pricing = channel.pricing
                    input_cost = pricing.get("input_cost_per_1k", 0.001)
                    output_cost = pricing.get("output_cost_per_1k", 0.002)
                    avg_cost = (input_cost + output_cost) / 2
                    if avg_cost <= 0.0001:
                        score = 0.9
                    elif avg_cost <= 0.001:
                        score = 0.7
                    elif model_name and any(tag in model_name.lower() for tag in [":free", "-free"]):
                        score = 1.0
                    else:
                        score = 0.1
                
                # æ£€æŸ¥æ¸ é“æ ‡ç­¾
                elif any(tag.lower() in free_tags for tag in getattr(channel, 'tags', [])):
                    score = 1.0
            
            free_scores[key] = score
        
        return free_scores
    
    def _batch_calculate_basic_scores(
        self, 
        channels: List[ChannelCandidate],
        request
    ) -> Tuple[Dict[str, float], Dict[str, float], Dict[str, float], Dict[str, float], Dict[str, float]]:
        """æ‰¹é‡è®¡ç®—åŸºç¡€è¯„åˆ†ï¼ˆæˆæœ¬ã€é€Ÿåº¦ã€è´¨é‡ã€å¯é æ€§ã€æœ¬åœ°ï¼‰"""
        cost_scores = {}
        speed_scores = {}
        quality_scores = {}
        reliability_scores = {}
        local_scores = {}
        
        # ğŸš€ æ‰¹é‡è·å–è¿è¡Œæ—¶çŠ¶æ€ - ä¼˜åŒ–å¥åº·è¯„åˆ†è·å–
        runtime_state = self.router.config_loader.runtime_state
        channel_stats = runtime_state.channel_stats
        
        # ğŸš€ ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨å†…å­˜ç´¢å¼•çš„å¥åº·ç¼“å­˜ï¼Œå‡å°‘å®æ—¶è®¡ç®—
        health_scores_dict = {}
        health_cache_hits = 0
        health_cache_misses = 0
        
        try:
            from core.utils.memory_index import get_memory_index
            memory_index = get_memory_index()
            for candidate in channels:
                cached_health = memory_index.get_health_score(candidate.channel.id, cache_ttl=600.0)  # 10åˆ†é’ŸTTL
                if cached_health is not None:
                    health_scores_dict[candidate.channel.id] = cached_health
                    health_cache_hits += 1
                else:
                    # å›é€€åˆ°è¿è¡Œæ—¶çŠ¶æ€
                    health_scores_dict[candidate.channel.id] = runtime_state.health_scores.get(candidate.channel.id, 1.0)
                    health_cache_misses += 1
            
            if health_cache_hits + health_cache_misses > 0:
                hit_rate = health_cache_hits / (health_cache_hits + health_cache_misses) * 100
                logger.debug(f"ğŸ¥ HEALTH CACHE: {health_cache_hits}/{health_cache_hits + health_cache_misses} hits ({hit_rate:.1f}%)")
                
        except Exception as e:
            # å¦‚æœå†…å­˜ç´¢å¼•å¤±è´¥ï¼Œä½¿ç”¨è¿è¡Œæ—¶çŠ¶æ€
            logger.warning(f"ğŸ¥ HEALTH CACHE: Failed to access memory index, using runtime state: {e}")
            health_scores_dict = runtime_state.health_scores
        
        local_tags = {"local", "æœ¬åœ°", "localhost", "127.0.0.1", "offline", "edge"}
        
        for candidate in channels:
            channel = candidate.channel
            model_name = candidate.matched_model
            key = f"{channel.id}_{model_name}" if model_name else channel.id
            
            # æˆæœ¬è¯„åˆ†
            cost_scores[key] = self.router._calculate_cost_score(channel, request)
            
            # é€Ÿåº¦è¯„åˆ† - ä¼˜åŒ–ç‰ˆæœ¬
            stats = channel_stats.get(channel.id)
            if stats and "avg_latency_ms" in stats:
                avg_latency = stats["avg_latency_ms"]
                base_latency = 2000.0
                score = max(0.0, 1.0 - (avg_latency / base_latency))
                speed_scores[key] = 0.1 + score * 0.9
            else:
                speed_scores[key] = channel.performance.get("speed_score", 0.8)
            
            # è´¨é‡è¯„åˆ† - ç›´æ¥è®¡ç®—
            quality_scores[key] = self.router._calculate_quality_score(channel, model_name)
            
            # å¯é æ€§è¯„åˆ†
            reliability_scores[key] = health_scores_dict.get(channel.id, 1.0)
            
            # æœ¬åœ°è¯„åˆ†
            score = 0.1
            channel_tags_lower = [tag.lower() for tag in channel.tags]
            if any(tag in local_tags for tag in channel_tags_lower):
                score = 1.0
            elif model_name:
                model_tags = self.router._extract_tags_from_model_name(model_name)
                model_tags_lower = [tag.lower() for tag in model_tags]
                if any(tag in local_tags for tag in model_tags_lower):
                    score = 1.0
            
            # æ£€æŸ¥base_url
            if score == 0.1:
                base_url = getattr(channel, 'base_url', None)
                if base_url:
                    base_url_lower = base_url.lower()
                    local_indicators = ["localhost", "127.0.0.1", "0.0.0.0", "::1"]
                    if any(indicator in base_url_lower for indicator in local_indicators):
                        score = 1.0
            
            local_scores[key] = score
        
        return cost_scores, speed_scores, quality_scores, reliability_scores, local_scores
    
    async def batch_score_channels(
        self, 
        channels: List[ChannelCandidate], 
        request
    ) -> BatchedScoreComponents:
        """æ‰¹é‡è¯„åˆ†æ¸ é“åˆ—è¡¨"""
        start_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(channels, request)
        cached_result = self._check_cache(cache_key)
        if cached_result:
            logger.info(f"BATCH_SCORER: Using cached scores for {len(channels)} channels")
            return cached_result
        
        logger.info(f"BATCH_SCORER: Computing scores for {len(channels)} channels")
        
        # æ‰¹é‡è·å–æ¨¡å‹è§„æ ¼ï¼ˆæœ€è€—æ—¶çš„æ“ä½œï¼‰
        model_specs = self._batch_get_model_specs(channels)
        
        # å¹¶è¡Œè®¡ç®—å„ç§è¯„åˆ†
        async def compute_scores():
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè®¡ç®—å¯†é›†å‹ä»»åŠ¡
            loop = asyncio.get_event_loop()
            
            # åˆ†æ‰¹å¹¶è¡Œæ‰§è¡Œè¯„åˆ†è®¡ç®—
            basic_scores_future = loop.run_in_executor(
                self.thread_pool,
                self._batch_calculate_basic_scores,
                channels, request
            )
            
            parameter_scores_future = loop.run_in_executor(
                self.thread_pool,
                self._batch_calculate_parameter_scores,
                channels, model_specs
            )
            
            context_scores_future = loop.run_in_executor(
                self.thread_pool,
                self._batch_calculate_context_scores,
                channels, model_specs
            )
            
            free_scores_future = loop.run_in_executor(
                self.thread_pool,
                self._batch_calculate_free_scores,
                channels, model_specs
            )
            
            # ç­‰å¾…æ‰€æœ‰è®¡ç®—å®Œæˆ
            basic_scores, parameter_scores, context_scores, free_scores = await asyncio.gather(
                basic_scores_future,
                parameter_scores_future, 
                context_scores_future,
                free_scores_future
            )
            
            cost_scores, speed_scores, quality_scores, reliability_scores, local_scores = basic_scores
            
            return BatchedScoreComponents(
                cost_scores=cost_scores,
                speed_scores=speed_scores,
                quality_scores=quality_scores,
                reliability_scores=reliability_scores,
                parameter_scores=parameter_scores,
                context_scores=context_scores,
                free_scores=free_scores,
                local_scores=local_scores,
                computation_time_ms=(time.time() - start_time) * 1000
            )
        
        result = await compute_scores()
        
        # ç¼“å­˜ç»“æœ
        self._store_cache(cache_key, result)
        
        logger.info(f"BATCH_SCORER: Computed {len(channels)} scores in {result.computation_time_ms:.1f}ms (avg: {result.computation_time_ms/len(channels):.1f}ms/channel)")
        
        return result
    
    def get_score_for_channel(
        self, 
        batch_result: BatchedScoreComponents,
        candidate: ChannelCandidate
    ) -> Dict[str, float]:
        """ä»æ‰¹é‡ç»“æœä¸­è·å–ç‰¹å®šæ¸ é“çš„è¯„åˆ†"""
        channel_id = candidate.channel.id
        model_name = candidate.matched_model
        key = f"{channel_id}_{model_name}" if model_name else channel_id
        
        return {
            'cost_score': batch_result.cost_scores.get(key, 0.5),
            'speed_score': batch_result.speed_scores.get(key, 0.5),
            'quality_score': batch_result.quality_scores.get(key, 0.5),
            'reliability_score': batch_result.reliability_scores.get(key, 0.5),
            'parameter_score': batch_result.parameter_scores.get(key, 0.5),
            'context_score': batch_result.context_scores.get(key, 0.5),
            'free_score': batch_result.free_scores.get(key, 0.1),
            'local_score': batch_result.local_scores.get(key, 0.1)
        }
    
    def cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        expired_keys = []
        
        for key, (cached_time, _) in self.cache.items():
            if (current_time - cached_time) > self.cache_timeout:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.cache[key]
        
        if expired_keys:
            logger.debug(f"BATCH_SCORER: Cleaned up {len(expired_keys)} expired cache entries")