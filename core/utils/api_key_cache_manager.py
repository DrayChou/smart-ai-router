#!/usr/bin/env python3
"""
API Keyçº§åˆ«ç¼“å­˜ç®¡ç†å™¨
è§£å†³åŒä¸€æ¸ é“ä¸åŒAPI Keyå¯èƒ½æœ‰ä¸åŒå®šä»·å’Œå¯ç”¨æ¨¡å‹çš„é—®é¢˜
"""

import hashlib
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from .model_analyzer import ModelSpecs, get_model_analyzer

logger = logging.getLogger(__name__)


class ApiKeyCacheManager:
    """API Keyçº§åˆ«çš„ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # åˆ›å»ºAPI Keyçº§åˆ«ç¼“å­˜å­ç›®å½•
        self.api_keys_cache_dir = self.cache_dir / "api_keys"
        self.api_keys_cache_dir.mkdir(exist_ok=True)

        # åˆ›å»ºæ˜ å°„æ–‡ä»¶ç›®å½•
        self.mappings_dir = self.cache_dir / "mappings"
        self.mappings_dir.mkdir(exist_ok=True)

        self.model_analyzer = get_model_analyzer()

        # å†…å­˜ç¼“å­˜ï¼Œé¿å…é¢‘ç¹æ–‡ä»¶è¯»å–
        self._memory_cache: Dict[str, Dict[str, Any]] = {}
        self._cache_ttl: Dict[str, datetime] = {}
        self._ttl_duration = timedelta(hours=1)  # 1å°æ—¶TTL

    def _get_api_key_hash(self, api_key: str) -> str:
        """ç”ŸæˆAPI Keyçš„å®‰å…¨å“ˆå¸Œå€¼"""
        if not api_key or api_key == "dummy":
            return "default"

        # ä½¿ç”¨SHA256ç”Ÿæˆä¸å¯é€†å“ˆå¸Œ
        hash_obj = hashlib.sha256(api_key.encode("utf-8"))
        return hash_obj.hexdigest()[:16]  # å–å‰16å­—ç¬¦ä½œä¸ºæ–‡ä»¶å

    def _get_cache_key(self, channel_id: str, api_key: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        api_key_hash = self._get_api_key_hash(api_key)
        return f"{channel_id}_{api_key_hash}"

    def _is_cache_valid(self, cache_key: str) -> bool:
        """æ£€æŸ¥å†…å­˜ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key not in self._cache_ttl:
            return False
        return datetime.now() < self._cache_ttl[cache_key]

    def save_api_key_models(
        self,
        channel_id: str,
        api_key: str,
        models_data: Dict[str, Any],
        provider: str = None,
    ) -> None:
        """ä¿å­˜ç‰¹å®šAPI Keyçš„æ¨¡å‹å‘ç°æ•°æ®"""

        cache_key = self._get_cache_key(channel_id, api_key)
        api_key_hash = self._get_api_key_hash(api_key)

        # æ„å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„
        cache_file = self.api_keys_cache_dir / f"{cache_key}.json"

        # åˆ†ææ‰€æœ‰æ¨¡å‹çš„å‚æ•°å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
        analyzed_models = {}
        if "models" in models_data:
            model_list = models_data["models"]
            response_data = models_data.get("response_data", {})

            for model_name in model_list:
                # æŸ¥æ‰¾å¯¹åº”çš„è¯¦ç»†ä¿¡æ¯
                model_detail = None
                if "data" in response_data:
                    for item in response_data["data"]:
                        if item.get("id") == model_name:
                            model_detail = item
                            break

                # åˆ†ææ¨¡å‹è§„æ ¼
                specs = self.model_analyzer.analyze_model(model_name, model_detail)
                analyzed_models[model_name] = {
                    "id": model_name,
                    "parameter_count": specs.parameter_count,
                    "context_length": specs.context_length,
                    "parameter_size_text": specs.parameter_size_text,
                    "context_text": specs.context_text,
                    "raw_data": model_detail or {},
                    "pricing": (
                        self._extract_pricing_info(model_detail)
                        if model_detail
                        else None
                    ),
                }

        # æ„å»ºå®Œæ•´çš„ç¼“å­˜æ•°æ®
        cache_data = {
            "channel_id": channel_id,
            "api_key_hash": api_key_hash,
            "provider": provider or models_data.get("provider"),
            "basic_info": {
                "base_url": models_data.get("base_url"),
                "models_url": models_data.get("models_url"),
                "last_updated": models_data.get(
                    "last_updated", datetime.now().isoformat()
                ),
                "status": models_data.get("status"),
                "model_count": len(analyzed_models),
            },
            "models": analyzed_models,
            "raw_response": models_data.get("response_data", {}),
            "analysis_metadata": {
                "analyzed_at": datetime.now().isoformat(),
                "analyzer_version": "2.0",  # å‡çº§ç‰ˆæœ¬å·
                "api_key_level": True,  # æ ‡è®°ä¸ºAPI Keyçº§åˆ«ç¼“å­˜
                "models_with_params": sum(
                    1 for m in analyzed_models.values() if m["parameter_count"]
                ),
                "models_with_context": sum(
                    1 for m in analyzed_models.values() if m["context_length"]
                ),
                "models_with_pricing": sum(
                    1 for m in analyzed_models.values() if m.get("pricing")
                ),
            },
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        try:
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)

            # æ›´æ–°å†…å­˜ç¼“å­˜
            self._memory_cache[cache_key] = cache_data
            self._cache_ttl[cache_key] = datetime.now() + self._ttl_duration

            # æ›´æ–°æ˜ å°„æ–‡ä»¶
            self._update_mapping(channel_id, api_key_hash, cache_key)

            logger.info(
                f"âœ… Saved API key level cache for {channel_id} (key: {api_key_hash[:8]}...): {len(analyzed_models)} models"
            )
            logger.info(
                f"   ğŸ“Š Models with parameter info: {cache_data['analysis_metadata']['models_with_params']}"
            )
            logger.info(
                f"   ğŸ’° Models with pricing info: {cache_data['analysis_metadata']['models_with_pricing']}"
            )

        except Exception as e:
            logger.error(f"âŒ Failed to save API key cache for {channel_id}: {e}")

    def _extract_pricing_info(
        self, model_detail: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ä»æ¨¡å‹è¯¦æƒ…ä¸­æå–å®šä»·ä¿¡æ¯"""
        if not model_detail:
            return None

        pricing = {}

        # å¸¸è§çš„å®šä»·å­—æ®µ
        for price_field in ["pricing", "cost", "price", "rates"]:
            if price_field in model_detail:
                pricing[price_field] = model_detail[price_field]

        # æ£€æŸ¥æ˜¯å¦æœ‰è¾“å…¥/è¾“å‡ºå®šä»·
        for field in model_detail:
            if any(
                keyword in field.lower()
                for keyword in ["input", "prompt", "completion", "output"]
            ):
                if any(
                    price_keyword in field.lower()
                    for price_keyword in ["price", "cost", "rate"]
                ):
                    pricing[field] = model_detail[field]

        # æ£€æŸ¥å…è´¹æ ‡è®°
        if "free" in str(model_detail).lower():
            pricing["is_free"] = True

        return pricing if pricing else None

    def _update_mapping(self, channel_id: str, api_key_hash: str, cache_key: str):
        """æ›´æ–°æ¸ é“åˆ°API Keyçš„æ˜ å°„"""
        mapping_file = self.mappings_dir / f"{channel_id}_mapping.json"

        # è¯»å–ç°æœ‰æ˜ å°„
        mapping = {}
        if mapping_file.exists():
            try:
                with open(mapping_file, "r", encoding="utf-8") as f:
                    mapping = json.load(f)
            except Exception as e:
                logger.warning(f"æ— æ³•è¯»å–æ˜ å°„æ–‡ä»¶ {mapping_file}: {e}")

        # æ›´æ–°æ˜ å°„
        mapping[api_key_hash] = {
            "cache_key": cache_key,
            "updated_at": datetime.now().isoformat(),
        }

        # ä¿å­˜æ˜ å°„
        try:
            with open(mapping_file, "w", encoding="utf-8") as f:
                json.dump(mapping, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"æ— æ³•ä¿å­˜æ˜ å°„æ–‡ä»¶ {mapping_file}: {e}")

    def load_api_key_models(
        self, channel_id: str, api_key: str
    ) -> Optional[Dict[str, Any]]:
        """åŠ è½½ç‰¹å®šAPI Keyçš„æ¨¡å‹æ•°æ®"""
        cache_key = self._get_cache_key(channel_id, api_key)

        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if self._is_cache_valid(cache_key):
            return self._memory_cache.get(cache_key)

        # ä»æ–‡ä»¶åŠ è½½
        cache_file = self.api_keys_cache_dir / f"{cache_key}.json"

        if not cache_file.exists():
            return None

        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cache_data = json.load(f)

            # æ›´æ–°å†…å­˜ç¼“å­˜
            self._memory_cache[cache_key] = cache_data
            self._cache_ttl[cache_key] = datetime.now() + self._ttl_duration

            return cache_data

        except Exception as e:
            logger.error(f"âŒ Failed to load API key cache for {channel_id}: {e}")
            return None

    def get_channel_api_keys(self, channel_id: str) -> List[str]:
        """è·å–æ¸ é“ä¸‹çš„æ‰€æœ‰API Keyå“ˆå¸Œ"""
        mapping_file = self.mappings_dir / f"{channel_id}_mapping.json"

        if not mapping_file.exists():
            return []

        try:
            with open(mapping_file, "r", encoding="utf-8") as f:
                mapping = json.load(f)
            return list(mapping.keys())
        except Exception as e:
            logger.error(f"æ— æ³•è¯»å–æ¸ é“æ˜ å°„ {channel_id}: {e}")
            return []

    def load_channel_models_fallback(self, channel_id: str) -> Optional[Dict[str, Any]]:
        """å›é€€åˆ°ä»»æ„å¯ç”¨çš„API Keyæ•°æ®ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰"""
        api_keys = self.get_channel_api_keys(channel_id)

        if not api_keys:
            return None

        # å°è¯•åŠ è½½ç¬¬ä¸€ä¸ªå¯ç”¨çš„API Keyæ•°æ®
        for api_key_hash in api_keys:
            cache_key = f"{channel_id}_{api_key_hash}"
            cache_file = self.api_keys_cache_dir / f"{cache_key}.json"

            if cache_file.exists():
                try:
                    with open(cache_file, "r", encoding="utf-8") as f:
                        return json.load(f)
                except Exception as e:
                    logger.warning(f"æ— æ³•åŠ è½½ç¼“å­˜ {cache_key}: {e}")
                    continue

        return None

    def clear_api_key_cache(self, channel_id: str, api_key: str) -> bool:
        """æ¸…é™¤ç‰¹å®šAPI Keyçš„ç¼“å­˜"""
        cache_key = self._get_cache_key(channel_id, api_key)

        # æ¸…é™¤å†…å­˜ç¼“å­˜
        self._memory_cache.pop(cache_key, None)
        self._cache_ttl.pop(cache_key, None)

        # åˆ é™¤æ–‡ä»¶ç¼“å­˜
        cache_file = self.api_keys_cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                cache_file.unlink()
                logger.info(f"âœ… Cleared API key cache for {channel_id}")
                return True
            except Exception as e:
                logger.error(f"âŒ Failed to clear API key cache: {e}")
                return False

        return True

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_files = len(list(self.api_keys_cache_dir.glob("*.json")))
        memory_entries = len(self._memory_cache)
        valid_memory_entries = sum(
            1 for key in self._memory_cache if self._is_cache_valid(key)
        )

        return {
            "total_cache_files": total_files,
            "memory_entries": memory_entries,
            "valid_memory_entries": valid_memory_entries,
            "cache_hit_rate": valid_memory_entries / max(1, memory_entries),
            "ttl_duration_hours": self._ttl_duration.total_seconds() / 3600,
        }


# å…¨å±€å®ä¾‹
_api_key_cache_manager = None


def get_api_key_cache_manager() -> ApiKeyCacheManager:
    """è·å–API Keyç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _api_key_cache_manager
    if _api_key_cache_manager is None:
        _api_key_cache_manager = ApiKeyCacheManager()
    return _api_key_cache_manager
