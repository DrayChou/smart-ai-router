"""
API Keyçº§åˆ«ç¼“å­˜ç®¡ç†å™¨
è§£å†³æ¸ é“çº§åˆ«å®šä»·æ¶æ„é—®é¢˜ï¼Œæ”¯æŒä¸åŒAPI Keyå¯¹åº”ä¸åŒç”¨æˆ·çº§åˆ«
"""

import hashlib
import logging
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Optional

logger = logging.getLogger(__name__)


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®æ•°æ®ç»“æ„"""

    cache_key: str
    channel_id: str
    api_key_hash: str
    user_level: str  # free/pro/premium/unknown
    models: list[str]
    models_pricing: dict[str, Any]
    status: str  # success/failed/partial
    discovered_at: datetime
    error_message: Optional[str] = None


class ApiKeyCacheManager:
    """API Keyçº§åˆ«ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, hash_length: int = 8):
        """
        Args:
            hash_length: API Keyå“ˆå¸Œé•¿åº¦ï¼Œç”¨äºç”Ÿæˆç¼“å­˜é”®
        """
        self.hash_length = hash_length

    def generate_cache_key(self, channel_id: str, api_key: str) -> str:
        """
        ç”ŸæˆAPI Keyçº§åˆ«çš„ç¼“å­˜é”®

        Args:
            channel_id: æ¸ é“ID
            api_key: APIå¯†é’¥

        Returns:
            ç¼“å­˜é”®ï¼Œæ ¼å¼: {channel_id}_{api_key_hash}

        Examples:
            >>> manager = ApiKeyCacheManager()
            >>> manager.generate_cache_key("siliconflow_1", "sk-abc123")
            'siliconflow_1_a1b2c3d4'
        """
        if not api_key:
            logger.warning(f"API key is empty for channel {channel_id}")
            return channel_id  # å›é€€åˆ°åŸå§‹æ¸ é“ID

        api_key_hash = hashlib.sha256(api_key.encode("utf-8")).hexdigest()[
            : self.hash_length
        ]
        return f"{channel_id}_{api_key_hash}"

    def parse_cache_key(self, cache_key: str) -> tuple[str, str]:
        """
        è§£æç¼“å­˜é”®ï¼Œæå–æ¸ é“IDå’ŒAPI Keyå“ˆå¸Œ

        Args:
            cache_key: ç¼“å­˜é”®

        Returns:
            (channel_id, api_key_hash) å…ƒç»„

        Examples:
            >>> manager = ApiKeyCacheManager()
            >>> manager.parse_cache_key("siliconflow_1_a1b2c3d4")
            ('siliconflow_1', 'a1b2c3d4')
        """
        # ä½¿ç”¨rsplitä»å³ä¾§åˆ†å‰²ï¼Œæ”¯æŒæ¸ é“IDä¸­åŒ…å«ä¸‹åˆ’çº¿
        parts = cache_key.rsplit("_", 1)
        if len(parts) == 2:
            return parts[0], parts[1]  # channel_id, api_key_hash
        return cache_key, ""  # å…¼å®¹æ—§æ ¼å¼

    def is_api_key_cache(self, cache_key: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºAPI Keyçº§åˆ«çš„ç¼“å­˜é”®

        Args:
            cache_key: ç¼“å­˜é”®

        Returns:
            Trueè¡¨ç¤ºæ˜¯API Keyçº§åˆ«ç¼“å­˜
        """
        _, api_key_hash = self.parse_cache_key(cache_key)
        return len(api_key_hash) == self.hash_length

    def find_cache_entries_by_channel(
        self, cache: dict[str, Any], channel_id: str
    ) -> list[str]:
        """
        æŸ¥æ‰¾ç‰¹å®šæ¸ é“çš„æ‰€æœ‰ç¼“å­˜æ¡ç›®

        Args:
            cache: å®Œæ•´çš„æ¨¡å‹ç¼“å­˜
            channel_id: æ¸ é“ID

        Returns:
            è¯¥æ¸ é“çš„æ‰€æœ‰ç¼“å­˜é”®åˆ—è¡¨
        """
        channel_keys = []
        for cache_key in cache.keys():
            parsed_channel_id, _ = self.parse_cache_key(cache_key)
            if parsed_channel_id == channel_id:
                channel_keys.append(cache_key)

        return channel_keys

    def find_cache_entry_by_channel_and_key(
        self, cache: dict[str, Any], channel_id: str, api_key: str
    ) -> Optional[str]:
        """
        æŸ¥æ‰¾ç‰¹å®šæ¸ é“å’ŒAPI Keyå¯¹åº”çš„ç¼“å­˜æ¡ç›®

        Args:
            cache: å®Œæ•´çš„æ¨¡å‹ç¼“å­˜
            channel_id: æ¸ é“ID
            api_key: APIå¯†é’¥

        Returns:
            åŒ¹é…çš„ç¼“å­˜é”®ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å›None
        """
        target_cache_key = self.generate_cache_key(channel_id, api_key)
        return target_cache_key if target_cache_key in cache else None

    def get_cache_statistics(self, cache: dict[str, Any]) -> dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

        Args:
            cache: å®Œæ•´çš„æ¨¡å‹ç¼“å­˜

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total_entries = len(cache)
        api_key_entries = 0
        legacy_entries = 0
        channel_groups: dict[str, int] = {}
        user_levels: dict[str, int] = {}

        for cache_key, cache_data in cache.items():
            if self.is_api_key_cache(cache_key):
                api_key_entries += 1

                # ç»Ÿè®¡æ¸ é“åˆ†ç»„
                channel_id, _ = self.parse_cache_key(cache_key)
                channel_groups[channel_id] = channel_groups.get(channel_id, 0) + 1

                # ç»Ÿè®¡ç”¨æˆ·çº§åˆ«
                user_level = cache_data.get("user_level", "unknown")
                user_levels[user_level] = user_levels.get(user_level, 0) + 1

            else:
                legacy_entries += 1

        return {
            "total_entries": total_entries,
            "api_key_entries": api_key_entries,
            "legacy_entries": legacy_entries,
            "channel_groups": channel_groups,
            "user_levels": user_levels,
            "api_key_coverage": (
                round(api_key_entries / total_entries * 100, 2)
                if total_entries > 0
                else 0
            ),
        }

    def migrate_legacy_cache(
        self, old_cache: dict[str, Any], channels_map: dict[str, Any]
    ) -> dict[str, Any]:
        """
        è¿ç§»æ—§ç¼“å­˜æ ¼å¼åˆ°æ–°çš„API Keyçº§åˆ«æ ¼å¼

        Args:
            old_cache: æ—§çš„æ¸ é“çº§åˆ«ç¼“å­˜
            channels_map: æ¸ é“æ˜ å°„ {channel_id: channel_config}

        Returns:
            æ–°æ ¼å¼çš„ç¼“å­˜
        """
        new_cache = {}
        migrated_count = 0
        kept_count = 0

        for cache_key, cache_data in old_cache.items():
            # å¦‚æœå·²ç»æ˜¯æ–°æ ¼å¼ï¼Œç›´æ¥ä¿ç•™
            if self.is_api_key_cache(cache_key):
                new_cache[cache_key] = cache_data
                kept_count += 1
                continue

            # å°è¯•è¿ç§»æ—§æ ¼å¼
            channel_id = cache_key
            channel_config = channels_map.get(channel_id)

            if channel_config and channel_config.get("api_key"):
                # ç”Ÿæˆæ–°çš„ç¼“å­˜é”®
                new_cache_key = self.generate_cache_key(
                    channel_id, channel_config["api_key"]
                )

                # åˆ›å»ºæ–°æ ¼å¼çš„ç¼“å­˜æ•°æ®
                new_cache[new_cache_key] = {
                    **cache_data,
                    "cache_key": new_cache_key,
                    "channel_id": channel_id,
                    "api_key_hash": new_cache_key.split("_")[-1],
                    "user_level": self._detect_user_level(cache_data, channel_config),
                    "migrated_from_legacy": True,
                    "migrated_at": datetime.now().isoformat(),
                }
                migrated_count += 1
                logger.info(f"Migrated cache: {cache_key} -> {new_cache_key}")

            else:
                # æ— æ³•è¿ç§»ï¼Œä¿æŒåŸæ ¼å¼ï¼ˆå…¼å®¹æ€§ï¼‰
                new_cache[cache_key] = cache_data
                kept_count += 1
                # ğŸš€ ä¼˜åŒ–ï¼šå‡å°‘é‡å¤è­¦å‘Šï¼Œåªåœ¨è°ƒè¯•æ¨¡å¼æ˜¾ç¤º
                logger.debug(
                    f"Cannot migrate cache key {cache_key}: no channel config or API key"
                )

        logger.info(
            f"Cache migration completed: {migrated_count} migrated, {kept_count} kept as-is"
        )
        return new_cache

    def _detect_user_level(
        self, cache_data: dict[str, Any], channel_config: dict[str, Any]
    ) -> str:
        """
        æ£€æµ‹ç”¨æˆ·ç­‰çº§ï¼ˆåŸºäºå·²æœ‰çš„ç¼“å­˜æ•°æ®å’Œæ¸ é“é…ç½®ï¼‰

        Args:
            cache_data: ç¼“å­˜æ•°æ®
            channel_config: æ¸ é“é…ç½®

        Returns:
            ç”¨æˆ·ç­‰çº§å­—ç¬¦ä¸²
        """
        provider = channel_config.get("provider", "")
        models = cache_data.get("models", [])

        # SiliconFlowç”¨æˆ·ç­‰çº§æ£€æµ‹
        if provider == "siliconflow":
            pro_models = [m for m in models if "Pro/" in m or "/Pro" in m]
            if pro_models:
                return "pro"
            return "free"

        # OpenRouterç”¨æˆ·ç­‰çº§æ£€æµ‹
        if provider == "openrouter":
            model_count = len(models)
            if model_count > 100:
                return "premium"
            elif model_count > 50:
                return "pro"
            return "free"

        # å…¶ä»–æä¾›å•†çš„é»˜è®¤æ£€æµ‹é€»è¾‘
        if models:
            # åŸºäºæ¨¡å‹æ•°é‡çš„ç®€å•ä¼°ç®—
            model_count = len(models)
            if model_count > 50:
                return "premium"
            elif model_count > 20:
                return "pro"
            return "free"

        return "unknown"

    def cleanup_invalid_entries(
        self, cache: dict[str, Any], channels_map: dict[str, Any]
    ) -> dict[str, Any]:
        """
        æ¸…ç†æ— æ•ˆçš„ç¼“å­˜æ¡ç›®

        Args:
            cache: æ¨¡å‹ç¼“å­˜
            channels_map: å½“å‰æœ‰æ•ˆçš„æ¸ é“æ˜ å°„

        Returns:
            æ¸…ç†åçš„ç¼“å­˜
        """
        cleaned_cache = {}
        removed_count = 0

        for cache_key, cache_data in cache.items():
            channel_id, _ = self.parse_cache_key(cache_key)

            # æ£€æŸ¥æ¸ é“æ˜¯å¦ä»ç„¶å­˜åœ¨
            if channel_id in channels_map:
                cleaned_cache[cache_key] = cache_data
            else:
                removed_count += 1
                logger.debug(
                    f"Removed invalid cache entry: {cache_key} (channel not found)"
                )

        if removed_count > 0:
            logger.info(f"Cleaned up {removed_count} invalid cache entries")

        return cleaned_cache


# å…¨å±€å®ä¾‹
_api_key_cache_manager: Optional[ApiKeyCacheManager] = None


def get_api_key_cache_manager() -> ApiKeyCacheManager:
    """è·å–å…¨å±€API Keyç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _api_key_cache_manager
    if _api_key_cache_manager is None:
        _api_key_cache_manager = ApiKeyCacheManager()
    return _api_key_cache_manager
