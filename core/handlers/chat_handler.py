# -*- coding: utf-8 -*-
"""
Chat completion request handler with improved architecture
"""
import time
import json
import asyncio
import uuid
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass
import logging

import httpx
from fastapi import HTTPException
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel

from ..json_router import JSONRouter, RoutingRequest, TagNotFoundError, RoutingScore
from ..yaml_config import YAMLConfigLoader
from ..utils.http_client_pool import get_http_pool
from ..utils.smart_cache import cache_get, cache_set
from ..utils.token_counter import get_cost_tracker
from ..utils.request_cache import get_request_cache
from ..utils.response_aggregator import get_response_aggregator, RequestMetadata
from ..utils.session_manager import get_session_manager
from ..utils.cost_estimator import get_cost_estimator
from ..utils.usage_tracker import get_usage_tracker, create_usage_record
from ..utils.channel_monitor import get_channel_monitor, check_api_error_and_alert
from ..utils.token_estimator import get_token_estimator, get_model_optimizer, TaskComplexity
from ..utils.request_interval_manager import get_request_interval_manager
from ..utils.text_processor import clean_model_response
from ..utils.logging_integration import get_enhanced_logger, log_api_request, log_api_response, log_channel_operation

logger = logging.getLogger(__name__)

# --- Request/Response Models ---

class ChatMessage(BaseModel):
    role: str
    content: Union[str, List[Dict[str, Any]]]

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: bool = False
    top_p: Optional[float] = 1.0
    frequency_penalty: Optional[float] = 0.0
    presence_penalty: Optional[float] = 0.0
    functions: Optional[List[Dict[str, Any]]] = None
    function_call: Optional[Union[str, Dict[str, str]]] = None
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[Union[str, Dict[str, Any]]] = None
    system: Optional[str] = None
    extra_params: Optional[Dict[str, Any]] = None

@dataclass
class RoutingResult:
    """è·¯ç”±ç»“æœåŒ…è£…"""
    candidates: List[RoutingScore]
    execution_time: float
    total_candidates: int
    
@dataclass 
class ChannelRequestInfo:
    """æ¸ é“è¯·æ±‚ä¿¡æ¯"""
    url: str
    headers: Dict[str, str]
    request_data: Dict[str, Any]
    channel: Any
    provider: Any
    matched_model: Optional[str]

# --- Core Handler Class ---

class ChatCompletionHandler:
    """èŠå¤©å®Œæˆè¯·æ±‚å¤„ç†å™¨"""
    
    def __init__(self, config_loader: YAMLConfigLoader, router: JSONRouter):
        self.config = config_loader
        self.router = router
        
    async def handle_request(self, request: ChatCompletionRequest) -> Union[JSONResponse, StreamingResponse]:
        """å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚çš„ä¸»å…¥å£"""
        start_time = time.time()
        request_id = f"req_{uuid.uuid4().hex[:8]}"
        
        # ğŸš€ ä½¿ç”¨æ™ºèƒ½æ—¥å¿—è®°å½•APIè¯·æ±‚ (AIRouteråŠŸèƒ½é›†æˆ)
        enhanced_logger = get_enhanced_logger(__name__)
        log_api_request(
            enhanced_logger,
            method="POST",
            url="/v1/chat/completions", 
            headers={"content-type": "application/json"},
            body=f"model: {request.model}, messages: {len(request.messages)} msgs, stream: {request.stream}",
            request_id=request_id,
            model=request.model,
            stream=request.stream
        )
        logger.info(f"REQUEST DETAILS: [{request_id}] {len(request.messages)} messages, max_tokens: {request.max_tokens}, temperature: {request.temperature}")
        
        try:
            # æ­¥éª¤1: è·¯ç”±è¯·æ±‚è·å–å€™é€‰æ¸ é“
            routing_result = await self._route_request_with_fallback(request, start_time)
            if not routing_result.candidates:
                return self._create_no_channels_error(request.model, time.time() - start_time)
            
            # æ­¥éª¤2: è¯·æ±‚å‰æˆæœ¬ä¼°ç®—å’Œä¼˜åŒ–å»ºè®®
            cost_preview = await self._perform_cost_estimation(request, routing_result.candidates, request_id)
            
            # æ­¥éª¤3: æ‰§è¡Œè¯·æ±‚å¹¶å¤„ç†é‡è¯•
            return await self._execute_request_with_retry(request, routing_result, start_time, request_id, cost_preview)
            
        except TagNotFoundError as e:
            return self._handle_tag_not_found_error(e, request.model, time.time() - start_time)
        except HTTPException:
            raise
        except Exception as e:
            return self._handle_unexpected_error(e, request.model, time.time() - start_time)
    
    async def handle_stream_request(self, request: ChatCompletionRequest):
        """å¤„ç†æµå¼è¯·æ±‚"""
        request_id = str(uuid.uuid4())
        start_time = time.time()
        
        logger.info(f"ANTHROPIC STREAM: [{request_id}] Starting stream request for model '{request.model}'")
        
        try:
            # æ­¥éª¤1: è·¯ç”±è¯·æ±‚è·å–å€™é€‰æ¸ é“
            routing_result = await self._route_request_with_fallback(request, start_time)
            if not routing_result.candidates:
                # è¿”å›é”™è¯¯æµ
                return StreamingResponse(
                    self._error_stream_generator("no_channels_available", "No available channels"),
                    media_type="text/event-stream"
                )
            
            # æ­¥éª¤2: æ‰§è¡Œæµå¼è¯·æ±‚
            return StreamingResponse(
                self._execute_stream_request_with_retry(request, routing_result, start_time, request_id),
                media_type="text/event-stream"
            )
                
        except Exception as e:
            logger.error(f"Stream error: {e}")
            return StreamingResponse(
                self._error_stream_generator("stream_error", str(e)),
                media_type="text/event-stream"
            )
    
    async def _execute_stream_request_with_retry(self, request: ChatCompletionRequest, routing_result: RoutingResult, start_time: float, request_id: str):
        """æ‰§è¡Œæµå¼è¯·æ±‚å¹¶å¤„ç†é‡è¯•é€»è¾‘"""
        last_error = None
        failed_channels = set()
        
        for attempt_num, routing_score in enumerate(routing_result.candidates, 1):
            channel = routing_score.channel
            provider = self.config.get_provider(channel.provider)
            
            if channel.id in failed_channels:
                continue
            
            if not provider:
                continue
            
            # æ£€æŸ¥æ¸ é“é—´éš”é™åˆ¶ï¼Œå¦‚æœéœ€è¦ç­‰å¾…åˆ™è·³è¿‡
            interval_manager = get_request_interval_manager()
            min_interval = getattr(channel, 'min_request_interval', 0)
            logger.info(f"ğŸ” STREAM INTERVAL CHECK: Channel '{channel.name}' min_interval={min_interval}s")
            if min_interval > 0:
                is_ready = interval_manager.is_channel_ready(channel.id, min_interval)
                logger.info(f"ğŸ” STREAM INTERVAL CHECK: Channel '{channel.name}' is_ready={is_ready}")
                if not is_ready:
                    wait_time = interval_manager.get_remaining_wait_time(channel.id, min_interval)
                    logger.info(f"â­ï¸ STREAM SKIP #{attempt_num}: Channel '{channel.name}' needs to wait {wait_time:.1f}s (min_interval={min_interval}s), trying next channel")
                    continue
            
            # åœ¨å‘é€æµå¼è¯·æ±‚å‰è®°å½•æ—¶é—´ï¼ˆç”¨äºé—´éš”æ§åˆ¶ï¼‰
            if min_interval > 0:
                interval_manager.record_request(channel.id)
                logger.info(f"ğŸ” STREAM INTERVAL RECORDED: Channel '{channel.name}' request time recorded")
            
            try:
                channel_info = self._prepare_channel_request_info(channel, provider, request, routing_score.matched_model)
                
                # åˆ›å»ºè¯·æ±‚å…ƒæ•°æ®
                aggregator = get_response_aggregator()
                metadata = aggregator.create_request_metadata(
                    request_id=request_id,
                    model_requested=request.model,
                    model_used=channel_info.request_data['model'],
                    channel_name=channel.name,
                    channel_id=channel.id,
                    provider=channel.provider,
                    attempt_count=attempt_num,
                    is_streaming=True,
                    routing_strategy=getattr(self.router, 'current_strategy', 'balanced'),
                    routing_score=routing_score.total_score,
                    routing_reason=routing_score.reason
                )
                
                # æ‰§è¡Œæµå¼è¯·æ±‚ - ç›´æ¥yieldæµå¼å“åº”çš„å†…å®¹
                async for chunk in self._stream_channel_api_with_summary(channel_info.url, channel_info.headers, channel_info.request_data, channel_info.channel.id, metadata):
                    yield chunk
                
                return  # æˆåŠŸåˆ™é€€å‡º
                
            except Exception as e:
                last_error = e
                failed_channels.add(channel.id)
                logger.error(f"Stream attempt {attempt_num} failed: {e}")
                continue
        
        # æ‰€æœ‰æ¸ é“éƒ½å¤±è´¥
        yield self._create_stream_error("all_channels_failed", str(last_error or "All channels failed"))
    
    def _create_stream_error(self, error_type: str, message: str) -> str:
        """åˆ›å»ºæµå¼é”™è¯¯æ¶ˆæ¯"""
        error_data = {
            "type": "error",
            "error": {
                "type": error_type,
                "message": message
            }
        }
        return f"data: {json.dumps(error_data)}\n\n"
    
    async def _error_stream_generator(self, error_type: str, message: str):
        """é”™è¯¯æµç”Ÿæˆå™¨"""
        yield self._create_stream_error(error_type, message)
    
    async def _route_request_with_fallback(self, request: ChatCompletionRequest, start_time: float) -> RoutingResult:
        """æ‰§è¡Œè·¯ç”±è¯·æ±‚å’Œæ™ºèƒ½é¢„æ£€"""
        routing_request = RoutingRequest(
            model=request.model,
            messages=[msg.dict() for msg in request.messages],
            stream=request.stream,
            required_capabilities=self._infer_capabilities(request),
            data=request.dict()  # ä¼ é€’å®Œæ•´çš„è¯·æ±‚æ•°æ®ç”¨äºèƒ½åŠ›æ£€æµ‹
        )
        
        logger.info(f"CHANNEL ROUTING: Starting routing process for model '{request.model}'")
        candidate_channels = await self.router.route_request(routing_request)
        
        if not candidate_channels:
            return RoutingResult(candidates=[], execution_time=time.time() - start_time, total_candidates=0)
        
        logger.info(f"CHANNEL SELECTION: Processing {len(candidate_channels)} channels with intelligent routing")
        
        # æ™ºèƒ½æ¸ é“é¢„æ£€ - å¦‚æœæœ‰å¤šä¸ªæ¸ é“ï¼Œå…ˆå¹¶å‘æ£€æŸ¥å‰3ä¸ª
        if len(candidate_channels) > 1:
            await self._perform_concurrent_channel_check(candidate_channels)
        
        return RoutingResult(
            candidates=candidate_channels,
            execution_time=time.time() - start_time,
            total_candidates=len(candidate_channels)
        )
    
    async def _perform_cost_estimation(self, request: ChatCompletionRequest, candidate_channels: List, request_id: str) -> Optional[Dict[str, Any]]:
        """æ‰§è¡ŒTokené¢„ä¼°å’Œæ™ºèƒ½æ¨¡å‹æ¨è"""
        try:
            # è·å–Tokené¢„ä¼°å™¨å’Œæ¨¡å‹ä¼˜åŒ–å™¨
            token_estimator = get_token_estimator()
            model_optimizer = get_model_optimizer()
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            messages = [{'role': msg.role, 'content': str(msg.content)} for msg in request.messages]
            
            # è¿›è¡ŒTokené¢„ä¼°
            token_estimate = token_estimator.estimate_tokens(messages)
            
            # å‡†å¤‡å€™é€‰æ¸ é“ä¿¡æ¯ï¼ˆåŒ…å«å®šä»·ï¼‰
            available_channels = []
            for channel_score in candidate_channels[:15]:  # åˆ†æå‰15ä¸ªå€™é€‰
                channel = channel_score.channel
                
                # å°è¯•è·å–å®šä»·ä¿¡æ¯
                input_price = getattr(channel, 'input_price', 0.0)
                output_price = getattr(channel, 'output_price', 0.0)
                
                # å¦‚æœæ²¡æœ‰å®šä»·ä¿¡æ¯ï¼Œå°è¯•ä»å®šä»·æ•°æ®ä¸­è·å–
                if input_price == 0.0 and output_price == 0.0:
                    pricing_info = self._get_model_pricing(channel.model_name, channel.provider)
                    if pricing_info:
                        input_price = pricing_info.get('input_price', 0.0)
                        output_price = pricing_info.get('output_price', 0.0)
                
                available_channels.append({
                    'id': channel.id,
                    'model_name': channel.model_name,
                    'provider': channel.provider,
                    'input_price': input_price,
                    'output_price': output_price,
                    'routing_score': channel_score.total_score,
                    'matched_model': channel_score.matched_model
                })
            
            # è·å–å½“å‰è·¯ç”±ç­–ç•¥
            current_strategy = getattr(self.router, 'current_strategy', 'balanced')
            
            # ç”Ÿæˆæ¨¡å‹æ¨è
            recommendations = model_optimizer.recommend_models(
                token_estimate, available_channels, current_strategy
            )
            
            # è®°å½•Tokené¢„ä¼°ç»“æœ
            complexity_name = token_estimate.task_complexity.value
            logger.info(f"ğŸ§  TOKEN ESTIMATE: [{request_id}] Input: {token_estimate.input_tokens}, "
                       f"Output: {token_estimate.estimated_output_tokens} (total: {token_estimate.total_tokens})")
            logger.info(f"ğŸ§  TASK COMPLEXITY: [{request_id}] {complexity_name.upper()} "
                       f"(confidence: {token_estimate.confidence:.1%})")
            
            # æ˜¾ç¤ºæ¨èç»“æœ
            if recommendations:
                best_rec = recommendations[0]
                logger.info(f"ğŸ¯ BEST RECOMMENDATION: [{request_id}] {best_rec.model_name} "
                           f"(${best_rec.estimated_cost:.6f}, {best_rec.estimated_time:.1f}s) - {best_rec.reason}")
                
                # æ˜¾ç¤ºå…è´¹é€‰é¡¹
                free_options = [r for r in recommendations[:5] if r.estimated_cost == 0]
                if free_options:
                    logger.info(f"ğŸ’° FREE OPTIONS: [{request_id}] Found {len(free_options)} free channels")
                    for free_rec in free_options[:3]:
                        logger.info(f"   â€¢ {free_rec.model_name} - {free_rec.reason}")
                
                # æˆæœ¬å¯¹æ¯”
                costs = [r.estimated_cost for r in recommendations[:5] if r.estimated_cost > 0]
                if len(costs) > 1:
                    savings = max(costs) - min(costs)
                    if savings > 0.001:  # åªæœ‰å½“èŠ‚çœè¶…è¿‡0.001ç¾å…ƒæ—¶æ‰æç¤º
                        logger.info(f"ğŸ’° COST SAVINGS: [{request_id}] Can save up to ${savings:.6f} by choosing optimal model")
            
            # æ„é€ è¿”å›æ•°æ®
            cost_preview = {
                'token_estimate': {
                    'input_tokens': token_estimate.input_tokens,
                    'estimated_output_tokens': token_estimate.estimated_output_tokens,
                    'total_tokens': token_estimate.total_tokens,
                    'confidence': token_estimate.confidence,
                    'task_complexity': complexity_name
                },
                'recommendations': [
                    {
                        'model_name': rec.model_name,
                        'channel_id': rec.channel_id,
                        'estimated_cost': rec.estimated_cost,
                        'estimated_time': rec.estimated_time,
                        'quality_score': rec.quality_score,
                        'reason': rec.reason,
                        'formatted_cost': f"${rec.estimated_cost:.6f}" if rec.estimated_cost > 0 else "Free"
                    }
                    for rec in recommendations[:10]  # è¿”å›å‰10ä¸ªæ¨è
                ],
                'optimization_strategy': current_strategy,
                'calculation_time_ms': 0  # Tokené¢„ä¼°å¾ˆå¿«ï¼ŒåŸºæœ¬æ— å»¶è¿Ÿ
            }
            
            return cost_preview
            
        except Exception as e:
            logger.warning(f"ğŸ§  TOKEN ESTIMATION FAILED: [{request_id}] {e}")
            return None
    
    def _get_model_pricing(self, model_name: str, provider: str) -> Optional[Dict[str, float]]:
        """è·å–æ¨¡å‹å®šä»·ä¿¡æ¯"""
        try:
            # å°è¯•ä»é™æ€å®šä»·åŠ è½½å™¨è·å–
            from ..utils.static_pricing import get_static_pricing_loader
            pricing_loader = get_static_pricing_loader()
            
            if provider == 'siliconflow':
                pricing_result = pricing_loader.get_siliconflow_pricing(model_name)
                if pricing_result:
                    return {
                        'input_price': pricing_result.input_price,
                        'output_price': pricing_result.output_price
                    }
            elif provider == 'doubao':
                pricing_result = pricing_loader.get_doubao_pricing(model_name)
                if pricing_result:
                    return {
                        'input_price': pricing_result.input_price,
                        'output_price': pricing_result.output_price
                    }
            
            return None
        except Exception as e:
            logger.warning(f"è·å–æ¨¡å‹å®šä»·å¤±è´¥ {model_name}: {e}")
            return None
    
    async def _perform_concurrent_channel_check(self, candidate_channels: List[RoutingScore]) -> None:
        """æ‰§è¡Œå¹¶å‘æ¸ é“å¯ç”¨æ€§æ£€æŸ¥"""
        logger.info(f"FAST CHECK: Pre-checking availability of top {min(3, len(candidate_channels))} channels")
        
        check_tasks = []
        for i, routing_score in enumerate(candidate_channels[:3]):
            channel = routing_score.channel
            provider = self.config.get_provider(channel.provider)
            if provider:
                channel_info = self._prepare_channel_request_info(channel, provider, None, routing_score.matched_model)
                check_tasks.append((i, channel, self._fast_channel_check(channel_info.url, channel_info.headers)))
        
        if check_tasks:
            check_results = await asyncio.gather(*[task[2] for task in check_tasks], return_exceptions=True)
            
            # æ‰¾å‡ºç¬¬ä¸€ä¸ªå¯ç”¨ä¸”ç¬¦åˆé—´éš”è¦æ±‚çš„æ¸ é“å¹¶ç§»åˆ°é¦–ä½
            interval_manager = get_request_interval_manager()
            for i, (original_index, channel, result) in enumerate(check_tasks):
                if i < len(check_results) and not isinstance(check_results[i], Exception):
                    is_available, status_code, message = check_results[i]
                    if is_available:
                        # æ£€æŸ¥é—´éš”é™åˆ¶
                        min_interval = getattr(channel, 'min_request_interval', 0)
                        logger.info(f"FAST CHECK DEBUG: Channel '{channel.name}' min_interval={min_interval} (id={channel.id}) type={type(channel)}")
                        if min_interval > 0:
                            is_ready = interval_manager.is_channel_ready(channel.id, min_interval)
                            if not is_ready:
                                wait_time = interval_manager.get_remaining_wait_time(channel.id, min_interval)
                                logger.info(f"ğŸ” FAST CHECK SKIP: Channel '{channel.name}' needs to wait {wait_time:.1f}s (min_interval={min_interval}s), not prioritizing")
                                continue
                        
                        logger.info(f"FAST CHECK: Channel '{channel.name}' is available (HTTP {status_code}), prioritizing")
                        priority_channel = candidate_channels.pop(original_index)
                        candidate_channels.insert(0, priority_channel)
                        break
                    else:
                        logger.info(f"FAST CHECK: Channel '{channel.name}' unavailable ({status_code}: {message})")
    
    async def _execute_request_with_retry(self, request: ChatCompletionRequest, routing_result: RoutingResult, start_time: float, request_id: str, cost_preview: Optional[Dict[str, Any]] = None) -> Union[JSONResponse, StreamingResponse]:
        """æ‰§è¡Œè¯·æ±‚å¹¶å¤„ç†é‡è¯•é€»è¾‘"""
        last_error = None
        failed_channels = set()  # æ™ºèƒ½æ¸ é“é»‘åå•
        
        for attempt_num, routing_score in enumerate(routing_result.candidates, 1):
            channel = routing_score.channel
            provider = self.config.get_provider(channel.provider)
            
            # æ£€æŸ¥æ¸ é“æ˜¯å¦å·²è¢«æ‹‰é»‘
            if channel.id in failed_channels:
                logger.info(f"âš« SKIP #{attempt_num}: Channel '{channel.name}' (ID: {channel.id}) is blacklisted due to previous failures")
                continue
            
            if not provider:
                logger.warning(f"ATTEMPT #{attempt_num}: Provider '{channel.provider}' for channel '{channel.name}' not found, skipping")
                continue
            
            # æ£€æŸ¥æ¸ é“é—´éš”é™åˆ¶ï¼Œå¦‚æœéœ€è¦ç­‰å¾…åˆ™è·³è¿‡
            interval_manager = get_request_interval_manager()
            min_interval = getattr(channel, 'min_request_interval', 0)
            logger.info(f"ğŸ” INTERVAL CHECK: [{request_id}] Channel '{channel.name}' min_interval={min_interval}s")
            if min_interval > 0:
                is_ready = interval_manager.is_channel_ready(channel.id, min_interval)
                logger.info(f"ğŸ” INTERVAL CHECK: [{request_id}] Channel '{channel.name}' is_ready={is_ready}")
                if not is_ready:
                    wait_time = interval_manager.get_remaining_wait_time(channel.id, min_interval)
                    logger.info(f"â­ï¸ SKIP #{attempt_num}: Channel '{channel.name}' needs to wait {wait_time:.1f}s (min_interval={min_interval}s), trying next channel")
                    continue
            
            logger.info(f"ATTEMPT #{attempt_num}: [{request_id}] Trying channel '{channel.name}' (ID: {channel.id}) with score {routing_score.total_score:.3f}")
            logger.info(f"ATTEMPT #{attempt_num}: [{request_id}] Score breakdown - {routing_score.reason}")
            
            # åœ¨å‘é€è¯·æ±‚å‰è®°å½•æ—¶é—´ï¼ˆç”¨äºé—´éš”æ§åˆ¶ï¼‰
            if min_interval > 0:
                interval_manager.record_request(channel.id)
                logger.info(f"ğŸ” INTERVAL RECORDED: [{request_id}] Channel '{channel.name}' request time recorded")
            
            try:
                channel_info = self._prepare_channel_request_info(channel, provider, request, routing_score.matched_model)
                
                logger.info(f"ğŸ“¡ FORWARDING: [{request_id}] Sending request to {channel_info.url}")
                logger.info(f"ğŸ“¡ FORWARDING: [{request_id}] Target model -> '{channel_info.request_data['model']}'")
                
                # åˆ›å»ºè¯·æ±‚å…ƒæ•°æ®
                aggregator = get_response_aggregator()
                metadata = aggregator.create_request_metadata(
                    request_id=request_id,
                    model_requested=request.model,
                    model_used=channel_info.request_data['model'],
                    channel_name=channel.name,
                    channel_id=channel.id,
                    provider=channel.provider,
                    attempt_count=attempt_num,
                    is_streaming=request.stream,
                    routing_strategy=getattr(self.router, 'current_strategy', 'balanced'),
                    routing_score=routing_score.total_score,
                    routing_reason=routing_score.reason,
                    cost_preview=cost_preview
                )
                
                # å­˜å‚¨å…ƒæ•°æ®åˆ°è¯·æ±‚ä¸­ï¼Œä¾›åç»­ä½¿ç”¨
                setattr(request, '_metadata', metadata)
                
                if request.stream:
                    return self._handle_streaming_request(request, channel_info, routing_score, attempt_num, metadata)
                else:
                    return await self._handle_regular_request(request, channel_info, routing_score, attempt_num, start_time, metadata)
                    
            except httpx.HTTPStatusError as e:
                last_error = e
                await self._handle_http_status_error(e, channel, attempt_num, failed_channels, routing_result.candidates)
                continue
            except httpx.RequestError as e:
                last_error = e
                self._handle_request_error(e, channel, attempt_num, routing_result.candidates)
                continue
        
        # æ‰€æœ‰æ¸ é“éƒ½å¤±è´¥äº†
        return self._create_all_channels_failed_error(request.model, routing_result.candidates, last_error, time.time() - start_time)
    
    def _handle_streaming_request(self, request: ChatCompletionRequest, channel_info: ChannelRequestInfo, routing_score: RoutingScore, attempt_num: int, metadata: RequestMetadata):
        """å¤„ç†æµå¼è¯·æ±‚"""
        logger.info(f"STREAMING: [{metadata.request_id}] Starting streaming response for channel '{channel_info.channel.name}'")
        
        # è¿”å›StreamingResponseå¯¹è±¡
        return StreamingResponse(
            self._stream_channel_api_with_summary(channel_info.url, channel_info.headers, channel_info.request_data, channel_info.channel.id, metadata),
            media_type="text/event-stream"
        )
    
    async def _handle_regular_request(self, request: ChatCompletionRequest, channel_info: ChannelRequestInfo, routing_score: RoutingScore, attempt_num: int, start_time: float, metadata: RequestMetadata) -> JSONResponse:
        """å¤„ç†å¸¸è§„è¯·æ±‚"""
        logger.info(f"â³ REQUEST: [{metadata.request_id}] Sending optimized request to channel '{channel_info.channel.name}'")
        
        response_json, ttfb = await self._call_channel_api(channel_info.url, channel_info.headers, channel_info.request_data)
        
        # æˆåŠŸï¼Œæ›´æ–°å¥åº·åº¦å¹¶è¿”å›
        latency = time.time() - start_time
        self.router.update_channel_health(channel_info.channel.id, True, latency)
        
        # ğŸš€ ä½¿ç”¨æ™ºèƒ½æ—¥å¿—è®°å½•æˆåŠŸå“åº” (AIRouteråŠŸèƒ½é›†æˆ)  
        log_channel_operation(
            enhanced_logger,
            operation="request",
            channel_id=channel_info.channel.id,
            model=response_json.get('model', 'unknown'),
            success=True,
            request_id=metadata.request_id,
            latency=latency,
            ttfb_ms=ttfb*1000,
            usage=response_json.get('usage', {})
        )
        
        # è®°å½•ä¼ ç»Ÿæ—¥å¿—ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        logger.info(f"SUCCESS: [{metadata.request_id}] Channel '{channel_info.channel.name}' responded successfully (latency: {latency:.3f}s)")
        logger.info(f"TIMING: [{metadata.request_id}] TTFB: {ttfb*1000:.1f}ms, Total: {latency*1000:.1f}ms")
        logger.info(f"RESPONSE: [{metadata.request_id}] Model used -> {response_json.get('model', 'unknown')}")
        logger.info(f"RESPONSE: [{metadata.request_id}] Usage -> {response_json.get('usage', {})}")
        
        # æ›´æ–°å…ƒæ•°æ®
        aggregator = get_response_aggregator()
        usage = response_json.get('usage', {})
        prompt_tokens = usage.get('prompt_tokens', 0)
        completion_tokens = usage.get('completion_tokens', 0)
        total_tokens = usage.get('total_tokens', prompt_tokens + completion_tokens)
        
        # è®¡ç®—æˆæœ¬ä¿¡æ¯
        cost_info = self._calculate_request_cost(channel_info.channel, prompt_tokens, completion_tokens, response_json.get('model'))
        
        # è·å–ç”¨æˆ·ä¼šè¯ä¿¡æ¯
        session_manager = get_session_manager()
        user_identifier = self._extract_user_identifier(request)
        session = session_manager.add_request(
            user_identifier=user_identifier,
            cost=cost_info['total_cost'],
            model=response_json.get('model', 'unknown'),
            channel=channel_info.channel.name
        )
        
        # æ›´æ–°èšåˆå™¨ (ä¼ é€’TTFBä¿¡æ¯)
        aggregator.update_tokens(metadata.request_id, prompt_tokens, completion_tokens, total_tokens)
        aggregator.update_cost(metadata.request_id, cost_info['total_cost'], session.total_cost, session.total_requests)
        aggregator.update_performance(metadata.request_id, ttfb=ttfb)  # TTFBå·²ç»æ˜¯ç§’ä¸ºå•ä½
        
        # å®Œæˆè¯·æ±‚å¹¶è·å–æœ€ç»ˆå…ƒæ•°æ®
        final_metadata = aggregator.finish_request(metadata.request_id)
        
        # è®°å½•ä½¿ç”¨æƒ…å†µåˆ°JSONLæ–‡ä»¶
        await self._record_usage_async(
            metadata.request_id,
            request,
            channel_info.channel,
            response_json.get('model', 'unknown'),
            prompt_tokens,
            completion_tokens,
            cost_info,
            latency,
            "success"
        )
        
        # ğŸš€ æ€ç»´é“¾æ¸…ç†å¤„ç† (AIRouteråŠŸèƒ½é›†æˆ)
        cleaned_response_json = self._clean_response_content(response_json)
        
        # ä½¿ç”¨æ–°çš„å“åº”æ±‡æ€»æ ¼å¼
        enhanced_response = aggregator.enhance_response_with_summary(cleaned_response_json, final_metadata)
        
        # è·å–æ±‡æ€»å¤´ä¿¡æ¯ï¼ˆè™½ç„¶éæµå¼ä¸»è¦åœ¨å“åº”ä½“ä¸­ï¼Œä½†ä¿ç•™å¤´ä¿¡æ¯ç”¨äºè°ƒè¯•ï¼‰
        debug_headers = aggregator.get_headers_summary(metadata.request_id)
        
        return JSONResponse(content=enhanced_response, headers=debug_headers)
    
    def _prepare_channel_request_info(self, channel, provider, request: Optional[ChatCompletionRequest], matched_model: Optional[str]) -> ChannelRequestInfo:
        """å‡†å¤‡æ¸ é“è¯·æ±‚ä¿¡æ¯"""
        base_url = (channel.base_url or provider.base_url).rstrip('/')
        if not base_url.endswith('/v1'):
            base_url += '/v1'
        url = f"{base_url}/chat/completions"

        headers = {"Content-Type": "application/json", "User-Agent": "smart-ai-router/0.2.0"}
        if provider.auth_type == "bearer":
            headers["Authorization"] = f"Bearer {channel.api_key}"
        elif provider.auth_type == "x-api-key":
            headers["x-api-key"] = channel.api_key

        request_data = {}
        if request:
            request_data = request.dict(exclude_unset=True)
            # æ™ºèƒ½æ¨¡å‹é€‰æ‹©é€»è¾‘
            if matched_model:
                request_data["model"] = matched_model
                logger.info(f"ğŸ“¡ MODEL SELECTION: Using matched model '{matched_model}' for routing")
            elif request.model.startswith("auto:") or request.model.startswith("tag:"):
                request_data["model"] = channel.model_name
                logger.info(f"ğŸ“¡ MODEL SELECTION: Using channel default model '{channel.model_name}' for virtual query")
            else:
                request_data["model"] = request.model
                logger.info(f"ğŸ“¡ MODEL SELECTION: Using requested model '{request.model}' for physical query")
        
        return ChannelRequestInfo(
            url=url,
            headers=headers,
            request_data=request_data,
            channel=channel,
            provider=provider,
            matched_model=matched_model
        )
    
    def _create_debug_headers(self, channel_info: ChannelRequestInfo, routing_score: RoutingScore, attempt_num: int, request_type: str, latency: Optional[float] = None) -> Dict[str, str]:
        """åˆ›å»ºè°ƒè¯•å¤´ä¿¡æ¯"""
        headers = {
            "X-Router-Channel": f"{channel_info.channel.name} (ID: {channel_info.channel.id})",
            "X-Router-Provider": getattr(channel_info.provider, 'name', channel_info.channel.provider),
            "X-Router-Model": routing_score.matched_model or channel_info.channel.model_name,
            "X-Router-Score": f"{routing_score.total_score:.3f}",
            "X-Router-Attempts": str(attempt_num),
            "X-Router-Score-Breakdown": f"cost:{routing_score.cost_score:.2f} speed:{routing_score.speed_score:.2f} quality:{routing_score.quality_score:.2f} reliability:{routing_score.reliability_score:.2f}",
            "X-Router-Type": request_type
        }
        
        if latency is not None:
            headers["X-Router-Latency"] = f"{latency:.3f}s"
        
        return headers
    
    def _invalidate_channel_cache(self, channel_id: str, channel_name: str, reason: str):
        """ç»Ÿä¸€çš„ç¼“å­˜å¤±æ•ˆæ–¹æ³•"""
        try:
            cache = get_request_cache()
            cache.invalidate_channel(channel_id)
            logger.info(f"ğŸ—‘ï¸  CACHE INVALIDATED: Cleared cached selections for channel '{channel_name}' due to {reason}")
        except Exception as e:
            logger.warning(f"CACHE INVALIDATION FAILED for channel '{channel_name}': {e}")

    async def _handle_http_status_error(self, error: httpx.HTTPStatusError, channel, attempt_num: int, failed_channels: set, total_candidates: List) -> None:
        """å¤„ç†HTTPçŠ¶æ€é”™è¯¯"""
        error_text = error.response.text if hasattr(error.response, 'text') else str(error)
        logger.warning(f"ATTEMPT #{attempt_num} FAILED: Channel '{channel.name}' returned HTTP {error.response.status_code}")
        logger.warning(f"ERROR DETAILS: {error_text[:200]}...")
        
        # è®°å½•æ¸ é“é”™è¯¯å¹¶å‘é€å‘Šè­¦
        check_api_error_and_alert(channel.id, channel.name, error.response.status_code, error_text)
        
        self.router.update_channel_health(channel.id, False)
        
        # æ™ºèƒ½æ‹‰é»‘ï¼šå¯¹äºè®¤è¯é”™è¯¯ç­‰æ°¸ä¹…æ€§é”™è¯¯ï¼Œæ‹‰é»‘æ•´ä¸ªæ¸ é“
        if error.response.status_code in [401, 403]:
            failed_channels.add(channel.id)
            logger.warning(f"ğŸš« CHANNEL BLACKLISTED: Channel '{channel.name}' (ID: {channel.id}) blacklisted due to HTTP {error.response.status_code}")
            logger.info(f"SKIP OPTIMIZATION: Will skip all remaining models from channel '{channel.name}'")
            
            # ä½¿ç›¸å…³ç¼“å­˜å¤±æ•ˆ - æ°¸ä¹…æ€§é”™è¯¯éœ€è¦ç«‹å³æ¸…é™¤ç¼“å­˜
            self._invalidate_channel_cache(channel.id, channel.name, "permanent error")
        
        # å¯¹äºä¸´æ—¶é”™è¯¯ï¼ˆå¦‚429, 500ï¼‰ï¼Œä¹Ÿä½¿ç¼“å­˜å¤±æ•ˆä½†ä¸æ°¸ä¹…æ‹‰é»‘ï¼Œå¹¶æ·»åŠ é€€é¿å»¶è¿Ÿ
        elif error.response.status_code in [429, 500, 502, 503, 504]:
            self._invalidate_channel_cache(channel.id, channel.name, "temporary error")
            
            # 429é”™è¯¯éœ€è¦ç‰¹åˆ«å¤„ç† - å®æ–½æ™ºèƒ½é€€é¿ç­–ç•¥
            if error.response.status_code == 429:
                failed_channels.add(channel.id)  # æš‚æ—¶æ‹‰é»‘ï¼Œé¿å…è¿ç»­é‡è¯•
                
                # å°è¯•ä»é”™è¯¯ä¿¡æ¯ä¸­æå–ç­‰å¾…æ—¶é—´
                wait_time = self._extract_rate_limit_wait_time(error_text)
                if wait_time:
                    backoff_time = min(wait_time, 60)  # æœ€å¤§ç­‰å¾…60ç§’
                    logger.warning(f"SMART RATE LIMIT: Channel '{channel.name}' suggests waiting {wait_time}s, applying {backoff_time}s backoff")
                else:
                    backoff_time = min(2 ** (attempt_num - 1), 16)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§16ç§’
                    logger.warning(f"RATE LIMIT: Channel '{channel.name}' rate limited, applying {backoff_time}s backoff")
                
                await asyncio.sleep(backoff_time)
        
        if attempt_num < len(total_candidates):
            logger.info(f"FAILOVER: Trying next channel (#{attempt_num + 1})")
    
    def _extract_rate_limit_wait_time(self, error_text: str) -> Optional[int]:
        """ä»é”™è¯¯ä¿¡æ¯ä¸­æå–å»ºè®®çš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰"""
        import re
        import json
        
        if not error_text:
            return None
        
        try:
            # å°è¯•è§£æJSONæ ¼å¼çš„é”™è¯¯ä¿¡æ¯
            if error_text.strip().startswith('{'):
                error_data = json.loads(error_text)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
                if isinstance(error_data, dict):
                    error_obj = error_data.get('error', {})
                    if isinstance(error_obj, dict):
                        message = error_obj.get('message', '')
                        code = error_obj.get('code')
                        
                        # å¦‚æœæ˜¯429é”™è¯¯ä¸”åŒ…å«rate limitç›¸å…³ä¿¡æ¯
                        if code == 429 or 'rate' in message.lower() or 'limit' in message.lower():
                            # ä»æ¶ˆæ¯ä¸­æå–ç­‰å¾…æ—¶é—´çš„å„ç§æ¨¡å¼
                            wait_patterns = [
                                r'retry after (\d+) seconds?',
                                r'retry in (\d+) seconds?', 
                                r'wait (\d+) seconds?',
                                r'try again in (\d+) seconds?',
                                r'retry shortly',  # é»˜è®¤ç­‰å¾…æ—¶é—´
                                r'è¯·(\d+)ç§’åé‡è¯•',
                                r'ç­‰å¾…(\d+)ç§’'
                            ]
                            
                            for pattern in wait_patterns:
                                match = re.search(pattern, message, re.IGNORECASE)
                                if match and match.groups():
                                    return int(match.group(1))
                                elif 'retry shortly' in message.lower():
                                    return 5  # é»˜è®¤çŸ­æš‚ç­‰å¾…5ç§’
            
            # ç›´æ¥ä»æ–‡æœ¬ä¸­æœç´¢ç­‰å¾…æ—¶é—´æ¨¡å¼
            text_patterns = [
                r'retry after (\d+) seconds?',
                r'retry in (\d+) seconds?',
                r'wait (\d+) seconds?', 
                r'try again in (\d+) seconds?',
                r'è¯·(\d+)ç§’åé‡è¯•',
                r'ç­‰å¾…(\d+)ç§’'
            ]
            
            for pattern in text_patterns:
                match = re.search(pattern, error_text, re.IGNORECASE)
                if match:
                    return int(match.group(1))
            
            # å¦‚æœåŒ…å«rate limitç›¸å…³å…³é”®è¯ä½†æ²¡æœ‰å…·ä½“æ—¶é—´ï¼Œè¿”å›é»˜è®¤ç­‰å¾…æ—¶é—´
            rate_limit_keywords = ['rate limit', 'rate-limit', 'too many requests', 'quota exceeded', 'temporarily rate-limited']
            if any(keyword in error_text.lower() for keyword in rate_limit_keywords):
                return 10  # é»˜è®¤ç­‰å¾…10ç§’
                
        except (json.JSONDecodeError, ValueError, AttributeError) as e:
            logger.debug(f"Failed to parse rate limit wait time from error: {e}")
        
        return None
    
    async def _record_usage_async(self, request_id: str, request: ChatCompletionRequest, 
                                  channel, model_used: str, input_tokens: int, output_tokens: int,
                                  cost_info: dict, response_time_ms: float, status: str = "success",
                                  error_message: str = None):
        """å¼‚æ­¥è®°å½•ä½¿ç”¨æƒ…å†µåˆ°JSONLæ–‡ä»¶"""
        try:
            tracker = get_usage_tracker()
            
            # åˆ›å»ºä½¿ç”¨è®°å½•
            usage_record = create_usage_record(
                model=model_used,
                channel_id=channel.id,
                channel_name=channel.name,
                provider=channel.provider,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                input_cost=cost_info.get('input_cost', 0.0),
                output_cost=cost_info.get('output_cost', 0.0),
                request_id=request_id,
                request_type="chat",
                status=status,
                error_message=error_message,
                response_time_ms=int(response_time_ms * 1000),  # è½¬æ¢ä¸ºæ¯«ç§’
                tags=self._extract_request_tags(request)
            )
            
            # å¼‚æ­¥è®°å½•
            await tracker.record_usage_async(usage_record)
            
        except Exception as e:
            logger.error(f"è®°å½•ä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
    
    def _extract_request_tags(self, request: ChatCompletionRequest) -> list:
        """ä»è¯·æ±‚ä¸­æå–æ ‡ç­¾ä¿¡æ¯"""
        tags = []
        
        # ä»æ¨¡å‹åç§°æå–æ ‡ç­¾
        if request.model.startswith('tag:'):
            tag_part = request.model[4:]  # å»æ‰ 'tag:' å‰ç¼€
            tags.extend(tag_part.split(','))
        
        # æ·»åŠ è¯·æ±‚ç‰¹å¾æ ‡ç­¾
        if request.stream:
            tags.append('streaming')
        if request.functions or request.tools:
            tags.append('function_calling')
        if request.max_tokens:
            if request.max_tokens <= 100:
                tags.append('short_response')
            elif request.max_tokens >= 2000:
                tags.append('long_response')
        
        return tags

    def _handle_request_error(self, error: httpx.RequestError, channel, attempt_num: int, total_candidates: List) -> None:
        """å¤„ç†è¯·æ±‚é”™è¯¯"""
        logger.warning(f"ATTEMPT #{attempt_num} FAILED: Channel '{channel.name}' network error: {str(error)}")
        self.router.update_channel_health(channel.id, False)
        
        # ç½‘ç»œé”™è¯¯ä¹Ÿå¯èƒ½æ˜¯æ¸ é“é—®é¢˜ï¼Œæ¸…é™¤ç›¸å…³ç¼“å­˜
        self._invalidate_channel_cache(channel.id, channel.name, "network error")
        
        if attempt_num < len(total_candidates):
            logger.info(f"FAILOVER: Trying next channel (#{attempt_num + 1})")
    
    def _infer_capabilities(self, request: ChatCompletionRequest) -> List[str]:
        """æ¨æ–­è¯·æ±‚æ‰€éœ€çš„èƒ½åŠ›"""
        capabilities = ["text"]
        if request.functions or request.tools:
            capabilities.append("function_calling")
        for message in request.messages:
            if isinstance(message.content, list):
                for item in message.content:
                    if isinstance(item, dict) and item.get("type") == "image_url":
                        capabilities.append("vision")
                        break
        return capabilities
    
    # --- Error Response Creators ---
    
    def _create_no_channels_error(self, model: str, execution_time: float) -> JSONResponse:
        """åˆ›å»ºæ— å¯ç”¨æ¸ é“é”™è¯¯å“åº”"""
        logger.error(f"ROUTING FAILED: No available channels found for model '{model}'")
        
        headers = {
            "X-Router-Status": "no-channels-found",
            "X-Router-Time": f"{execution_time:.3f}s",
            "X-Router-Model-Requested": model
        }
        
        return JSONResponse(
            status_code=503,
            content={"detail": f"No available channels found for model '{model}'."},
            headers=headers
        )
    
    def _handle_tag_not_found_error(self, error: TagNotFoundError, model: str, execution_time: float) -> JSONResponse:
        """å¤„ç†æ ‡ç­¾æœªæ‰¾åˆ°é”™è¯¯"""
        logger.error(f"TAG NOT FOUND: {error} (after {execution_time:.3f}s)")
        
        headers = {
            "X-Router-Status": "tag-not-found",
            "X-Router-Tags": ",".join(error.tags),
            "X-Router-Time": f"{execution_time:.3f}s",
            "X-Router-Model-Requested": model
        }
        
        return JSONResponse(
            status_code=404,
            content={"detail": str(error)},
            headers=headers
        )
    
    def _handle_unexpected_error(self, error: Exception, model: str, execution_time: float) -> HTTPException:
        """å¤„ç†æ„å¤–é”™è¯¯"""
        logger.error(f"ğŸ’¥ UNEXPECTED ERROR: Internal error occurred after {execution_time:.3f}s: {error}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An internal server error occurred: {error}")
    
    def _create_all_channels_failed_error(self, model: str, candidates: List, last_error: Optional[Exception], execution_time: float) -> JSONResponse:
        """åˆ›å»ºæ‰€æœ‰æ¸ é“å¤±è´¥é”™è¯¯å“åº”"""
        logger.error(f"ğŸ’¥ ALL CHANNELS FAILED: All {len(candidates)} channels failed for model '{model}' after {execution_time:.3f}s")
        
        error_detail = f"All available channels failed. Last error: {str(last_error)}"
        if hasattr(last_error, 'response'):
            error_detail += f" - Details: {last_error.response.text}"

        logger.error(f"ğŸ’¥ FINAL ERROR: {error_detail}")
        
        headers = {
            "X-Router-Status": "all-channels-failed",
            "X-Router-Attempts": str(len(candidates)),
            "X-Router-Time": f"{execution_time:.3f}s",
            "X-Router-Model-Requested": model
        }
        
        return JSONResponse(
            status_code=503,
            content={"detail": error_detail},
            headers=headers
        )
    
    # --- Low-level API calls (moved from main.py) ---
    
    async def _fast_channel_check(self, url: str, headers: dict) -> Tuple[bool, int, str]:
        """å¿«é€Ÿæ£€æŸ¥æ¸ é“æ˜¯å¦å¯ç”¨"""
        cache_key = f"{url}:{hash(str(headers))}"
        
        cached_result = await cache_get("channel_availability", cache_key)
        if cached_result:
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ¸ é“å¯ç”¨æ€§ç»“æœ: {url}")
            return cached_result['available'], cached_result['status_code'], cached_result['message']
        
        try:
            http_pool = get_http_pool()
            
            test_data = {
                "model": "test-availability",
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 1
            }
            
            async with http_pool.stream('POST', url, json=test_data, headers=headers) as response:
                if response.status_code in [200, 400, 404, 422]:
                    result = {'available': True, 'status_code': response.status_code, 'message': "OK"}
                    await cache_set("channel_availability", cache_key, result)
                    return True, response.status_code, "OK"
                else:
                    result = {'available': False, 'status_code': response.status_code, 'message': "Service error"}
                    await cache_set("channel_availability", cache_key, result)
                    return False, response.status_code, "Service error"
        except Exception as e:
            result = {'available': False, 'status_code': 0, 'message': str(e)}
            await cache_set("channel_availability", cache_key, result)
            return False, 0, str(e)
    
    async def _call_channel_api(self, url: str, headers: dict, request_data: dict):
        """ä¼˜åŒ–çš„APIè°ƒç”¨ - è¿”å›å“åº”å’ŒTTFBæ—¶é—´"""
        http_pool = get_http_pool()
        
        # è®°å½•å¼€å§‹æ—¶é—´
        request_start = time.time()
        
        async with http_pool.stream('POST', url, json=request_data, headers=headers) as response:
            # è®°å½•é¦–å­—èŠ‚æ—¶é—´ (TTFB)
            ttfb = time.time() - request_start
            
            if response.status_code != 200:
                # è¯»å–é”™è¯¯å†…å®¹ï¼Œé™åˆ¶å¤§å°ä»¥é¿å…å†…å­˜é—®é¢˜
                error_content = await response.aread()
                if len(error_content) > 1024:
                    error_content = error_content[:1024]
                response._content = error_content
                response.raise_for_status()
            
            content = await response.aread()
            result = response.json()
            
            # è¿”å›å“åº”å’ŒTTFBæ—¶é—´ (ä¸ä¿®æ”¹åŸå§‹å“åº”)
            return result, ttfb
    
    async def _stream_channel_api(self, url: str, headers: dict, request_data: dict, channel_id: str):
        """ä¼˜åŒ–çš„æµå¼APIè°ƒç”¨"""
        chunk_count = 0
        stream_start_time = time.time()
        ttfb = None
        first_token_received = False
        
        logger.info(f"STREAM START: Initiating optimized streaming request to channel '{channel_id}'")
        
        try:
            http_pool = get_http_pool()
            
            async with http_pool.stream("POST", url, json=request_data, headers=headers) as response:
                if response.status_code != 200:
                    # è¯»å–é”™è¯¯å†…å®¹ï¼Œé™åˆ¶å¤§å°ä»¥é¿å…å†…å­˜é—®é¢˜
                    error_body = await response.aread()
                    if len(error_body) > 1024:
                        error_body = error_body[:1024]
                    logger.error(f"STREAM ERROR: Channel '{channel_id}' returned status {response.status_code}")
                    
                    error_text = error_body.decode('utf-8', errors='ignore')[:200]
                    logger.error(f"STREAM ERROR DETAILS: {error_text}")
                    self.router.update_channel_health(channel_id, False)
                    
                    yield f"data: {json.dumps({'error': {'message': f'Upstream API error: {error_text}', 'code': response.status_code}})}\n\n"
                    return

                logger.info(f"STREAM CONNECTED: Successfully connected to channel '{channel_id}', starting optimized data flow")
                
                async for chunk in response.aiter_bytes(chunk_size=8192):
                    if chunk:
                        chunk_count += 1
                        
                        # è®°å½•é¦–æ¬¡æ¥æ”¶åˆ°æ•°æ®çš„æ—¶é—´ (TTFB)
                        if not first_token_received:
                            ttfb = time.time() - stream_start_time
                            first_token_received = True
                            logger.info(f"FIRST TOKEN: Received first token from channel '{channel_id}' in {ttfb:.3f}s")
                        
                        if chunk_count % 20 == 0:
                            logger.debug(f"STREAMING: Received {chunk_count} chunks from channel '{channel_id}'")
                    yield chunk
                        
            stream_duration = time.time() - stream_start_time
            self.router.update_channel_health(channel_id, True, stream_duration)
            logger.info(f"STREAM COMPLETE: Channel '{channel_id}' completed optimized streaming {chunk_count} chunks in {stream_duration:.3f}s")
            
        except httpx.HTTPStatusError as e:
            error_text = e.response.text if hasattr(e.response, 'text') else str(e)
            logger.error(f"STREAM FAILED: Channel '{channel_id}' HTTP error {e.response.status_code}: {error_text[:200]}...")
            self.router.update_channel_health(channel_id, False)
            yield f"data: {json.dumps({'error': {'message': f'Upstream API error: {error_text}', 'code': e.response.status_code}})}\n\n"
            
        except Exception as e:
            logger.error(f"STREAM EXCEPTION: Streaming request for channel '{channel_id}' failed: {e}", exc_info=True)
            self.router.update_channel_health(channel_id, False)
            yield f"data: {json.dumps({'error': {'message': str(e), 'code': 500}})}\n\n"

    def _clean_response_content(self, response_json: dict) -> dict:
        """
        æ¸…ç†å“åº”å†…å®¹ï¼Œç§»é™¤æ¨ç†æ¨¡å‹çš„æ€ç»´é“¾æ ‡ç­¾
        
        Args:
            response_json (dict): åŸå§‹APIå“åº”
            
        Returns:
            dict: æ¸…ç†åçš„å“åº”ï¼Œç§»é™¤æ€ç»´é“¾å†…å®¹
        """
        if not response_json or not isinstance(response_json, dict):
            return response_json
        
        try:
            # åˆ›å»ºå“åº”å‰¯æœ¬é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            cleaned_response = response_json.copy()
            
            # å¤„ç†choicesæ•°ç»„ä¸­çš„æ¶ˆæ¯å†…å®¹
            choices = cleaned_response.get('choices', [])
            if not choices:
                return cleaned_response
            
            cleaned_choices = []
            for choice in choices:
                cleaned_choice = choice.copy() if isinstance(choice, dict) else choice
                
                # å¤„ç†messageå†…å®¹
                if isinstance(cleaned_choice, dict) and 'message' in cleaned_choice:
                    message = cleaned_choice['message']
                    if isinstance(message, dict) and 'content' in message:
                        original_content = message.get('content', '')
                        if isinstance(original_content, str) and original_content:
                            # ğŸš€ åº”ç”¨æ€ç»´é“¾æ¸…ç† (AIRouteré›†æˆåŠŸèƒ½)
                            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ€ç»´é“¾æ¸…ç† (é»˜è®¤å¯ç”¨ä»¥æ”¯æŒæ¨ç†æ¨¡å‹)
                            should_clean_thinking = getattr(self.config_loader.config, 'clean_thinking_chains', True)
                            
                            cleaned_content = clean_model_response(
                                original_content, 
                                remove_thinking=should_clean_thinking, 
                                clean_sensitive=False,
                                max_length=None
                            )
                            
                            # æ›´æ–°æ¶ˆæ¯å†…å®¹
                            cleaned_message = message.copy()
                            cleaned_message['content'] = cleaned_content
                            cleaned_choice['message'] = cleaned_message
                            
                            # è®°å½•æ¸…ç†æ•ˆæœ (ä»…åœ¨å®é™…æ¸…ç†æ—¶è®°å½•)
                            if len(cleaned_content) < len(original_content):
                                reduction = len(original_content) - len(cleaned_content)
                                logger.info(f"ğŸ§¹ THINKING CHAINS CLEANED: Reduced content by {reduction} characters")
                
                # å¤„ç†deltaå†…å®¹ (æµå¼å“åº”)
                elif isinstance(cleaned_choice, dict) and 'delta' in cleaned_choice:
                    delta = cleaned_choice['delta']
                    if isinstance(delta, dict) and 'content' in delta:
                        original_content = delta.get('content', '')
                        if isinstance(original_content, str) and original_content:
                            # å¯¹äºæµå¼å“åº”ï¼Œåªè¿›è¡ŒåŸºç¡€çš„æ€ç»´é“¾æ¸…ç†
                            # é¿å…ç ´åæµå¼ä¼ è¾“çš„è¿ç»­æ€§
                            should_clean_thinking = getattr(self.config_loader.config, 'clean_thinking_chains', True)
                            
                            cleaned_content = clean_model_response(
                                original_content,
                                remove_thinking=should_clean_thinking,
                                clean_sensitive=False,
                                max_length=None
                            )
                            
                            cleaned_delta = delta.copy()
                            cleaned_delta['content'] = cleaned_content
                            cleaned_choice['delta'] = cleaned_delta
                
                cleaned_choices.append(cleaned_choice)
            
            # æ›´æ–°æ¸…ç†åçš„choices
            cleaned_response['choices'] = cleaned_choices
            
            return cleaned_response
            
        except Exception as e:
            # å¦‚æœæ¸…ç†è¿‡ç¨‹å‡ºç°å¼‚å¸¸ï¼Œè¿”å›åŸå§‹å“åº”å¹¶è®°å½•é”™è¯¯
            logger.warning(f"Response content cleaning failed: {e}, returning original response")
            return response_json

    def _add_cost_information(self, response_json: dict, channel, routing_score: RoutingScore) -> dict:
        """ä¸ºå“åº”æ·»åŠ å®æ—¶æˆæœ¬ä¿¡æ¯"""
        try:
            # è·å–usageä¿¡æ¯
            usage = response_json.get('usage', {})
            if not usage:
                logger.warning("No usage information in response, cannot calculate cost")
                return response_json
            
            prompt_tokens = usage.get('prompt_tokens', 0)
            completion_tokens = usage.get('completion_tokens', 0)
            total_tokens = usage.get('total_tokens', prompt_tokens + completion_tokens)
            
            # è®¡ç®—æˆæœ¬
            model_used = response_json.get('model', 'unknown')
            cost_info = self._calculate_request_cost(channel, prompt_tokens, completion_tokens, model_used)
            
            # è·å–æˆæœ¬è¿½è¸ªå™¨è¿›è¡Œè®°å½•
            cost_tracker = get_cost_tracker()
            cost_tracker.add_request_cost(
                cost=cost_info['total_cost'],
                model=response_json.get('model', 'unknown'),
                channel_id=channel.id,
                tokens={
                    'prompt_tokens': prompt_tokens,
                    'completion_tokens': completion_tokens,
                    'total_tokens': total_tokens
                }
            )
            
            # è·å–ä¼šè¯ç»Ÿè®¡
            session_summary = cost_tracker.get_session_summary()
            
            # åˆ›å»ºå¢å¼ºçš„å“åº”
            enhanced_response = response_json.copy()
            
            # æ·»åŠ æˆæœ¬ä¿¡æ¯åˆ°usageéƒ¨åˆ†
            enhanced_usage = usage.copy()
            enhanced_usage.update({
                'cost_breakdown': cost_info,
                'session_cost': session_summary.get('formatted_total_cost', '$0.00'),
                'session_requests': session_summary.get('total_requests', 0)
            })
            enhanced_response['usage'] = enhanced_usage
            
            # æ·»åŠ è·¯ç”±ä¿¡æ¯
            enhanced_response['smart_router'] = {
                'channel_used': channel.name,
                'channel_id': channel.id,
                'routing_score': round(routing_score.total_score, 3),
                'cost_score': round(routing_score.cost_score, 3),
                'speed_score': round(routing_score.speed_score, 3),
                'provider': channel.provider
            }
            
            logger.info(f"ğŸ’° COST: Request cost ${cost_info['total_cost']:.6f} | Session total: {session_summary.get('formatted_total_cost', '$0.00')}")
            
            return enhanced_response
            
        except Exception as e:
            logger.error(f"Failed to add cost information: {e}")
            return response_json

    def _calculate_request_cost(self, channel, prompt_tokens: int, completion_tokens: int, model_name: str = None) -> dict:
        """è®¡ç®—å•æ¬¡è¯·æ±‚çš„æˆæœ¬ - ä¼˜å…ˆä½¿ç”¨æ¨¡å‹çº§åˆ«å®šä»·ä¿¡æ¯"""
        input_cost_per_token = 0.0
        output_cost_per_token = 0.0
        
        # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨æ¨¡å‹çº§åˆ«çš„å®šä»·ä¿¡æ¯ï¼ˆä»ç¼“å­˜ä¸­è·å–ï¼‰
        if model_name:
            try:
                # å¯¼å…¥é…ç½®åŠ è½½å™¨æ¥è®¿é—®æ¨¡å‹ç¼“å­˜
                from core.yaml_config import YAMLConfigLoader
                config_loader = YAMLConfigLoader()
                
                model_cache = config_loader.get_model_cache()
                if channel.id in model_cache:
                    discovered_info = model_cache[channel.id]
                    if isinstance(discovered_info, dict) and 'models_data' in discovered_info:
                        models_data = discovered_info['models_data']
                        if model_name in models_data:
                            model_data = models_data[model_name]
                            if 'raw_data' in model_data and 'pricing' in model_data['raw_data']:
                                pricing = model_data['raw_data']['pricing']
                                prompt_cost = float(pricing.get('prompt', '0'))
                                completion_cost = float(pricing.get('completion', '0'))
                                if prompt_cost == 0.0 and completion_cost == 0.0:
                                    logger.debug(f"COST: Using model-level free pricing for '{model_name}' (prompt={prompt_cost}, completion={completion_cost})")
                                    input_cost_per_token = 0.0
                                    output_cost_per_token = 0.0
                                else:
                                    # æ¨¡å‹æœ‰æ˜ç¡®çš„æ”¶è´¹å®šä»·
                                    logger.debug(f"COST: Using model-level pricing for '{model_name}' (prompt={prompt_cost}, completion={completion_cost})")
                                    input_cost_per_token = prompt_cost
                                    output_cost_per_token = completion_cost
                            else:
                                logger.debug(f"COST: No model-level pricing found for '{model_name}', falling back to channel pricing")
                        else:
                            logger.debug(f"COST: Model '{model_name}' not found in cache for channel '{channel.id}'")
                    else:
                        logger.debug(f"COST: Invalid cache structure for channel '{channel.id}'")
            except Exception as e:
                logger.warning(f"COST: Error accessing model pricing for '{model_name}': {e}")
        
        # ğŸ”¥ å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¨¡å‹çº§åˆ«å®šä»·ï¼Œä½¿ç”¨æ¸ é“çº§åˆ«é…ç½®
        if input_cost_per_token == 0.0 and output_cost_per_token == 0.0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ˜ç¡®çš„å…è´¹æ¨¡å‹ï¼ˆ:free åç¼€ï¼‰
            if model_name and (":free" in model_name.lower() or "-free" in model_name.lower()):
                logger.debug(f"COST: Model '{model_name}' has explicit :free suffix, using zero cost")
                input_cost_per_token = 0.0
                output_cost_per_token = 0.0
            # ä½¿ç”¨æ¸ é“çº§åˆ«å®šä»·
            elif hasattr(channel, 'cost_per_token') and channel.cost_per_token:
                input_cost_per_token = channel.cost_per_token.get('input', 0.0)
                output_cost_per_token = channel.cost_per_token.get('output', 0.0)
                logger.debug(f"COST: Using channel cost_per_token (input={input_cost_per_token}, output={output_cost_per_token})")
            # å›é€€åˆ°pricingé…ç½®
            elif hasattr(channel, 'pricing') and channel.pricing:
                input_cost_per_token = channel.pricing.get('input_cost_per_1k', 0.001) / 1000
                output_cost_per_token = channel.pricing.get('output_cost_per_1k', 0.002) / 1000
                logger.debug(f"COST: Using channel pricing (input={input_cost_per_token}, output={output_cost_per_token})")
            else:
                # é»˜è®¤ä¼°ç®—å€¼
                input_cost_per_token = 0.0000005  # $0.0005 per 1K tokens
                output_cost_per_token = 0.0000015  # $0.0015 per 1K tokens
                logger.debug(f"COST: Using default pricing (input={input_cost_per_token}, output={output_cost_per_token})")
        
        input_cost = prompt_tokens * input_cost_per_token
        output_cost = completion_tokens * output_cost_per_token
        total_cost = input_cost + output_cost
        
        return {
            'input_tokens': prompt_tokens,
            'output_tokens': completion_tokens,
            'input_cost_per_token': input_cost_per_token,
            'output_cost_per_token': output_cost_per_token,
            'input_cost': round(input_cost, 8),
            'output_cost': round(output_cost, 8),
            'total_cost': round(total_cost, 8),
            'currency': 'USD'
        }
    
    async def _stream_channel_api_with_summary(self, url: str, headers: dict, request_data: dict, channel_id: str, metadata: RequestMetadata):
        """ä¼˜åŒ–çš„æµå¼APIè°ƒç”¨ï¼Œåœ¨ç»“æŸæ—¶æ·»åŠ æ±‡æ€»ä¿¡æ¯"""
        chunk_count = 0
        stream_start_time = time.time()
        aggregator = get_response_aggregator()
        
        logger.info(f"STREAM START: [{metadata.request_id}] Initiating optimized streaming request to channel '{channel_id}'")
        
        try:
            http_pool = get_http_pool()
            
            async with http_pool.stream("POST", url, json=request_data, headers=headers) as response:
                if response.status_code != 200:
                    # è¯»å–é”™è¯¯å†…å®¹ï¼Œé™åˆ¶å¤§å°ä»¥é¿å…å†…å­˜é—®é¢˜
                    error_body = await response.aread()
                    if len(error_body) > 1024:
                        error_body = error_body[:1024]
                    logger.error(f"STREAM ERROR: [{metadata.request_id}] Channel '{channel_id}' returned status {response.status_code}")
                    
                    error_text = error_body.decode('utf-8', errors='ignore')
                    logger.error(f"STREAM ERROR DETAILS: [{metadata.request_id}] {error_text[:200]}")
                    self.router.update_channel_health(channel_id, False)
                    
                    # æ£€æµ‹æµå¼å“åº”ä¸­çš„é€Ÿç‡é™åˆ¶é”™è¯¯
                    if response.status_code == 429:
                        wait_time = self._extract_rate_limit_wait_time(error_text)
                        if wait_time:
                            logger.warning(f"STREAM RATE LIMIT: [{metadata.request_id}] Channel '{channel_id}' suggests waiting {wait_time}s")
                            # åœ¨é”™è¯¯å“åº”ä¸­åŒ…å«ç­‰å¾…æ—¶é—´ä¿¡æ¯
                            yield f"data: {json.dumps({'error': {'message': f'Rate limited: retry after {wait_time}s - {error_text[:100]}', 'code': response.status_code, 'retry_after': wait_time}})}\\n\\n"
                        else:
                            yield f"data: {json.dumps({'error': {'message': f'Rate limited - {error_text[:100]}', 'code': response.status_code}})}\\n\\n"
                    else:
                        yield f"data: {json.dumps({'error': {'message': f'Upstream API error: {error_text[:100]}', 'code': response.status_code}})}\\n\\n"
                    
                    # è®¾ç½®é”™è¯¯ä¿¡æ¯å¹¶å®Œæˆè¯·æ±‚
                    aggregator.set_error(metadata.request_id, str(response.status_code), error_text[:200])
                    final_metadata = aggregator.finish_request(metadata.request_id)
                    
                    # å‘é€é”™è¯¯æƒ…å†µä¸‹çš„æ±‡æ€»ä¿¡æ¯
                    yield aggregator.create_sse_summary_event(final_metadata)
                    yield "data: [DONE]\\n\\n"
                    return

                logger.info(f"STREAM CONNECTED: [{metadata.request_id}] Successfully connected to channel '{channel_id}', starting optimized data flow")
                
                # è®°å½•TTFBæ—¶é—´ - ä¿®æ­£ï¼šè¿æ¥å»ºç«‹åçš„ç¬¬ä¸€ä¸ªæ•°æ®å—æ—¶é—´
                ttfb_recorded = False
                first_data_time = None
                
                # è®°å½•tokenä¿¡æ¯çš„å˜é‡
                total_prompt_tokens = 0
                total_completion_tokens = 0
                total_tokens = 0
                
                async for chunk in response.aiter_bytes(chunk_size=8192):
                    if chunk:
                        chunk_count += 1
                        
                        # è®°å½•ç¬¬ä¸€ä¸ªæ•°æ®å—çš„æ—¶é—´ä½œä¸ºTTFBï¼ˆçœŸæ­£çš„é¦–å­—èŠ‚æ—¶é—´ï¼‰
                        if not ttfb_recorded:
                            first_data_time = time.time()
                            ttfb = first_data_time - stream_start_time
                            aggregator.update_performance(metadata.request_id, ttfb=ttfb)
                            ttfb_recorded = True
                            logger.info(f"TTFB: [{metadata.request_id}] First data received in {ttfb:.3f}s")
                        
                        # è§£æå“åº”ä»¥æå–tokenä¿¡æ¯å¹¶æ£€æµ‹æµå¼é”™è¯¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
                        try:
                            chunk_str = chunk.decode('utf-8', errors='ignore')
                            
                            # æ£€æµ‹æµå¼æ•°æ®ä¸­çš„é€Ÿç‡é™åˆ¶é”™è¯¯
                            if 'data: ' in chunk_str and '"error"' in chunk_str:
                                lines = chunk_str.split('\\n')
                                for line in lines:
                                    if line.startswith('data: ') and line != 'data: [DONE]':
                                        try:
                                            data = json.loads(line[6:])  # å»æ‰ 'data: '
                                            if 'error' in data:
                                                error_obj = data['error']
                                                error_code = error_obj.get('code')
                                                error_message = error_obj.get('message', '')
                                                
                                                # æ£€æµ‹é€Ÿç‡é™åˆ¶é”™è¯¯
                                                if error_code == 429 or 'rate limit' in error_message.lower() or 'temporarily rate-limited' in error_message.lower():
                                                    wait_time = self._extract_rate_limit_wait_time(error_message)
                                                    if wait_time:
                                                        logger.warning(f"CONTENT RATE LIMIT: [{metadata.request_id}] Channel '{channel_id}' content suggests waiting {wait_time}s")
                                                        # ä¿®æ”¹é”™è¯¯ä¿¡æ¯ä»¥åŒ…å«ç­‰å¾…æ—¶é—´
                                                        error_obj['retry_after'] = wait_time
                                                        data['error'] = error_obj
                                                        
                                                        # é‡æ–°æ„é€ ä¿®æ”¹åçš„chunk
                                                        modified_line = f"data: {json.dumps(data)}"
                                                        chunk_str = chunk_str.replace(line, modified_line)
                                                        chunk = chunk_str.encode('utf-8')
                                                    else:
                                                        logger.warning(f"CONTENT RATE LIMIT: [{metadata.request_id}] Channel '{channel_id}' rate limited in streaming content")
                                        except (json.JSONDecodeError, KeyError):
                                            pass
                            
                            # æå–tokenä½¿ç”¨ä¿¡æ¯
                            if 'data: ' in chunk_str and '"usage"' in chunk_str:
                                # å°è¯•æå–æœ€åusageä¿¡æ¯
                                lines = chunk_str.split('\\n')
                                for line in lines:
                                    if line.startswith('data: ') and line != 'data: [DONE]':
                                        try:
                                            data = json.loads(line[6:])  # å»æ‰ 'data: '
                                            if 'usage' in data:
                                                usage = data['usage']
                                                total_prompt_tokens = usage.get('prompt_tokens', total_prompt_tokens)
                                                total_completion_tokens = usage.get('completion_tokens', total_completion_tokens) 
                                                total_tokens = usage.get('total_tokens', total_tokens)
                                        except (json.JSONDecodeError, KeyError):
                                            pass
                        except Exception:
                            pass  # å¿½ç•¥è§£æé”™è¯¯
                        
                        if chunk_count % 20 == 0:
                            logger.debug(f"STREAMING: [{metadata.request_id}] Received {chunk_count} chunks from channel '{channel_id}'")
                    
                    yield chunk
                
                # æ›´æ–°tokenå’Œæˆæœ¬ä¿¡æ¯
                if total_tokens > 0:
                    aggregator.update_tokens(metadata.request_id, total_prompt_tokens, total_completion_tokens, total_tokens)
                    # è®¡ç®—æˆæœ¬ï¼ˆéœ€è¦ä¼°ç®—æ¨¡å‹åï¼‰
                    cost_info = self._calculate_request_cost(None, total_prompt_tokens, total_completion_tokens, metadata.model_used)
                    
                    # è·å–ç”¨æˆ·ä¼šè¯ä¿¡æ¯ (æµå¼è¯·æ±‚éœ€è¦ä»å…ƒæ•°æ®ä¸­è·å–åŸå§‹è¯·æ±‚)
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä¼ é€’åŸå§‹è¯·æ±‚å¯¹è±¡
                    session_manager = get_session_manager()
                    user_identifier = session_manager.create_user_identifier("streaming-user", "streaming-client")
                    session = session_manager.add_request(
                        user_identifier=user_identifier,
                        cost=cost_info['total_cost'],
                        model=metadata.model_used,
                        channel=metadata.channel_name
                    )
                    
                    aggregator.update_cost(metadata.request_id, cost_info['total_cost'], session.total_cost, session.total_requests)
                    
                    # æµå¼å“åº”ä¸“ç”¨ï¼šè®¡ç®—å‡†ç¡®çš„tokenç”Ÿæˆæ—¶é—´
                    if first_data_time and total_completion_tokens > 0:
                        generation_time = time.time() - first_data_time  # ä»ç¬¬ä¸€ä¸ªtokenåˆ°æœ€åä¸€ä¸ªtokençš„æ—¶é—´
                        if generation_time > 0:
                            tokens_per_second = total_completion_tokens / generation_time
                            # æ›´æ–°å‡†ç¡®çš„tokenç”Ÿæˆé€Ÿåº¦
                            aggregator.update_performance(metadata.request_id, tokens_per_second=tokens_per_second)
                            logger.info(f"TOKEN SPEED: [{metadata.request_id}] {total_completion_tokens} tokens in {generation_time:.3f}s = {tokens_per_second:.1f} tokens/sec")
                
                # å®Œæˆè¯·æ±‚å¹¶ç”Ÿæˆæ±‡æ€»
                final_metadata = aggregator.finish_request(metadata.request_id)
                        
            stream_duration = time.time() - stream_start_time
            self.router.update_channel_health(channel_id, True, stream_duration)
            logger.info(f"STREAM COMPLETE: [{metadata.request_id}] Channel '{channel_id}' completed optimized streaming {chunk_count} chunks in {stream_duration:.3f}s")
            
            # åœ¨[DONE]ä¹‹å‰å‘é€æ±‡æ€»ä¿¡æ¯
            yield aggregator.create_sse_summary_event(final_metadata)
            yield "data: [DONE]\\n\\n"
            
        except httpx.HTTPStatusError as e:
            error_text = e.response.text if hasattr(e.response, 'text') else str(e)
            logger.error(f"STREAM FAILED: [{metadata.request_id}] Channel '{channel_id}' HTTP error {e.response.status_code}: {error_text[:200]}...")
            self.router.update_channel_health(channel_id, False)
            
            # è®¾ç½®é”™è¯¯ä¿¡æ¯å¹¶å®Œæˆè¯·æ±‚
            aggregator.set_error(metadata.request_id, str(e.response.status_code), error_text)
            final_metadata = aggregator.finish_request(metadata.request_id)
            
            yield f"data: {json.dumps({'error': {'message': f'Upstream API error: {error_text}', 'code': e.response.status_code}})}\\n\\n"
            yield aggregator.create_sse_summary_event(final_metadata)
            yield "data: [DONE]\\n\\n"
            
        except Exception as e:
            logger.error(f"STREAM EXCEPTION: [{metadata.request_id}] Streaming request for channel '{channel_id}' failed: {e}", exc_info=True)
            self.router.update_channel_health(channel_id, False)
            
            # è®¾ç½®é”™è¯¯ä¿¡æ¯å¹¶å®Œæˆè¯·æ±‚
            aggregator.set_error(metadata.request_id, "500", str(e))
            final_metadata = aggregator.finish_request(metadata.request_id)
            
            yield f"data: {json.dumps({'error': {'message': str(e), 'code': 500}})}\\n\\n"
            yield aggregator.create_sse_summary_event(final_metadata)
            yield "data: [DONE]\\n\\n"
    
    def _extract_user_identifier(self, request) -> str:
        """ä»è¯·æ±‚ä¸­æå–ç”¨æˆ·æ ‡è¯†ç¬¦"""
        # è¿™é‡Œå¯ä»¥ä»è¯·æ±‚å¤´æˆ–å…¶ä»–åœ°æ–¹æå–API keyå’ŒUser-Agent
        # æš‚æ—¶ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥ä»FastAPIçš„Requestå¯¹è±¡ä¸­è·å–
        api_key = getattr(request, 'api_key', None) or "anonymous"
        user_agent = getattr(request, 'user_agent', None) or "unknown-client"
        
        session_manager = get_session_manager()
        return session_manager.create_user_identifier(api_key, user_agent)