# -*- coding: utf-8 -*-
"""
Chat completion request handler with improved architecture
"""
import time
import json
import asyncio
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass
import logging

import httpx
from fastapi import HTTPException
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel

from ..json_router import JSONRouter, RoutingRequest, TagNotFoundError, RoutingScore
from ..yaml_config import YAMLConfigLoader
from ..utils.http_client_pool import get_http_pool
from ..utils.smart_cache import cache_get, cache_set
from ..utils.token_counter import get_cost_tracker
from ..utils.request_cache import get_request_cache

logger = logging.getLogger(__name__)

# --- Request/Response Models ---

class ChatMessage(BaseModel):
    role: str
    content: Union[str, List[Dict[str, Any]]]

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: bool = False
    top_p: Optional[float] = 1.0
    frequency_penalty: Optional[float] = 0.0
    presence_penalty: Optional[float] = 0.0
    functions: Optional[List[Dict[str, Any]]] = None
    function_call: Optional[Union[str, Dict[str, str]]] = None
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[Union[str, Dict[str, Any]]] = None

@dataclass
class RoutingResult:
    """è·¯ç”±ç»“æœåŒ…è£…"""
    candidates: List[RoutingScore]
    execution_time: float
    total_candidates: int
    
@dataclass 
class ChannelRequestInfo:
    """æ¸ é“è¯·æ±‚ä¿¡æ¯"""
    url: str
    headers: Dict[str, str]
    request_data: Dict[str, Any]
    channel: Any
    provider: Any
    matched_model: Optional[str]

# --- Core Handler Class ---

class ChatCompletionHandler:
    """èŠå¤©å®Œæˆè¯·æ±‚å¤„ç†å™¨"""
    
    def __init__(self, config_loader: YAMLConfigLoader, router: JSONRouter):
        self.config = config_loader
        self.router = router
        
    async def handle_request(self, request: ChatCompletionRequest) -> Union[JSONResponse, StreamingResponse]:
        """å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚çš„ä¸»å…¥å£"""
        start_time = time.time()
        
        logger.info(f"ğŸŒ API REQUEST: Received chat completion request for model '{request.model}' (stream: {request.stream})")
        logger.info(f"ğŸŒ REQUEST DETAILS: {len(request.messages)} messages, max_tokens: {request.max_tokens}, temperature: {request.temperature}")
        
        try:
            # æ­¥éª¤1: è·¯ç”±è¯·æ±‚è·å–å€™é€‰æ¸ é“
            routing_result = await self._route_request_with_fallback(request, start_time)
            if not routing_result.candidates:
                return self._create_no_channels_error(request.model, time.time() - start_time)
            
            # æ­¥éª¤2: æ‰§è¡Œè¯·æ±‚å¹¶å¤„ç†é‡è¯•
            return await self._execute_request_with_retry(request, routing_result, start_time)
            
        except TagNotFoundError as e:
            return self._handle_tag_not_found_error(e, request.model, time.time() - start_time)
        except HTTPException:
            raise
        except Exception as e:
            return self._handle_unexpected_error(e, request.model, time.time() - start_time)
    
    async def _route_request_with_fallback(self, request: ChatCompletionRequest, start_time: float) -> RoutingResult:
        """æ‰§è¡Œè·¯ç”±è¯·æ±‚å’Œæ™ºèƒ½é¢„æ£€"""
        routing_request = RoutingRequest(
            model=request.model,
            messages=[msg.dict() for msg in request.messages],
            stream=request.stream,
            required_capabilities=self._infer_capabilities(request)
        )
        
        logger.info(f"ğŸ”„ CHANNEL ROUTING: Starting routing process for model '{request.model}'")
        candidate_channels = await self.router.route_request(routing_request)
        
        if not candidate_channels:
            return RoutingResult(candidates=[], execution_time=time.time() - start_time, total_candidates=0)
        
        logger.info(f"ğŸ”„ CHANNEL SELECTION: Processing {len(candidate_channels)} channels with intelligent routing")
        
        # æ™ºèƒ½æ¸ é“é¢„æ£€ - å¦‚æœæœ‰å¤šä¸ªæ¸ é“ï¼Œå…ˆå¹¶å‘æ£€æŸ¥å‰3ä¸ª
        if len(candidate_channels) > 1:
            await self._perform_concurrent_channel_check(candidate_channels)
        
        return RoutingResult(
            candidates=candidate_channels,
            execution_time=time.time() - start_time,
            total_candidates=len(candidate_channels)
        )
    
    async def _perform_concurrent_channel_check(self, candidate_channels: List[RoutingScore]) -> None:
        """æ‰§è¡Œå¹¶å‘æ¸ é“å¯ç”¨æ€§æ£€æŸ¥"""
        logger.info(f"âš¡ FAST CHECK: Pre-checking availability of top {min(3, len(candidate_channels))} channels")
        
        check_tasks = []
        for i, routing_score in enumerate(candidate_channels[:3]):
            channel = routing_score.channel
            provider = self.config.get_provider(channel.provider)
            if provider:
                channel_info = self._prepare_channel_request_info(channel, provider, None, routing_score.matched_model)
                check_tasks.append((i, channel, self._fast_channel_check(channel_info.url, channel_info.headers)))
        
        if check_tasks:
            check_results = await asyncio.gather(*[task[2] for task in check_tasks], return_exceptions=True)
            
            # æ‰¾å‡ºç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¸ é“å¹¶ç§»åˆ°é¦–ä½
            for i, (original_index, channel, result) in enumerate(check_tasks):
                if i < len(check_results) and not isinstance(check_results[i], Exception):
                    is_available, status_code, message = check_results[i]
                    if is_available:
                        logger.info(f"âš¡ FAST CHECK: Channel '{channel.name}' is available (HTTP {status_code}), prioritizing")
                        priority_channel = candidate_channels.pop(original_index)
                        candidate_channels.insert(0, priority_channel)
                        break
                    else:
                        logger.info(f"âš¡ FAST CHECK: Channel '{channel.name}' unavailable ({status_code}: {message})")
    
    async def _execute_request_with_retry(self, request: ChatCompletionRequest, routing_result: RoutingResult, start_time: float) -> Union[JSONResponse, StreamingResponse]:
        """æ‰§è¡Œè¯·æ±‚å¹¶å¤„ç†é‡è¯•é€»è¾‘"""
        last_error = None
        failed_channels = set()  # æ™ºèƒ½æ¸ é“é»‘åå•
        
        for attempt_num, routing_score in enumerate(routing_result.candidates, 1):
            channel = routing_score.channel
            provider = self.config.get_provider(channel.provider)
            
            # æ£€æŸ¥æ¸ é“æ˜¯å¦å·²è¢«æ‹‰é»‘
            if channel.id in failed_channels:
                logger.info(f"âš« SKIP #{attempt_num}: Channel '{channel.name}' (ID: {channel.id}) is blacklisted due to previous failures")
                continue
            
            if not provider:
                logger.warning(f"âŒ ATTEMPT #{attempt_num}: Provider '{channel.provider}' for channel '{channel.name}' not found, skipping")
                continue
            
            logger.info(f"ğŸš€ ATTEMPT #{attempt_num}: Trying channel '{channel.name}' (ID: {channel.id}) with score {routing_score.total_score:.3f}")
            logger.info(f"ğŸš€ ATTEMPT #{attempt_num}: Score breakdown - {routing_score.reason}")
            
            try:
                channel_info = self._prepare_channel_request_info(channel, provider, request, routing_score.matched_model)
                
                logger.info(f"ğŸ“¡ FORWARDING: Sending request to {channel_info.url}")
                logger.info(f"ğŸ“¡ FORWARDING: Target model -> '{channel_info.request_data['model']}'")
                
                if request.stream:
                    return await self._handle_streaming_request(request, channel_info, routing_score, attempt_num)
                else:
                    return await self._handle_regular_request(request, channel_info, routing_score, attempt_num, start_time)
                    
            except httpx.HTTPStatusError as e:
                last_error = e
                self._handle_http_status_error(e, channel, attempt_num, failed_channels, routing_result.candidates)
                continue
            except httpx.RequestError as e:
                last_error = e
                self._handle_request_error(e, channel, attempt_num, routing_result.candidates)
                continue
        
        # æ‰€æœ‰æ¸ é“éƒ½å¤±è´¥äº†
        return self._create_all_channels_failed_error(request.model, routing_result.candidates, last_error, time.time() - start_time)
    
    async def _handle_streaming_request(self, request: ChatCompletionRequest, channel_info: ChannelRequestInfo, routing_score: RoutingScore, attempt_num: int) -> StreamingResponse:
        """å¤„ç†æµå¼è¯·æ±‚"""
        logger.info(f"ğŸŒŠ STREAMING: Starting streaming response for channel '{channel_info.channel.name}'")
        
        debug_headers = self._create_debug_headers(channel_info, routing_score, attempt_num, "streaming")
        
        return StreamingResponse(
            self._stream_channel_api(channel_info.url, channel_info.headers, channel_info.request_data, channel_info.channel.id),
            media_type="text/event-stream",
            headers=debug_headers
        )
    
    async def _handle_regular_request(self, request: ChatCompletionRequest, channel_info: ChannelRequestInfo, routing_score: RoutingScore, attempt_num: int, start_time: float) -> JSONResponse:
        """å¤„ç†å¸¸è§„è¯·æ±‚"""
        logger.info(f"â³ REQUEST: Sending optimized request to channel '{channel_info.channel.name}'")
        
        response_json = await self._call_channel_api(channel_info.url, channel_info.headers, channel_info.request_data)
        
        # æˆåŠŸï¼Œæ›´æ–°å¥åº·åº¦å¹¶è¿”å›
        latency = time.time() - start_time
        self.router.update_channel_health(channel_info.channel.id, True, latency)
        
        logger.info(f"âœ… SUCCESS: Channel '{channel_info.channel.name}' responded successfully (latency: {latency:.3f}s)")
        logger.info(f"âœ… RESPONSE: Model used -> {response_json.get('model', 'unknown')}")
        logger.info(f"âœ… RESPONSE: Usage -> {response_json.get('usage', {})}")
        
        # è®¡ç®—å¹¶æ·»åŠ æˆæœ¬ä¿¡æ¯
        enhanced_response = self._add_cost_information(response_json, channel_info.channel, routing_score)
        
        debug_headers = self._create_debug_headers(channel_info, routing_score, attempt_num, "regular", latency)
        
        return JSONResponse(content=enhanced_response, headers=debug_headers)
    
    def _prepare_channel_request_info(self, channel, provider, request: Optional[ChatCompletionRequest], matched_model: Optional[str]) -> ChannelRequestInfo:
        """å‡†å¤‡æ¸ é“è¯·æ±‚ä¿¡æ¯"""
        base_url = (channel.base_url or provider.base_url).rstrip('/')
        if not base_url.endswith('/v1'):
            base_url += '/v1'
        url = f"{base_url}/chat/completions"

        headers = {"Content-Type": "application/json", "User-Agent": "smart-ai-router/0.2.0"}
        if provider.auth_type == "bearer":
            headers["Authorization"] = f"Bearer {channel.api_key}"
        elif provider.auth_type == "x-api-key":
            headers["x-api-key"] = channel.api_key

        request_data = {}
        if request:
            request_data = request.dict(exclude_unset=True)
            # æ™ºèƒ½æ¨¡å‹é€‰æ‹©é€»è¾‘
            if matched_model:
                request_data["model"] = matched_model
                logger.info(f"ğŸ“¡ MODEL SELECTION: Using matched model '{matched_model}' for routing")
            elif request.model.startswith("auto:") or request.model.startswith("tag:"):
                request_data["model"] = channel.model_name
                logger.info(f"ğŸ“¡ MODEL SELECTION: Using channel default model '{channel.model_name}' for virtual query")
            else:
                request_data["model"] = request.model
                logger.info(f"ğŸ“¡ MODEL SELECTION: Using requested model '{request.model}' for physical query")
        
        return ChannelRequestInfo(
            url=url,
            headers=headers,
            request_data=request_data,
            channel=channel,
            provider=provider,
            matched_model=matched_model
        )
    
    def _create_debug_headers(self, channel_info: ChannelRequestInfo, routing_score: RoutingScore, attempt_num: int, request_type: str, latency: Optional[float] = None) -> Dict[str, str]:
        """åˆ›å»ºè°ƒè¯•å¤´ä¿¡æ¯"""
        headers = {
            "X-Router-Channel": f"{channel_info.channel.name} (ID: {channel_info.channel.id})",
            "X-Router-Provider": getattr(channel_info.provider, 'name', channel_info.channel.provider),
            "X-Router-Model": routing_score.matched_model or channel_info.channel.model_name,
            "X-Router-Score": f"{routing_score.total_score:.3f}",
            "X-Router-Attempts": str(attempt_num),
            "X-Router-Score-Breakdown": f"cost:{routing_score.cost_score:.2f} speed:{routing_score.speed_score:.2f} quality:{routing_score.quality_score:.2f} reliability:{routing_score.reliability_score:.2f}",
            "X-Router-Type": request_type
        }
        
        if latency is not None:
            headers["X-Router-Latency"] = f"{latency:.3f}s"
        
        return headers
    
    def _invalidate_channel_cache(self, channel_id: str, channel_name: str, reason: str):
        """ç»Ÿä¸€çš„ç¼“å­˜å¤±æ•ˆæ–¹æ³•"""
        try:
            cache = get_request_cache()
            cache.invalidate_channel(channel_id)
            logger.info(f"ğŸ—‘ï¸  CACHE INVALIDATED: Cleared cached selections for channel '{channel_name}' due to {reason}")
        except Exception as e:
            logger.warning(f"âš ï¸  CACHE INVALIDATION FAILED for channel '{channel_name}': {e}")

    def _handle_http_status_error(self, error: httpx.HTTPStatusError, channel, attempt_num: int, failed_channels: set, total_candidates: List) -> None:
        """å¤„ç†HTTPçŠ¶æ€é”™è¯¯"""
        error_text = error.response.text if hasattr(error.response, 'text') else str(error)
        logger.warning(f"âŒ ATTEMPT #{attempt_num} FAILED: Channel '{channel.name}' returned HTTP {error.response.status_code}")
        logger.warning(f"âŒ ERROR DETAILS: {error_text[:200]}...")
        
        self.router.update_channel_health(channel.id, False)
        
        # æ™ºèƒ½æ‹‰é»‘ï¼šå¯¹äºè®¤è¯é”™è¯¯ç­‰æ°¸ä¹…æ€§é”™è¯¯ï¼Œæ‹‰é»‘æ•´ä¸ªæ¸ é“
        if error.response.status_code in [401, 403]:
            failed_channels.add(channel.id)
            logger.warning(f"ğŸš« CHANNEL BLACKLISTED: Channel '{channel.name}' (ID: {channel.id}) blacklisted due to HTTP {error.response.status_code}")
            logger.info(f"âš¡ SKIP OPTIMIZATION: Will skip all remaining models from channel '{channel.name}'")
            
            # ä½¿ç›¸å…³ç¼“å­˜å¤±æ•ˆ - æ°¸ä¹…æ€§é”™è¯¯éœ€è¦ç«‹å³æ¸…é™¤ç¼“å­˜
            self._invalidate_channel_cache(channel.id, channel.name, "permanent error")
        
        # å¯¹äºä¸´æ—¶é”™è¯¯ï¼ˆå¦‚429, 500ï¼‰ï¼Œä¹Ÿä½¿ç¼“å­˜å¤±æ•ˆä½†ä¸æ°¸ä¹…æ‹‰é»‘
        elif error.response.status_code in [429, 500, 502, 503, 504]:
            self._invalidate_channel_cache(channel.id, channel.name, "temporary error")
        
        if attempt_num < len(total_candidates):
            logger.info(f"ğŸ”„ FAILOVER: Trying next channel (#{attempt_num + 1})")
    
    def _handle_request_error(self, error: httpx.RequestError, channel, attempt_num: int, total_candidates: List) -> None:
        """å¤„ç†è¯·æ±‚é”™è¯¯"""
        logger.warning(f"âŒ ATTEMPT #{attempt_num} FAILED: Channel '{channel.name}' network error: {str(error)}")
        self.router.update_channel_health(channel.id, False)
        
        # ç½‘ç»œé”™è¯¯ä¹Ÿå¯èƒ½æ˜¯æ¸ é“é—®é¢˜ï¼Œæ¸…é™¤ç›¸å…³ç¼“å­˜
        self._invalidate_channel_cache(channel.id, channel.name, "network error")
        
        if attempt_num < len(total_candidates):
            logger.info(f"ğŸ”„ FAILOVER: Trying next channel (#{attempt_num + 1})")
    
    def _infer_capabilities(self, request: ChatCompletionRequest) -> List[str]:
        """æ¨æ–­è¯·æ±‚æ‰€éœ€çš„èƒ½åŠ›"""
        capabilities = ["text"]
        if request.functions or request.tools:
            capabilities.append("function_calling")
        for message in request.messages:
            if isinstance(message.content, list):
                for item in message.content:
                    if isinstance(item, dict) and item.get("type") == "image_url":
                        capabilities.append("vision")
                        break
        return capabilities
    
    # --- Error Response Creators ---
    
    def _create_no_channels_error(self, model: str, execution_time: float) -> JSONResponse:
        """åˆ›å»ºæ— å¯ç”¨æ¸ é“é”™è¯¯å“åº”"""
        logger.error(f"âŒ ROUTING FAILED: No available channels found for model '{model}'")
        
        headers = {
            "X-Router-Status": "no-channels-found",
            "X-Router-Time": f"{execution_time:.3f}s",
            "X-Router-Model-Requested": model
        }
        
        return JSONResponse(
            status_code=503,
            content={"detail": f"No available channels found for model '{model}'."},
            headers=headers
        )
    
    def _handle_tag_not_found_error(self, error: TagNotFoundError, model: str, execution_time: float) -> JSONResponse:
        """å¤„ç†æ ‡ç­¾æœªæ‰¾åˆ°é”™è¯¯"""
        logger.error(f"âŒ TAG NOT FOUND: {error} (after {execution_time:.3f}s)")
        
        headers = {
            "X-Router-Status": "tag-not-found",
            "X-Router-Tags": ",".join(error.tags),
            "X-Router-Time": f"{execution_time:.3f}s",
            "X-Router-Model-Requested": model
        }
        
        return JSONResponse(
            status_code=404,
            content={"detail": str(error)},
            headers=headers
        )
    
    def _handle_unexpected_error(self, error: Exception, model: str, execution_time: float) -> HTTPException:
        """å¤„ç†æ„å¤–é”™è¯¯"""
        logger.error(f"ğŸ’¥ UNEXPECTED ERROR: Internal error occurred after {execution_time:.3f}s: {error}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An internal server error occurred: {error}")
    
    def _create_all_channels_failed_error(self, model: str, candidates: List, last_error: Optional[Exception], execution_time: float) -> JSONResponse:
        """åˆ›å»ºæ‰€æœ‰æ¸ é“å¤±è´¥é”™è¯¯å“åº”"""
        logger.error(f"ğŸ’¥ ALL CHANNELS FAILED: All {len(candidates)} channels failed for model '{model}' after {execution_time:.3f}s")
        
        error_detail = f"All available channels failed. Last error: {str(last_error)}"
        if hasattr(last_error, 'response'):
            error_detail += f" - Details: {last_error.response.text}"

        logger.error(f"ğŸ’¥ FINAL ERROR: {error_detail}")
        
        headers = {
            "X-Router-Status": "all-channels-failed",
            "X-Router-Attempts": str(len(candidates)),
            "X-Router-Time": f"{execution_time:.3f}s",
            "X-Router-Model-Requested": model
        }
        
        return JSONResponse(
            status_code=503,
            content={"detail": error_detail},
            headers=headers
        )
    
    # --- Low-level API calls (moved from main.py) ---
    
    async def _fast_channel_check(self, url: str, headers: dict) -> Tuple[bool, int, str]:
        """å¿«é€Ÿæ£€æŸ¥æ¸ é“æ˜¯å¦å¯ç”¨"""
        cache_key = f"{url}:{hash(str(headers))}"
        
        cached_result = await cache_get("channel_availability", cache_key)
        if cached_result:
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ¸ é“å¯ç”¨æ€§ç»“æœ: {url}")
            return cached_result['available'], cached_result['status_code'], cached_result['message']
        
        try:
            http_pool = get_http_pool()
            
            test_data = {
                "model": "test-availability",
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 1
            }
            
            async with http_pool.stream('POST', url, json=test_data, headers=headers) as response:
                if response.status_code in [200, 400, 404, 422]:
                    result = {'available': True, 'status_code': response.status_code, 'message': "OK"}
                    await cache_set("channel_availability", cache_key, result)
                    return True, response.status_code, "OK"
                else:
                    result = {'available': False, 'status_code': response.status_code, 'message': "Service error"}
                    await cache_set("channel_availability", cache_key, result)
                    return False, response.status_code, "Service error"
        except Exception as e:
            result = {'available': False, 'status_code': 0, 'message': str(e)}
            await cache_set("channel_availability", cache_key, result)
            return False, 0, str(e)
    
    async def _call_channel_api(self, url: str, headers: dict, request_data: dict):
        """ä¼˜åŒ–çš„APIè°ƒç”¨"""
        http_pool = get_http_pool()
        
        async with http_pool.stream('POST', url, json=request_data, headers=headers) as response:
            if response.status_code != 200:
                # è¯»å–é”™è¯¯å†…å®¹ï¼Œé™åˆ¶å¤§å°ä»¥é¿å…å†…å­˜é—®é¢˜
                error_content = await response.aread()
                if len(error_content) > 1024:
                    error_content = error_content[:1024]
                response._content = error_content
                response.raise_for_status()
            
            content = await response.aread()
            return response.json()
    
    async def _stream_channel_api(self, url: str, headers: dict, request_data: dict, channel_id: str):
        """ä¼˜åŒ–çš„æµå¼APIè°ƒç”¨"""
        chunk_count = 0
        stream_start_time = time.time()
        
        logger.info(f"ğŸŒŠ STREAM START: Initiating optimized streaming request to channel '{channel_id}'")
        
        try:
            http_pool = get_http_pool()
            
            async with http_pool.stream("POST", url, json=request_data, headers=headers) as response:
                if response.status_code != 200:
                    # è¯»å–é”™è¯¯å†…å®¹ï¼Œé™åˆ¶å¤§å°ä»¥é¿å…å†…å­˜é—®é¢˜
                    error_body = await response.aread()
                    if len(error_body) > 1024:
                        error_body = error_body[:1024]
                    logger.error(f"ğŸŒŠ STREAM ERROR: Channel '{channel_id}' returned status {response.status_code}")
                    
                    error_text = error_body.decode('utf-8', errors='ignore')[:200]
                    logger.error(f"ğŸŒŠ STREAM ERROR DETAILS: {error_text}")
                    self.router.update_channel_health(channel_id, False)
                    
                    yield f"data: {json.dumps({'error': {'message': f'Upstream API error: {error_text}', 'code': response.status_code}})}\n\n"
                    return

                logger.info(f"ğŸŒŠ STREAM CONNECTED: Successfully connected to channel '{channel_id}', starting optimized data flow")
                
                async for chunk in response.aiter_bytes(chunk_size=8192):
                    if chunk:
                        chunk_count += 1
                        if chunk_count % 20 == 0:
                            logger.debug(f"ğŸŒŠ STREAMING: Received {chunk_count} chunks from channel '{channel_id}'")
                    yield chunk
                        
            stream_duration = time.time() - stream_start_time
            self.router.update_channel_health(channel_id, True, stream_duration)
            logger.info(f"ğŸŒŠ STREAM COMPLETE: Channel '{channel_id}' completed optimized streaming {chunk_count} chunks in {stream_duration:.3f}s")
            
        except httpx.HTTPStatusError as e:
            error_text = e.response.text if hasattr(e.response, 'text') else str(e)
            logger.error(f"ğŸŒŠ STREAM FAILED: Channel '{channel_id}' HTTP error {e.response.status_code}: {error_text[:200]}...")
            self.router.update_channel_health(channel_id, False)
            yield f"data: {json.dumps({'error': {'message': f'Upstream API error: {error_text}', 'code': e.response.status_code}})}\n\n"
            
        except Exception as e:
            logger.error(f"ğŸŒŠ STREAM EXCEPTION: Streaming request for channel '{channel_id}' failed: {e}", exc_info=True)
            self.router.update_channel_health(channel_id, False)
            yield f"data: {json.dumps({'error': {'message': str(e), 'code': 500}})}\n\n"

    def _add_cost_information(self, response_json: dict, channel, routing_score: RoutingScore) -> dict:
        """ä¸ºå“åº”æ·»åŠ å®æ—¶æˆæœ¬ä¿¡æ¯"""
        try:
            # è·å–usageä¿¡æ¯
            usage = response_json.get('usage', {})
            if not usage:
                logger.warning("No usage information in response, cannot calculate cost")
                return response_json
            
            prompt_tokens = usage.get('prompt_tokens', 0)
            completion_tokens = usage.get('completion_tokens', 0)
            total_tokens = usage.get('total_tokens', prompt_tokens + completion_tokens)
            
            # è®¡ç®—æˆæœ¬
            model_used = response_json.get('model', 'unknown')
            cost_info = self._calculate_request_cost(channel, prompt_tokens, completion_tokens, model_used)
            
            # è·å–æˆæœ¬è¿½è¸ªå™¨è¿›è¡Œè®°å½•
            cost_tracker = get_cost_tracker()
            cost_tracker.add_request_cost(
                cost=cost_info['total_cost'],
                model=response_json.get('model', 'unknown'),
                channel_id=channel.id,
                tokens={
                    'prompt_tokens': prompt_tokens,
                    'completion_tokens': completion_tokens,
                    'total_tokens': total_tokens
                }
            )
            
            # è·å–ä¼šè¯ç»Ÿè®¡
            session_summary = cost_tracker.get_session_summary()
            
            # åˆ›å»ºå¢å¼ºçš„å“åº”
            enhanced_response = response_json.copy()
            
            # æ·»åŠ æˆæœ¬ä¿¡æ¯åˆ°usageéƒ¨åˆ†
            enhanced_usage = usage.copy()
            enhanced_usage.update({
                'cost_breakdown': cost_info,
                'session_cost': session_summary.get('formatted_total_cost', '$0.00'),
                'session_requests': session_summary.get('total_requests', 0)
            })
            enhanced_response['usage'] = enhanced_usage
            
            # æ·»åŠ è·¯ç”±ä¿¡æ¯
            enhanced_response['smart_router'] = {
                'channel_used': channel.name,
                'channel_id': channel.id,
                'routing_score': round(routing_score.total_score, 3),
                'cost_score': round(routing_score.cost_score, 3),
                'speed_score': round(routing_score.speed_score, 3),
                'provider': channel.provider
            }
            
            logger.info(f"ğŸ’° COST: Request cost ${cost_info['total_cost']:.6f} | Session total: {session_summary.get('formatted_total_cost', '$0.00')}")
            
            return enhanced_response
            
        except Exception as e:
            logger.error(f"Failed to add cost information: {e}")
            return response_json

    def _calculate_request_cost(self, channel, prompt_tokens: int, completion_tokens: int, model_name: str = None) -> dict:
        """è®¡ç®—å•æ¬¡è¯·æ±‚çš„æˆæœ¬ - ä¼˜å…ˆä½¿ç”¨æ¨¡å‹çº§åˆ«å®šä»·ä¿¡æ¯"""
        input_cost_per_token = 0.0
        output_cost_per_token = 0.0
        
        # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨æ¨¡å‹çº§åˆ«çš„å®šä»·ä¿¡æ¯ï¼ˆä»ç¼“å­˜ä¸­è·å–ï¼‰
        if model_name:
            try:
                # å¯¼å…¥é…ç½®åŠ è½½å™¨æ¥è®¿é—®æ¨¡å‹ç¼“å­˜
                from core.yaml_config import YAMLConfigLoader
                config_loader = YAMLConfigLoader()
                
                model_cache = config_loader.get_model_cache()
                if channel.id in model_cache:
                    discovered_info = model_cache[channel.id]
                    if isinstance(discovered_info, dict) and 'models_data' in discovered_info:
                        models_data = discovered_info['models_data']
                        if model_name in models_data:
                            model_data = models_data[model_name]
                            if 'raw_data' in model_data and 'pricing' in model_data['raw_data']:
                                pricing = model_data['raw_data']['pricing']
                                prompt_cost = float(pricing.get('prompt', '0'))
                                completion_cost = float(pricing.get('completion', '0'))
                                if prompt_cost == 0.0 and completion_cost == 0.0:
                                    logger.debug(f"COST: Using model-level free pricing for '{model_name}' (prompt={prompt_cost}, completion={completion_cost})")
                                    input_cost_per_token = 0.0
                                    output_cost_per_token = 0.0
                                else:
                                    # æ¨¡å‹æœ‰æ˜ç¡®çš„æ”¶è´¹å®šä»·
                                    logger.debug(f"COST: Using model-level pricing for '{model_name}' (prompt={prompt_cost}, completion={completion_cost})")
                                    input_cost_per_token = prompt_cost
                                    output_cost_per_token = completion_cost
                            else:
                                logger.debug(f"COST: No model-level pricing found for '{model_name}', falling back to channel pricing")
                        else:
                            logger.debug(f"COST: Model '{model_name}' not found in cache for channel '{channel.id}'")
                    else:
                        logger.debug(f"COST: Invalid cache structure for channel '{channel.id}'")
            except Exception as e:
                logger.warning(f"COST: Error accessing model pricing for '{model_name}': {e}")
        
        # ğŸ”¥ å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¨¡å‹çº§åˆ«å®šä»·ï¼Œä½¿ç”¨æ¸ é“çº§åˆ«é…ç½®
        if input_cost_per_token == 0.0 and output_cost_per_token == 0.0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ˜ç¡®çš„å…è´¹æ¨¡å‹ï¼ˆ:free åç¼€ï¼‰
            if model_name and (":free" in model_name.lower() or "-free" in model_name.lower()):
                logger.debug(f"COST: Model '{model_name}' has explicit :free suffix, using zero cost")
                input_cost_per_token = 0.0
                output_cost_per_token = 0.0
            # ä½¿ç”¨æ¸ é“çº§åˆ«å®šä»·
            elif hasattr(channel, 'cost_per_token') and channel.cost_per_token:
                input_cost_per_token = channel.cost_per_token.get('input', 0.0)
                output_cost_per_token = channel.cost_per_token.get('output', 0.0)
                logger.debug(f"COST: Using channel cost_per_token (input={input_cost_per_token}, output={output_cost_per_token})")
            # å›é€€åˆ°pricingé…ç½®
            elif hasattr(channel, 'pricing') and channel.pricing:
                input_cost_per_token = channel.pricing.get('input_cost_per_1k', 0.001) / 1000
                output_cost_per_token = channel.pricing.get('output_cost_per_1k', 0.002) / 1000
                logger.debug(f"COST: Using channel pricing (input={input_cost_per_token}, output={output_cost_per_token})")
            else:
                # é»˜è®¤ä¼°ç®—å€¼
                input_cost_per_token = 0.0000005  # $0.0005 per 1K tokens
                output_cost_per_token = 0.0000015  # $0.0015 per 1K tokens
                logger.debug(f"COST: Using default pricing (input={input_cost_per_token}, output={output_cost_per_token})")
        
        input_cost = prompt_tokens * input_cost_per_token
        output_cost = completion_tokens * output_cost_per_token
        total_cost = input_cost + output_cost
        
        return {
            'input_tokens': prompt_tokens,
            'output_tokens': completion_tokens,
            'input_cost_per_token': input_cost_per_token,
            'output_cost_per_token': output_cost_per_token,
            'input_cost': round(input_cost, 8),
            'output_cost': round(output_cost, 8),
            'total_cost': round(total_cost, 8),
            'currency': 'USD'
        }