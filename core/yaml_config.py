"""
åŸºäºYAMLçš„é…ç½®åŠ è½½å™¨ - Pydanticç‰ˆæœ¬
"""

import asyncio
import json
import logging
import threading
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml

from .auth import generate_secure_token as generate_random_token
from .config.async_loader import get_async_config_loader, load_config_async
from .config_models import Channel, Config, Provider
from .utils.api_key_cache import get_api_key_cache_manager
from .utils.async_file_ops import get_async_file_manager

logger = logging.getLogger(__name__)


@dataclass
class RuntimeState:
    """è¿è¡Œæ—¶çŠ¶æ€"""

    channel_stats: Dict[str, Any] = field(default_factory=dict)
    request_history: List[Dict[str, Any]] = field(default_factory=list)
    health_scores: Dict[str, float] = field(default_factory=dict)
    cost_tracking: Dict[str, Any] = field(default_factory=dict)


class YAMLConfigLoader:
    """åŸºäºPydanticçš„YAMLé…ç½®åŠ è½½å™¨"""

    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path or self._get_default_path("router_config.yaml")

        # è¿è¡Œæ—¶çŠ¶æ€
        self.runtime_state: RuntimeState = RuntimeState()

        # æ¨¡å‹ç¼“å­˜
        self.model_cache: Dict[str, Dict] = {}

        # API Keyç¼“å­˜ç®¡ç†å™¨
        self.api_key_cache_manager = get_api_key_cache_manager()

        # è¿ç§»çŠ¶æ€æ ‡å¿—ï¼Œé˜²æ­¢é‡å¤è¿ç§»
        self._migration_completed = False
        self._migration_in_progress = False

        # åŠ è½½å¹¶è§£æé…ç½®
        self.config: Config = self._load_and_validate_config()

        # åˆ›å»ºæ¸ é“æ˜ å°„
        self.channels_map = {ch.id: ch for ch in self.config.channels}

        # åŠ è½½æ¨¡å‹ç¼“å­˜
        self._load_model_cache_from_disk()

        logger.info(
            f"Config loaded: {len(self.config.providers)} providers, {len(self.config.channels)} channels"
        )

    @classmethod
    async def create_async(
        cls, config_path: Optional[str] = None
    ) -> "YAMLConfigLoader":
        """
        å¼‚æ­¥åˆ›å»ºé…ç½®åŠ è½½å™¨å®ä¾‹

        è¿™æ˜¯ Phase 1 ä¼˜åŒ–çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä½¿ç”¨å¼‚æ­¥é…ç½®åŠ è½½å™¨
        æ›¿ä»£åŒæ­¥çš„ __init__ æ–¹æ³•ä»¥å‡å°‘ 2-4 ç§’çš„å¯åŠ¨å»¶è¿Ÿ
        """
        start_time = asyncio.get_event_loop().time()

        # åˆ›å»ºå®ä¾‹ä½†è·³è¿‡åŒæ­¥åˆå§‹åŒ–
        instance = cls.__new__(cls)

        # è®¾ç½®åŸºæœ¬å±æ€§
        instance.config_path = config_path or instance._get_default_path(
            "router_config.yaml"
        )
        instance.runtime_state = RuntimeState()
        instance.model_cache = {}
        instance.api_key_cache_manager = get_api_key_cache_manager()
        instance._migration_completed = False
        instance._migration_in_progress = False

        try:
            # ğŸš€ ä½¿ç”¨å¼‚æ­¥é…ç½®åŠ è½½å™¨æ›¿ä»£åŒæ­¥åŠ è½½
            logger.info("å¼€å§‹å¼‚æ­¥é…ç½®åŠ è½½...")
            instance.config = await load_config_async(instance.config_path)

            # åˆ›å»ºæ¸ é“æ˜ å°„
            instance.channels_map = {ch.id: ch for ch in instance.config.channels}

            # ğŸš€ å¼‚æ­¥åŠ è½½æ¨¡å‹ç¼“å­˜ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            await instance._load_model_cache_async()

            elapsed = asyncio.get_event_loop().time() - start_time
            logger.info(
                f"å¼‚æ­¥é…ç½®åŠ è½½å®Œæˆ: {len(instance.config.providers)} providers, "
                f"{len(instance.config.channels)} channels, è€—æ—¶: {elapsed:.2f}s"
            )

            return instance

        except Exception as e:
            logger.error(f"å¼‚æ­¥é…ç½®åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥æ¨¡å¼: {e}")
            # å¦‚æœå¼‚æ­¥åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥åŠ è½½
            return cls(config_path)

    async def _load_model_cache_async(self):
        """å¼‚æ­¥åŠ è½½æ¨¡å‹ç¼“å­˜"""
        try:
            cache_file = (
                Path(__file__).parent.parent / "cache" / "discovered_models.json"
            )
            if not cache_file.exists():
                logger.debug("æ¨¡å‹ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½")
                return

            # ä½¿ç”¨å¼‚æ­¥æ–‡ä»¶ç®¡ç†å™¨
            file_manager = get_async_file_manager()
            raw_cache = await file_manager.read_json(cache_file, {})

            if raw_cache:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»ç¼“å­˜æ ¼å¼
                if (
                    self._needs_cache_migration(raw_cache)
                    and not self._migration_completed
                    and not self._migration_in_progress
                ):
                    logger.info("æ£€æµ‹åˆ°ä¼ ç»Ÿç¼“å­˜æ ¼å¼ï¼Œä½¿ç”¨ç°æœ‰ç¼“å­˜å¹¶å®‰æ’åå°è¿ç§»")
                    self.model_cache = raw_cache  # ä¸´æ—¶ä½¿ç”¨åŸå§‹ç¼“å­˜

                    # æ ‡è®°è¿ç§»æ­£åœ¨è¿›è¡Œ
                    self._migration_in_progress = True

                    # ğŸš€ å¯åŠ¨åå°è¿ç§»ä»»åŠ¡ï¼ˆä¸é˜»å¡å¯åŠ¨ï¼‰
                    asyncio.create_task(self._async_cache_migration(raw_cache))
                else:
                    self.model_cache = raw_cache

                logger.debug(f"å¼‚æ­¥åŠ è½½æ¨¡å‹ç¼“å­˜æˆåŠŸ: {len(self.model_cache)} æ¡è®°å½•")
            else:
                logger.debug("æ¨¡å‹ç¼“å­˜ä¸ºç©º")

        except Exception as e:
            logger.error(f"å¼‚æ­¥åŠ è½½æ¨¡å‹ç¼“å­˜å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç³»ç»Ÿç»§ç»­è¿è¡Œ

    async def _async_cache_migration(self, raw_cache: Dict[str, Dict]):
        """å¼‚æ­¥ç¼“å­˜è¿ç§»ä»»åŠ¡"""
        try:
            logger.info("å¼€å§‹åå°ç¼“å­˜è¿ç§»...")

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè¿ç§»é€»è¾‘
            loop = asyncio.get_event_loop()
            migrated_cache = await loop.run_in_executor(
                None, self._perform_cache_migration, raw_cache
            )

            # æ›´æ–°ç¼“å­˜
            self.model_cache = migrated_cache
            self._migration_completed = True
            self._migration_in_progress = False

            # å¼‚æ­¥ä¿å­˜è¿ç§»åçš„ç¼“å­˜
            cache_file = (
                Path(__file__).parent.parent / "cache" / "discovered_models.json"
            )
            file_manager = get_async_file_manager()
            await file_manager.write_json(cache_file, migrated_cache)

            logger.info("åå°ç¼“å­˜è¿ç§»å®Œæˆ")

        except Exception as e:
            logger.error(f"åå°ç¼“å­˜è¿ç§»å¤±è´¥: {e}")
            self._migration_in_progress = False

    def _perform_cache_migration(self, raw_cache: Dict[str, Dict]) -> Dict[str, Dict]:
        """æ‰§è¡Œç¼“å­˜è¿ç§»é€»è¾‘ï¼ˆCPUå¯†é›†å‹ï¼Œåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
        # è¿™é‡Œæ”¾ç½®åŸæœ‰çš„ç¼“å­˜è¿ç§»é€»è¾‘
        migrated_cache = {}

        for provider_id, provider_data in raw_cache.items():
            if isinstance(provider_data, dict) and "models" in provider_data:
                # æ–°æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                migrated_cache[provider_id] = provider_data
            else:
                # æ—§æ ¼å¼ï¼Œéœ€è¦è¿ç§»
                migrated_cache[provider_id] = {
                    "models": provider_data if isinstance(provider_data, list) else [],
                    "last_discovery": datetime.now().isoformat(),
                    "total_models": (
                        len(provider_data) if isinstance(provider_data, list) else 0
                    ),
                }

        return migrated_cache

    def _get_default_path(self, filename: str) -> str:
        """è·å–é…ç½®æ–‡ä»¶çš„é»˜è®¤è·¯å¾„"""
        project_root = Path(__file__).parent.parent
        config_file = project_root / "config" / filename
        if config_file.exists():
            return str(config_file)

        # å°è¯•å…¶ä»–å¸¸è§é…ç½®æ–‡ä»¶
        alternatives = ["test_config.yaml", "router_config.yaml.template"]
        for alt in alternatives:
            alt_file = project_root / "config" / alt
            if alt_file.exists():
                logger.warning(f"Using fallback config: {alt}")
                return str(alt_file)

        raise FileNotFoundError(
            f"Configuration file {filename} not found in 'config' directory."
        )

    def _load_and_validate_config(self) -> Config:
        """åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¸ºå…¼å®¹æ€§ä¿ç•™ï¼‰"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                raw_data = yaml.safe_load(f) or {}

            # æ£€æŸ¥Tokené…ç½®å¹¶è‡ªåŠ¨ç”Ÿæˆ
            config_modified = self._ensure_auth_token(raw_data)

            # ä½¿ç”¨Pydanticè¿›è¡ŒéªŒè¯å’Œè§£æ
            config = Config.parse_obj(raw_data)

            # å¦‚æœé…ç½®è¢«ä¿®æ”¹ï¼Œä¿å­˜å›æ–‡ä»¶
            if config_modified:
                self._save_config_to_file(raw_data)

            return config

        except Exception as e:
            logger.error(f"Failed to load config from {self.config_path}: {e}")
            raise

    async def _load_and_validate_config_async(self) -> Config:
        """å¼‚æ­¥åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶"""
        try:
            file_manager = get_async_file_manager()
            raw_data = await file_manager.read_yaml(self.config_path, {})

            # æ£€æŸ¥Tokené…ç½®å¹¶è‡ªåŠ¨ç”Ÿæˆ
            config_modified = self._ensure_auth_token(raw_data)

            # ä½¿ç”¨Pydanticè¿›è¡ŒéªŒè¯å’Œè§£æ
            config = Config.parse_obj(raw_data)

            # å¦‚æœé…ç½®è¢«ä¿®æ”¹ï¼Œå¼‚æ­¥ä¿å­˜å›æ–‡ä»¶
            if config_modified:
                await self._save_config_to_file_async(raw_data)

            return config

        except Exception as e:
            logger.error(f"Failed to load config from {self.config_path}: {e}")
            raise

    def _ensure_auth_token(self, config_data: Dict[str, Any]) -> bool:
        """
        ç¡®ä¿è®¤è¯é…ç½®ä¸­æœ‰Tokenï¼Œå¦‚æœå¯ç”¨è®¤è¯ä½†æ²¡æœ‰Tokenåˆ™è‡ªåŠ¨ç”Ÿæˆ

        Returns:
            bool: å¦‚æœé…ç½®è¢«ä¿®æ”¹åˆ™è¿”å›True
        """
        auth_config = config_data.get("auth", {})

        # å¦‚æœè®¤è¯å¯ç”¨ä½†æ²¡æœ‰api_tokenï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if auth_config.get("enabled", False) and not auth_config.get("api_token"):
            new_token = generate_random_token()
            auth_config["api_token"] = new_token
            config_data["auth"] = auth_config

            logger.info(f"Auto-generated API token for authentication: {new_token}")
            logger.warning(
                "Please save this token! You will need it to access the API."
            )

            return True

        return False

    def _save_config_to_file(self, config_data: Dict[str, Any]) -> None:
        """
        ä¿å­˜é…ç½®æ•°æ®åˆ°æ–‡ä»¶ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¸ºå…¼å®¹æ€§ä¿ç•™ï¼‰
        """
        try:
            with open(self.config_path, "w", encoding="utf-8") as f:
                yaml.dump(
                    config_data,
                    f,
                    default_flow_style=False,
                    allow_unicode=True,
                    indent=2,
                )
            logger.info(f"Configuration updated and saved to {self.config_path}")
        except Exception as e:
            logger.error(f"Failed to save config to {self.config_path}: {e}")
            raise

    async def _save_config_to_file_async(self, config_data: Dict[str, Any]) -> None:
        """
        å¼‚æ­¥ä¿å­˜é…ç½®æ•°æ®åˆ°æ–‡ä»¶
        """
        try:
            file_manager = get_async_file_manager()
            success = await file_manager.write_yaml(self.config_path, config_data)

            if success:
                logger.info(f"Configuration updated and saved to {self.config_path}")
            else:
                raise Exception("Failed to write config file")
        except Exception as e:
            logger.error(f"Failed to save config to {self.config_path}: {e}")
            raise

    def _schedule_cache_migration(self, raw_cache: Dict[str, Any]) -> None:
        """Schedule cache migration regardless of event loop availability."""

        async def _runner():
            await self._migrate_cache_background(raw_cache)

        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            logger.info(
                "No event loop available for background migration, spawning worker thread"
            )

            def _thread_target():
                try:
                    asyncio.run(_runner())
                except Exception as exc:
                    logger.error(
                        "BACKGROUND MIGRATION: Threaded migration failed: %s", exc
                    )
                    self._migration_in_progress = False

            threading.Thread(
                target=_thread_target, name="cache-migration", daemon=True
            ).start()
        else:
            loop.create_task(_runner())

    def _load_model_cache_from_disk(self):
        """ä»ç£ç›˜åŠ è½½æ¨¡å‹å‘ç°ä»»åŠ¡çš„ç¼“å­˜ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¸ºå…¼å®¹æ€§ä¿ç•™ï¼‰"""
        try:
            # ç›´æ¥ä»ç¼“å­˜æ–‡ä»¶åŠ è½½
            cache_file = (
                Path(__file__).parent.parent / "cache" / "discovered_models.json"
            )
            if cache_file.exists():
                with open(cache_file, "r", encoding="utf-8") as f:
                    raw_cache = json.load(f)

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»ç¼“å­˜æ ¼å¼
                    if (
                        self._needs_cache_migration(raw_cache)
                        and not self._migration_completed
                        and not self._migration_in_progress
                    ):
                        logger.info(
                            "CACHE MIGRATION: Detected legacy cache format, using as-is and scheduling background migration"
                        )
                        # ğŸš€ ä¼˜åŒ–ï¼šå…ˆä½¿ç”¨ç°æœ‰ç¼“å­˜ï¼Œé¿å…é˜»å¡å¯åŠ¨
                        self.model_cache = raw_cache  # ä¸´æ—¶ä½¿ç”¨åŸå§‹ç¼“å­˜

                        # æ ‡è®°è¿ç§»æ­£åœ¨è¿›è¡Œ
                        self._migration_in_progress = True

                        # ğŸš€ å¯åŠ¨åå°è¿ç§»ä»»åŠ¡ï¼ˆä¸é˜»å¡å¯åŠ¨ï¼‰
                        self._schedule_cache_migration(raw_cache)
                    else:
                        self.model_cache = raw_cache
                        if not self._needs_cache_migration(raw_cache):
                            self._migration_completed = True

                        # æ¸…ç†æ— æ•ˆæ¡ç›®ï¼ˆä»…å¯¹å·²è¿ç§»çš„ç¼“å­˜æ‰§è¡Œï¼‰
                        self.model_cache = (
                            self.api_key_cache_manager.cleanup_invalid_entries(
                                self.model_cache, self._get_channels_for_migration()
                            )
                        )

                    # è¾“å‡ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
                    stats = self.api_key_cache_manager.get_cache_statistics(
                        self.model_cache
                    )
                    logger.info(
                        f"Loaded model cache: {stats['total_entries']} entries, "
                        f"{stats['api_key_entries']} API key-level, {stats['legacy_entries']} legacy, "
                        f"{stats['api_key_coverage']}% coverage"
                    )

                    # ğŸš€ ç«‹å³æ„å»ºå†…å­˜ç´¢å¼•ï¼ˆå¯åŠ¨æ—¶é¢„åŠ è½½ï¼‰
                    self._build_memory_index()
            else:
                logger.warning("Model cache file not found")
                self.model_cache = {}
        except Exception as e:
            # ä½œä¸ºåå¤‡æ–¹æ¡ˆï¼Œå°è¯•ä»ä»»åŠ¡æ¨¡å—åŠ è½½
            try:
                from .scheduler.tasks.model_discovery import get_model_discovery_task

                task = get_model_discovery_task()
                raw_cache = task.cached_models
                if raw_cache:
                    # åº”ç”¨ç›¸åŒçš„è¿ç§»é€»è¾‘
                    if self._needs_cache_migration(raw_cache):
                        logger.info(
                            "Migrating task cache from legacy format to API key-level format"
                        )
                        self.model_cache = (
                            self.api_key_cache_manager.migrate_legacy_cache(
                                raw_cache, self._get_channels_for_migration()
                            )
                        )
                    else:
                        self.model_cache = raw_cache
                    logger.info(
                        f"Loaded model cache from task for {len(self.model_cache)} entries"
                    )
                else:
                    self.model_cache = {}
            except Exception as e2:
                logger.warning(
                    f"Failed to load model cache from both sources: {e}, {e2}"
                )
                self.model_cache = {}

    def get_channels_by_model(self, model_name: str) -> List[Channel]:
        """æ ¹æ®æ¨¡å‹åç§°è·å–æ¸ é“"""
        return [ch for ch in self.get_enabled_channels() if ch.model_name == model_name]

    def get_channels_by_tag(self, tag: str) -> List[Channel]:
        """æ ¹æ®æ ‡ç­¾è·å–æ¸ é“"""
        return [ch for ch in self.get_enabled_channels() if tag in ch.tags]

    def _build_memory_index(self):
        """æ„å»ºå†…å­˜ç´¢å¼•ï¼ˆå¯åŠ¨æ—¶é¢„åŠ è½½ï¼‰"""
        try:
            if not self.model_cache:
                logger.warning("MEMORY INDEX: No model cache to index")
                return

            from core.utils.memory_index import get_memory_index

            memory_index = get_memory_index()

            # è·å–æ¸ é“é…ç½®ç”¨äºæ ‡ç­¾ç»§æ‰¿
            channel_configs = []
            try:
                from core.scheduler.tasks.model_discovery import get_merged_config

                merged_config = get_merged_config()
                channel_configs = merged_config.get("channels", [])
                logger.debug(
                    f"MEMORY INDEX: Loaded {len(channel_configs)} channel configs for tag mapping"
                )
            except Exception as e:
                logger.warning(f"MEMORY INDEX: Failed to load channel configs: {e}")

            stats = memory_index.build_index_from_cache(
                self.model_cache, channel_configs
            )

            logger.info(
                f"MEMORY INDEX READY: {stats.total_models} models, {stats.total_tags} tags, "
                f"{stats.memory_usage_mb:.1f}MB memory in {stats.build_time_ms:.1f}ms"
            )

        except Exception as e:
            logger.error(f"MEMORY INDEX BUILD FAILED: {e}")
            # ä¸å½±å“ç³»ç»Ÿå¯åŠ¨ï¼Œç»§ç»­è¿è¡Œ

    def _needs_cache_migration(self, cache: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦éœ€è¦è¿ç§»åˆ°API Keyçº§åˆ«æ ¼å¼"""
        if not cache:
            return False

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ—§æ ¼å¼çš„ç¼“å­˜é”®
        for cache_key in cache.keys():
            if not self.api_key_cache_manager.is_api_key_cache(cache_key):
                return True
        return False

    def _get_channels_for_migration(self) -> Dict[str, Any]:
        """è·å–ç”¨äºç¼“å­˜è¿ç§»çš„æ¸ é“æ˜ å°„"""
        channels_map = {}
        for channel in self.config.channels:
            channels_map[channel.id] = {
                "id": channel.id,
                "provider": getattr(
                    channel, "provider_name", getattr(channel, "provider", "unknown")
                ),
                "api_key": getattr(channel, "api_key", ""),
                "enabled": channel.enabled,
            }
        return channels_map

    def _save_migrated_cache(self):
        """ä¿å­˜è¿ç§»åçš„ç¼“å­˜ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¸ºå…¼å®¹æ€§ä¿ç•™ï¼‰"""
        try:
            cache_file = (
                Path(__file__).parent.parent / "cache" / "discovered_models.json"
            )
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(self.model_cache, f, indent=2, ensure_ascii=False)
            logger.info("Migrated cache saved successfully")
        except Exception as e:
            logger.error(f"Failed to save migrated cache: {e}")

    async def _save_migrated_cache_async(self):
        """å¼‚æ­¥ä¿å­˜è¿ç§»åçš„ç¼“å­˜"""
        try:
            cache_file = (
                Path(__file__).parent.parent / "cache" / "discovered_models.json"
            )
            file_manager = get_async_file_manager()
            success = await file_manager.write_json(
                cache_file, self.model_cache, indent=2
            )

            if success:
                logger.info("Migrated cache saved successfully")
            else:
                raise Exception("Failed to write cache file")
        except Exception as e:
            logger.error(f"Failed to save migrated cache async: {e}")

    async def _migrate_cache_background(self, raw_cache: Dict[str, Any]):
        """åå°è¿ç§»ç¼“å­˜æ ¼å¼ï¼ˆä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰"""
        try:
            logger.info("BACKGROUND MIGRATION: Starting cache migration in background")

            # æ‰§è¡Œè¿ç§»
            migrated_cache = self.api_key_cache_manager.migrate_legacy_cache(
                raw_cache, self._get_channels_for_migration()
            )

            # æ¸…ç†æ— æ•ˆæ¡ç›®
            cleaned_cache = self.api_key_cache_manager.cleanup_invalid_entries(
                migrated_cache, self._get_channels_for_migration()
            )

            # ğŸš€ å…ˆä¿å­˜å·²æ¸…ç†çš„ç¼“å­˜åˆ°ç£ç›˜ï¼Œä½†ä¸ç«‹å³æ›´æ–°å†…å­˜ç¼“å­˜
            self.model_cache = cleaned_cache  # ä¸´æ—¶è®¾ç½®ä»¥ä¾¿ä¿å­˜
            await self._save_migrated_cache_async()

            # ğŸš€ æ ‡è®°è¿ç§»å®ŒæˆçŠ¶æ€
            self._migration_completed = True
            self._migration_in_progress = False

            # ğŸš€ é‡å»ºå†…å­˜ç´¢å¼•ä»¥ä½¿ç”¨æ–°ç¼“å­˜ï¼ˆä¸€æ¬¡æ€§æ“ä½œï¼‰
            self._build_memory_index()

            logger.info("BACKGROUND MIGRATION: Cache migration completed successfully")

        except Exception as e:
            logger.error(
                f"BACKGROUND MIGRATION: Failed to migrate cache in background: {e}"
            )
            # ğŸš€ è¿ç§»å¤±è´¥æ—¶é‡ç½®çŠ¶æ€ï¼Œå…è®¸é‡è¯•
            self._migration_in_progress = False

    async def _load_model_cache_from_disk_async(self):
        """å¼‚æ­¥ä»ç£ç›˜åŠ è½½æ¨¡å‹å‘ç°ä»»åŠ¡çš„ç¼“å­˜"""
        try:
            file_manager = get_async_file_manager()
            cache_file = (
                Path(__file__).parent.parent / "cache" / "discovered_models.json"
            )

            if await file_manager.file_exists(cache_file):
                raw_cache = await file_manager.read_json(cache_file, {})

                # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»ç¼“å­˜æ ¼å¼
                if self._needs_cache_migration(raw_cache):
                    logger.info(
                        "Migrating cache from legacy format to API key-level format"
                    )
                    self.model_cache = self.api_key_cache_manager.migrate_legacy_cache(
                        raw_cache, self._get_channels_for_migration()
                    )
                    # å¼‚æ­¥ä¿å­˜è¿ç§»åçš„ç¼“å­˜
                    await self._save_migrated_cache_async()
                else:
                    self.model_cache = raw_cache

                # æ¸…ç†æ— æ•ˆæ¡ç›®
                self.model_cache = self.api_key_cache_manager.cleanup_invalid_entries(
                    self.model_cache, self._get_channels_for_migration()
                )

                # è¾“å‡ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
                stats = self.api_key_cache_manager.get_cache_statistics(
                    self.model_cache
                )
                logger.info(
                    f"Loaded model cache: {stats['total_entries']} entries, "
                    f"{stats['api_key_entries']} API key-level, {stats['legacy_entries']} legacy, "
                    f"{stats['api_key_coverage']}% coverage"
                )

                # ğŸš€ ç«‹å³æ„å»ºå†…å­˜ç´¢å¼•ï¼ˆå¯åŠ¨æ—¶é¢„åŠ è½½ï¼‰
                self._build_memory_index()
            else:
                logger.warning("Model cache file not found")
                self.model_cache = {}
        except Exception as e:
            # ä½œä¸ºåå¤‡æ–¹æ¡ˆï¼Œå°è¯•ä»ä»»åŠ¡æ¨¡å—åŠ è½½
            try:
                from .scheduler.tasks.model_discovery import get_model_discovery_task

                task = get_model_discovery_task()
                raw_cache = task.cached_models
                if raw_cache:
                    # åº”ç”¨ç›¸åŒçš„è¿ç§»é€»è¾‘
                    if self._needs_cache_migration(raw_cache):
                        logger.info(
                            "Migrating task cache from legacy format to API key-level format"
                        )
                        self.model_cache = (
                            self.api_key_cache_manager.migrate_legacy_cache(
                                raw_cache, self._get_channels_for_migration()
                            )
                        )
                    else:
                        self.model_cache = raw_cache
                    logger.info(
                        f"Loaded model cache from task for {len(self.model_cache)} entries"
                    )
                else:
                    self.model_cache = {}
            except Exception as e2:
                logger.warning(
                    f"Failed to load model cache from both sources: {e}, {e2}"
                )
                self.model_cache = {}

    def get_model_cache(self) -> Dict[str, Dict]:
        """è·å–æ¨¡å‹ç¼“å­˜ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.model_cache

    def get_model_cache_by_channel_and_key(
        self, channel_id: str, api_key: str
    ) -> Optional[Dict]:
        """è·å–ç‰¹å®šAPI Keyçš„æ¨¡å‹ç¼“å­˜"""
        cache_key = self.api_key_cache_manager.generate_cache_key(channel_id, api_key)
        return self.model_cache.get(cache_key)

    def get_model_cache_by_channel(self, channel_id: str) -> Dict[str, Any]:
        """è·å–æ¸ é“ä¸‹æ‰€æœ‰API Keyçš„ç¼“å­˜ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        # æŸ¥æ‰¾è¯¥æ¸ é“çš„æ‰€æœ‰ç¼“å­˜æ¡ç›®
        channel_cache_keys = self.api_key_cache_manager.find_cache_entries_by_channel(
            self.model_cache, channel_id
        )

        if not channel_cache_keys:
            # å°è¯•æŸ¥æ‰¾æ—§æ ¼å¼ç¼“å­˜
            legacy_cache = self.model_cache.get(channel_id)
            if legacy_cache:
                return legacy_cache
            return {}

        # å¦‚æœåªæœ‰ä¸€ä¸ªAPI Keyï¼Œè¿”å›å…¶ç¼“å­˜ï¼ˆå‘åå…¼å®¹ï¼‰
        if len(channel_cache_keys) == 1:
            return self.model_cache[channel_cache_keys[0]]

        # å¤šä¸ªAPI Keyçš„æƒ…å†µï¼Œè¿”å›åˆå¹¶ç»“æœ
        merged_models = []
        latest_status = "unknown"
        latest_update = None

        for cache_key in channel_cache_keys:
            cache_data = self.model_cache[cache_key]
            models = cache_data.get("models", [])
            merged_models.extend(models)

            # ä½¿ç”¨æœ€æ–°çš„çŠ¶æ€å’Œæ›´æ–°æ—¶é—´
            if cache_data.get("last_updated", "") > (latest_update or ""):
                latest_update = cache_data.get("last_updated")
                latest_status = cache_data.get("status", "unknown")

        # å»é‡å¹¶æ’åº
        unique_models = sorted(list(set(merged_models)))

        return {
            "channel_id": channel_id,
            "models": unique_models,
            "model_count": len(unique_models),
            "status": latest_status,
            "last_updated": latest_update,
            "merged_from_keys": len(channel_cache_keys),
            "note": f"Merged from {len(channel_cache_keys)} API keys",
        }

    def get_enabled_channels(self) -> List[Channel]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ¸ é“"""
        return [ch for ch in self.config.channels if ch.enabled]

    def get_channel_by_id(self, channel_id: str) -> Optional[Channel]:
        """æ ¹æ®IDè·å–æ¸ é“"""
        for channel in self.config.channels:
            if channel.id == channel_id:
                return channel
        return None

    def update_model_cache(self, new_cache: Dict[str, Dict]):
        """æ›´æ–°æ¨¡å‹ç¼“å­˜ï¼ˆæ”¯æŒAPI Keyçº§åˆ«ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»æ–°çš„ç¼“å­˜æ•°æ®
        if self._needs_cache_migration(new_cache):
            logger.info("Migrating new cache data to API key-level format")
            migrated_cache = self.api_key_cache_manager.migrate_legacy_cache(
                new_cache, self._get_channels_for_migration()
            )
            self.model_cache.update(migrated_cache)
        else:
            self.model_cache.update(new_cache)

        # è¾“å‡ºæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        stats = self.api_key_cache_manager.get_cache_statistics(self.model_cache)
        logger.info(
            f"Updated model cache: {stats['total_entries']} entries, "
            f"{stats['api_key_coverage']}% API key-level coverage"
        )

    def update_model_cache_for_channel_and_key(
        self, channel_id: str, api_key: str, cache_data: Dict[str, Any]
    ):
        """æ›´æ–°ç‰¹å®šæ¸ é“å’ŒAPI Keyçš„æ¨¡å‹ç¼“å­˜"""
        cache_key = self.api_key_cache_manager.generate_cache_key(channel_id, api_key)

        # æ·»åŠ API Keyç›¸å…³å…ƒæ•°æ®
        enhanced_cache_data = {
            **cache_data,
            "cache_key": cache_key,
            "channel_id": channel_id,
            "api_key_hash": cache_key.split("_")[-1] if "_" in cache_key else "",
            "updated_at": datetime.now().isoformat(),
        }

        self.model_cache[cache_key] = enhanced_cache_data
        logger.info(
            f"Updated cache for channel '{channel_id}' with API key hash '{enhanced_cache_data['api_key_hash'][:8]}...'"
        )

    def invalidate_cache_for_channel(
        self, channel_id: str, api_key: Optional[str] = None
    ):
        """ä½¿ç‰¹å®šæ¸ é“çš„ç¼“å­˜å¤±æ•ˆ"""
        if api_key:
            # æ¸…é™¤ç‰¹å®šAPI Keyçš„ç¼“å­˜
            cache_key = self.api_key_cache_manager.generate_cache_key(
                channel_id, api_key
            )
            if cache_key in self.model_cache:
                del self.model_cache[cache_key]
                logger.info(
                    f"Invalidated cache for channel '{channel_id}' with specific API key"
                )
        else:
            # æ¸…é™¤è¯¥æ¸ é“æ‰€æœ‰API Keyçš„ç¼“å­˜
            channel_cache_keys = (
                self.api_key_cache_manager.find_cache_entries_by_channel(
                    self.model_cache, channel_id
                )
            )
            for cache_key in channel_cache_keys:
                if cache_key in self.model_cache:
                    del self.model_cache[cache_key]

            # åŒæ—¶æ¸…é™¤å¯èƒ½å­˜åœ¨çš„æ—§æ ¼å¼ç¼“å­˜
            if channel_id in self.model_cache:
                del self.model_cache[channel_id]

            logger.info(
                f"Invalidated {len(channel_cache_keys)} cache entries for channel '{channel_id}'"
            )

    def update_channel_health(
        self, channel_id: str, success: bool, latency: Optional[float] = None
    ):
        """æ›´æ–°æ¸ é“å¥åº·çŠ¶æ€"""
        current_health = self.runtime_state.health_scores.get(channel_id, 1.0)

        if success:
            new_health = min(1.0, current_health * 1.01 + 0.01)
        else:
            new_health = max(0.0, current_health * 0.9 - 0.1)

        self.runtime_state.health_scores[channel_id] = new_health

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if channel_id not in self.runtime_state.channel_stats:
            self.runtime_state.channel_stats[channel_id] = {
                "request_count": 0,
                "success_count": 0,
                "total_latency": 0.0,
                "avg_latency_ms": 0.0,
            }

        stats = self.runtime_state.channel_stats[channel_id]
        stats["request_count"] = stats.get("request_count", 0) + 1

        if success:
            stats["success_count"] = stats.get("success_count", 0) + 1
            if latency is not None:
                stats["total_latency"] = stats.get("total_latency", 0.0) + latency
                stats["avg_latency_ms"] = (stats["total_latency"] * 1000) / stats[
                    "success_count"
                ]

        if new_health < 0.3:
            logger.warning(
                f"Channel {channel_id} health score is low: {new_health:.3f}"
            )

    def get_server_config(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡å™¨é…ç½®"""
        return {
            "host": self.config.server.host,
            "port": self.config.server.port,
            "debug": self.config.server.debug,
            "cors_origins": self.config.server.cors_origins,
        }

    def get_routing_config(self) -> Dict[str, Any]:
        """è·å–è·¯ç”±é…ç½®"""
        return {
            "default_strategy": self.config.routing.default_strategy,
            "enable_fallback": self.config.routing.enable_fallback,
            "max_retry_attempts": self.config.routing.max_retry_attempts,
        }

    def get_tasks_config(self) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡é…ç½®"""
        return {
            "model_discovery": {
                "enabled": self.config.tasks.model_discovery.enabled,
                "interval_hours": self.config.tasks.model_discovery.interval_hours,
                "run_on_startup": self.config.tasks.model_discovery.run_on_startup,
            },
            # ğŸ—‘ï¸ Removed pricing_discovery - was generating unused cache files
            "health_check": {
                "enabled": self.config.tasks.health_check.enabled,
                "interval_minutes": self.config.tasks.health_check.interval_minutes,
                "run_on_startup": self.config.tasks.health_check.run_on_startup,
            },
            "api_key_validation": {
                "enabled": self.config.tasks.api_key_validation.enabled,
                "interval_hours": self.config.tasks.api_key_validation.interval_hours,
                "run_on_startup": self.config.tasks.api_key_validation.run_on_startup,
            },
        }

    def get_system_config(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿé…ç½®"""
        return {
            "name": self.config.system.name,
            "version": self.config.system.version,
            "storage_mode": self.config.system.storage_mode,
        }

    def get_provider(self, provider_name: str) -> Optional[Provider]:
        """æ ¹æ®åç§°è·å–Provideré…ç½®"""
        provider = self.config.providers.get(provider_name)
        if provider:
            return provider

        # å¦‚æœæ‰¾ä¸åˆ°providerï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤çš„OpenAIå…¼å®¹é…ç½®
        logger.warning(
            f"Provider '{provider_name}' not found, using default OpenAI-compatible config"
        )
        return Provider(
            name=provider_name,
            display_name=provider_name.title(),
            adapter_class="OpenAIAdapter",
            base_url="https://api.openai.com",
            auth_type="bearer",
            rate_limit=60,
            capabilities=["text", "function_calling"],
        )

    # --- Simple mutation helpers for YAML-first workflow ---
    def set_channel_enabled(self, channel_id: str, enabled: bool) -> bool:
        """Enable/disable a channel and persist to YAML, then reload config.

        This edits the YAML file directly to keep truth in one place, then
        refreshes the in-memory Pydantic config and maps.
        """
        try:
            from copy import deepcopy

            import yaml as _yaml

            with open(self.config_path, "r", encoding="utf-8") as f:
                raw = _yaml.safe_load(f) or {}

            changed = False
            for ch in raw.get("channels", []):
                if ch.get("id") == channel_id:
                    ch["enabled"] = bool(enabled)
                    changed = True
                    break

            if not changed:
                logger.warning(
                    f"Channel '{channel_id}' not found in YAML; no changes applied"
                )
                return False

            # Save back and reload Pydantic config
            self._save_config_to_file(raw)
            self.config = self._load_and_validate_config()
            self.channels_map = {ch.id: ch for ch in self.config.channels}
            logger.info(
                f"Configuration updated: channel '{channel_id}' enabled={enabled}"
            )
            return True
        except Exception as e:
            logger.error(f"Failed to update channel enabled flag: {e}")
            return False

    def set_channel_priority(self, channel_id: str, priority: int) -> bool:
        """Set channel priority and persist to YAML, then reload config."""
        try:
            import yaml as _yaml

            with open(self.config_path, "r", encoding="utf-8") as f:
                raw = _yaml.safe_load(f) or {}

            changed = False
            for ch in raw.get("channels", []):
                if ch.get("id") == channel_id:
                    ch["priority"] = int(priority)
                    changed = True
                    break

            if not changed:
                logger.warning(
                    f"Channel '{channel_id}' not found in YAML; no changes applied"
                )
                return False

            self._save_config_to_file(raw)
            self.config = self._load_and_validate_config()
            self.channels_map = {ch.id: ch for ch in self.config.channels}
            logger.info(
                f"Configuration updated: channel '{channel_id}' priority={priority}"
            )
            return True
        except Exception as e:
            logger.error(f"Failed to update channel priority: {e}")
            return False

    @property
    def config_data(self) -> Dict[str, Any]:
        """ä¸ºå…¼å®¹æ€§æä¾›config_dataå±æ€§"""
        return {
            "system": self.get_system_config(),
            "server": self.get_server_config(),
            "providers": {k: v.dict() for k, v in self.config.providers.items()},
            "channels": [ch.dict() for ch in self.config.channels],
            "routing": self.get_routing_config(),
            "tasks": self.get_tasks_config(),
        }


# å…¨å±€é…ç½®åŠ è½½å™¨å®ä¾‹
_yaml_config_loader: Optional[YAMLConfigLoader] = None


def get_yaml_config_loader() -> YAMLConfigLoader:
    """è·å–å…¨å±€YAMLé…ç½®åŠ è½½å™¨å®ä¾‹"""
    global _yaml_config_loader
    if _yaml_config_loader is None:
        _yaml_config_loader = YAMLConfigLoader()
    return _yaml_config_loader
