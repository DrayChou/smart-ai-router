#!/usr/bin/env python3
"""
æ¨¡å‹å‘ç°ä»»åŠ¡ - è‡ªåŠ¨è·å–å„æ¸ é“çš„æ¨¡å‹åˆ—è¡¨å¹¶ç¼“å­˜
é›†æˆæ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼Œä¼˜åŒ–æ¨¡å‹å‘ç°é¢‘ç‡å’Œæ€§èƒ½
"""

import asyncio
import json
import logging
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Optional

import httpx

from core.utils.api_key_cache import get_api_key_cache_manager
from core.utils.async_file_ops import get_async_file_manager
from core.utils.channel_cache_manager import get_channel_cache_manager
from core.utils.api_key_cache_manager import get_api_key_cache_manager as get_api_key_level_cache_manager

logger = logging.getLogger(__name__)

class ModelDiscoveryTask:
    """æ¨¡å‹å‘ç°ä»»åŠ¡ç±»"""

    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.models_cache_file = self.cache_dir / "discovered_models.json"
        self.merged_config_file = self.cache_dir / "merged_config.json"
        self.discovery_log_file = self.cache_dir / "model_discovery_log.json"

        # å…¨å±€ç¼“å­˜å˜é‡
        self.cached_models: dict[str, dict] = {}
        self.merged_config: dict[str, Any] = {}
        self.last_update: Optional[datetime] = None

        # API Keyç¼“å­˜ç®¡ç†å™¨ (æ—§ç‰ˆï¼Œç”¨äºéªŒè¯)
        self.api_key_cache_manager = get_api_key_cache_manager()

        # æ¸ é“ç¼“å­˜ç®¡ç†å™¨ (å…¼å®¹æ€§ä¿ç•™)
        self.channel_cache_manager = get_channel_cache_manager()
        
        # API Keyçº§åˆ«ç¼“å­˜ç®¡ç†å™¨ (æ–°ç‰ˆï¼Œè§£å†³å®šä»·é—®é¢˜)
        self.api_key_level_cache_manager = get_api_key_level_cache_manager()

        # åŠ è½½ç°æœ‰ç¼“å­˜
        self._load_cache()

    def _load_cache(self):
        """åŠ è½½ç°æœ‰ç¼“å­˜æ•°æ®ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¸ºå…¼å®¹æ€§ä¿ç•™ï¼‰"""
        try:
            if self.models_cache_file.exists():
                with open(self.models_cache_file, encoding='utf-8') as f:
                    self.cached_models = json.load(f)
                logger.info(f"å·²åŠ è½½ç¼“å­˜çš„æ¨¡å‹æ•°æ®: {len(self.cached_models)} ä¸ªæ¸ é“")

            if self.merged_config_file.exists():
                with open(self.merged_config_file, encoding='utf-8') as f:
                    self.merged_config = json.load(f)
                logger.info("å·²åŠ è½½åˆå¹¶åçš„é…ç½®æ•°æ®")

        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

    async def _load_cache_async(self):
        """å¼‚æ­¥åŠ è½½ç°æœ‰ç¼“å­˜æ•°æ®"""
        try:
            file_manager = get_async_file_manager()

            if await file_manager.file_exists(self.models_cache_file):
                self.cached_models = await file_manager.read_json(self.models_cache_file, {})
                logger.info(f"å¼‚æ­¥åŠ è½½ç¼“å­˜çš„æ¨¡å‹æ•°æ®: {len(self.cached_models)} ä¸ªæ¸ é“")

            if await file_manager.file_exists(self.merged_config_file):
                self.merged_config = await file_manager.read_json(self.merged_config_file, {})
                logger.info("å¼‚æ­¥åŠ è½½åˆå¹¶åçš„é…ç½®æ•°æ®")

        except Exception as e:
            logger.error(f"å¼‚æ­¥åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜æ•°æ®ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¸ºå…¼å®¹æ€§ä¿ç•™ï¼‰"""
        try:
            # ä¿å­˜æ¨¡å‹ç¼“å­˜
            with open(self.models_cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cached_models, f, indent=2, ensure_ascii=False)

            # ä¿å­˜åˆå¹¶åçš„é…ç½®
            with open(self.merged_config_file, 'w', encoding='utf-8') as f:
                json.dump(self.merged_config, f, indent=2, ensure_ascii=False)

            logger.info(f"ç¼“å­˜å·²ä¿å­˜åˆ° {self.cache_dir}")

        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    async def _save_cache_async(self):
        """å¼‚æ­¥ä¿å­˜ç¼“å­˜æ•°æ®"""
        try:
            file_manager = get_async_file_manager()

            # å¼‚æ­¥ä¿å­˜æ¨¡å‹ç¼“å­˜
            models_success = await file_manager.write_json(
                self.models_cache_file,
                self.cached_models,
                indent=2
            )

            # ä½¿ç”¨ChannelCacheManagerä¿å­˜å„æ¸ é“è¯¦ç»†ç¼“å­˜ï¼ˆå¸¦æ¨¡å‹åˆ†æï¼‰
            for channel_id, cache_data in self.cached_models.items():
                # ä½¿ç”¨ChannelCacheManagerè¿›è¡Œæ™ºèƒ½æ¨¡å‹åˆ†æå’Œç¼“å­˜
                self.channel_cache_manager.save_channel_models(channel_id, cache_data)

            # å¼‚æ­¥ä¿å­˜åˆå¹¶åçš„é…ç½®
            config_success = await file_manager.write_json(
                self.merged_config_file,
                self.merged_config,
                indent=2
            )

            if models_success and config_success:
                logger.info(f"å¼‚æ­¥ç¼“å­˜å·²ä¿å­˜åˆ° {self.cache_dir}")
            else:
                logger.warning("éƒ¨åˆ†å¼‚æ­¥ç¼“å­˜ä¿å­˜å¤±è´¥")

        except Exception as e:
            logger.error(f"å¼‚æ­¥ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _get_models_endpoint(self, base_url: str) -> str:
        """æ ¹æ®base_urlè‡ªé€‚åº”è®¡ç®—modelsç«¯ç‚¹"""
        base_url = base_url.rstrip('/')

        # å¸¸è§çš„æ¨¡å¼åŒ¹é…
        patterns = [
            # OpenAIæ ‡å‡†æ ¼å¼
            ("/v1", "/v1/models"),
            ("/openai/v1", "/openai/v1/models"),

            # ç‰¹æ®Šå‚å•†æ ¼å¼
            ("", "/v1/models"),  # é»˜è®¤æ·»åŠ v1
        ]

        for pattern, endpoint in patterns:
            if base_url.endswith(pattern):
                if pattern:
                    return base_url.replace(pattern, endpoint)
                else:
                    return f"{base_url}/v1/models"

        # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œå°è¯•ç›´æ¥æ·»åŠ 
        if not base_url.endswith('/v1'):
            return f"{base_url}/v1/models"
        else:
            return f"{base_url}/models"

    def _get_fallback_models_from_config(self, channel: dict[str, Any]) -> list[str]:
        """ä»é…ç½®æ–‡ä»¶è·å–æ¸ é“çš„æ¨¡å‹åˆ—è¡¨ä½œä¸ºå›é€€"""
        try:
            # ä»é…ç½®çš„æ¨¡å‹åˆ—è¡¨ä¸­è·å–
            configured_models = channel.get('configured_models', [])
            logger.info(f"æ¸ é“ {channel.get('id')} å›é€€æ£€æŸ¥: configured_models={configured_models}, model_name={channel.get('model_name')}")

            if configured_models:
                logger.info(f"ä½¿ç”¨é…ç½®çš„æ¨¡å‹åˆ—è¡¨ä½œä¸ºå›é€€: {channel.get('id')} è·å¾— {len(configured_models)} ä¸ªæ¨¡å‹")
                return configured_models

            # å¦‚æœæ²¡æœ‰é…ç½®æ¨¡å‹åˆ—è¡¨ï¼Œä½†æœ‰model_nameï¼Œåˆ™è¿”å›å•ä¸ªæ¨¡å‹
            model_name = channel.get('model_name')
            if model_name and model_name != 'auto':
                logger.info(f"ä½¿ç”¨å•ä¸ªæ¨¡å‹ä½œä¸ºå›é€€: {channel.get('id')} æ¨¡å‹ {model_name}")
                return [model_name]

            logger.warning(f"æ¸ é“ {channel.get('id')} æ²¡æœ‰å¯ç”¨çš„å›é€€æ¨¡å‹é…ç½®")
            return []

        except Exception as e:
            logger.warning(f"ä»é…ç½®è·å–æ¨¡å‹å›é€€å¤±è´¥: {e}")
            return []

    async def _fetch_models_from_channel(self, channel: dict[str, Any]) -> Optional[dict[str, Any]]:
        """ä»å•ä¸ªæ¸ é“è·å–æ¨¡å‹åˆ—è¡¨"""
        channel_id = channel.get('id')
        provider = channel.get('provider')
        base_url = channel.get('base_url')
        api_key = channel.get('api_key')

        # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥æ¸ é“é…ç½®
        if channel_id in ['oneapi_13', 'oneapi_33', 'oneapi_55']:
            logger.info(f"è°ƒè¯•æ¸ é“ {channel_id}: configured_models={channel.get('configured_models')}, keys={list(channel.keys())}")

        if not all([base_url, api_key]):
            logger.warning(f"æ¸ é“ {channel_id} ç¼ºå°‘å¿…è¦ä¿¡æ¯ï¼Œè·³è¿‡")
            return None

        # è·å–æ¨¡å‹ç«¯ç‚¹URL
        models_url = self._get_models_endpoint(base_url)

        # æ„å»ºè¯·æ±‚å¤´
        headers = {
            "User-Agent": "smart-ai-router/0.1.0",
            "Accept": "application/json"
        }

        # æ ¹æ®providerç±»å‹è®¾ç½®è®¤è¯å¤´
        if provider in ['openai', 'burn_hair', 'groq', 'siliconflow']:
            headers["Authorization"] = f"Bearer {api_key}"
        elif provider == 'anthropic':
            headers["x-api-key"] = api_key
        else:
            # é»˜è®¤ä½¿ç”¨Bearerè®¤è¯
            headers["Authorization"] = f"Bearer {api_key}"

        try:
            timeout = httpx.Timeout(30.0)
            async with httpx.AsyncClient(timeout=timeout) as client:
                logger.info(f"æ­£åœ¨è·å– {channel_id} çš„æ¨¡å‹åˆ—è¡¨: {models_url}")

                response = await client.get(models_url, headers=headers)

                if response.status_code == 200:
                    data = response.json()

                    # è§£æä¸åŒæ ¼å¼çš„å“åº”å¹¶æå–è¯¦ç»†ä¿¡æ¯
                    models = []
                    models_data = {}

                    if 'data' in data and isinstance(data['data'], list):
                        # OpenAIæ ‡å‡†æ ¼å¼
                        for model in data['data']:
                            model_id = model.get('id')
                            if model_id:
                                models.append(model_id)
                                # æå–è¯¦ç»†çš„æ¨¡å‹ä¿¡æ¯
                                models_data[model_id] = self._extract_model_info(model)
                    elif isinstance(data, list):
                        # ç›´æ¥æ˜¯æ¨¡å‹åˆ—è¡¨
                        for model in data:
                            if isinstance(model, dict):
                                model_id = model.get('id')
                                if model_id:
                                    models.append(model_id)
                                    models_data[model_id] = self._extract_model_info(model)
                            else:
                                models.append(str(model))
                    elif 'models' in data:
                        # å…¶ä»–æ ¼å¼
                        models_list = data['models']
                        if isinstance(models_list, list):
                            for model in models_list:
                                if isinstance(model, dict):
                                    model_id = model.get('id')
                                    if model_id:
                                        models.append(model_id)
                                        models_data[model_id] = self._extract_model_info(model)
                                else:
                                    models.append(str(model))

                    # ğŸš€ æå–æ¨¡å‹å®šä»·ä¿¡æ¯ç”¨äºå¤šProviderå…è´¹ç­–ç•¥
                    models_pricing = self._extract_models_pricing(models_data, data, provider)

                    result = {
                        'channel_id': channel_id,
                        'provider': provider,
                        'base_url': base_url,
                        'models_url': models_url,
                        'models': models,
                        'models_data': models_data,  # æ·»åŠ è¯¦ç»†çš„æ¨¡å‹æ•°æ®
                        'models_pricing': models_pricing,  # ğŸš€ æ–°å¢ï¼šæ¯ä¸ªæ¨¡å‹çš„å®šä»·ä¿¡æ¯
                        'model_count': len(models),
                        'last_updated': datetime.now().isoformat(),
                        'status': 'success',
                        'api_key': api_key,  # ä¿å­˜API Keyç”¨äºç¼“å­˜
                        'user_level': self._detect_user_level(models, provider),
                        'response_data': data  # ä¿å­˜åŸå§‹å“åº”ç”¨äºè°ƒè¯•
                    }

                    logger.info(f"æˆåŠŸè·å– {channel_id} çš„ {len(models)} ä¸ªæ¨¡å‹")
                    return result

                else:
                    logger.warning(f"è·å– {channel_id} æ¨¡å‹å¤±è´¥: {response.status_code} - {response.text[:100]}")
                    # å°è¯•ä»é…ç½®æ–‡ä»¶å›é€€
                    fallback_models = self._get_fallback_models_from_config(channel)
                    if fallback_models:
                        return {
                            'channel_id': channel_id,
                            'provider': provider,
                            'base_url': base_url,
                            'models_url': models_url,
                            'models': fallback_models,
                            'model_count': len(fallback_models),
                            'last_updated': datetime.now().isoformat(),
                            'status': 'success_fallback',
                            'note': f'Fallback to configured models due to HTTP {response.status_code}'
                        }
                    else:
                        return {
                            'channel_id': channel_id,
                            'provider': provider,
                            'base_url': base_url,
                            'models_url': models_url,
                            'models': [],
                            'model_count': 0,
                            'last_updated': datetime.now().isoformat(),
                            'status': 'error',
                            'error': f"HTTP {response.status_code}: {response.text[:200]}"
                        }

        except Exception as e:
            logger.warning(f"è·å– {channel_id} æ¨¡å‹æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            # å°è¯•ä»é…ç½®æ–‡ä»¶å›é€€
            fallback_models = self._get_fallback_models_from_config(channel)
            if fallback_models:
                return {
                    'channel_id': channel_id,
                    'provider': provider,
                    'base_url': base_url,
                    'models_url': models_url,
                    'models': fallback_models,
                    'model_count': len(fallback_models),
                    'last_updated': datetime.now().isoformat(),
                    'status': 'success_fallback',
                    'note': f'Fallback to configured models due to exception: {str(e)}'
                }
            else:
                return {
                    'channel_id': channel_id,
                    'provider': provider,
                    'base_url': base_url,
                    'models_url': models_url,
                    'models': [],
                    'model_count': 0,
                    'last_updated': datetime.now().isoformat(),
                    'status': 'error',
                    'error': str(e)
                }

    async def discover_models(self, channels: list[dict[str, Any]]) -> dict[str, dict]:
        """å‘ç°æ‰€æœ‰æ¸ é“çš„æ¨¡å‹"""
        logger.info(f"å¼€å§‹æ¨¡å‹å‘ç°ä»»åŠ¡ï¼Œå…± {len(channels)} ä¸ªæ¸ é“")

        # è¿‡æ»¤å¯ç”¨çš„æ¸ é“
        enabled_channels = [ch for ch in channels if ch.get('enabled', True)]
        logger.info(f"å¯ç”¨çš„æ¸ é“æ•°é‡: {len(enabled_channels)}")

        # å¹¶å‘è·å–æ¨¡å‹
        tasks = []
        for channel in enabled_channels:
            task = self._fetch_models_from_channel(channel)
            tasks.append(task)

        # æ‰§è¡Œå¹¶å‘è¯·æ±‚
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # æ•´ç†ç»“æœ
        discovered_models = {}
        successful_count = 0
        failed_count = 0

        for result in results:
            if isinstance(result, Exception):
                logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {result}")
                failed_count += 1
                continue

            if result:
                channel_id = result['channel_id']
                discovered_models[channel_id] = result

                if result['status'] == 'success':
                    successful_count += 1
                    
                    # ä¿å­˜åˆ°API Keyçº§åˆ«ç¼“å­˜ (æ–°æ¶æ„)
                    try:
                        # ä»ç»“æœä¸­è·å–API Keyä¿¡æ¯
                        api_key = result.get('api_key', '')
                        provider = result.get('provider', 'unknown')
                        
                        self.api_key_level_cache_manager.save_api_key_models(
                            channel_id, api_key, result, provider
                        )
                        logger.debug(f"âœ… Saved API key level cache for {channel_id}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Failed to save API key level cache for {channel_id}: {e}")
                    
                    # å…¼å®¹æ€§ï¼šä»ç„¶ä¿å­˜åˆ°æ—§çš„æ¸ é“ç¼“å­˜
                    try:
                        self.channel_cache_manager.save_channel_models(channel_id, result)
                        logger.debug(f"âœ… Also saved to legacy channel cache for {channel_id}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Failed to save legacy channel cache for {channel_id}: {e}")
                else:
                    failed_count += 1

        # è®°å½•å‘ç°æ—¥å¿—
        discovery_log = {
            'timestamp': datetime.now().isoformat(),
            'total_channels': len(channels),
            'enabled_channels': len(enabled_channels),
            'successful_requests': successful_count,
            'failed_requests': failed_count,
            'discovered_models': discovered_models
        }

        # ä¿å­˜å‘ç°æ—¥å¿—
        try:
            with open(self.discovery_log_file, 'w', encoding='utf-8') as f:
                json.dump(discovery_log, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜å‘ç°æ—¥å¿—å¤±è´¥: {e}")

        logger.info(f"æ¨¡å‹å‘ç°å®Œæˆ: æˆåŠŸ {successful_count}, å¤±è´¥ {failed_count}")

        return discovered_models

    def merge_with_config(self, original_config_data: dict[str, Any], discovered_models: dict[str, dict]) -> dict[str, Any]:
        """å°†å‘ç°çš„æ¨¡å‹ä¸åŸå§‹é…ç½®æ•°æ®åˆå¹¶"""
        merged = original_config_data.copy()

        # æ›´æ–°æ¸ é“ä¿¡æ¯
        if 'channels' in merged:
            for channel in merged['channels']:
                channel_id = channel.get('id')
                if channel_id in discovered_models:
                    model_info = discovered_models[channel_id]

                    # æ·»åŠ å‘ç°çš„æ¨¡å‹ä¿¡æ¯
                    channel['discovered_models'] = model_info['models']
                    channel['model_count'] = model_info['model_count']
                    channel['models_last_updated'] = model_info['last_updated']
                    channel['discovery_status'] = model_info['status']

                    # å¦‚æœå‘ç°äº†æ¨¡å‹ä½†é…ç½®ä¸­æ²¡æœ‰model_nameï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹
                    if not channel.get('model_name') and model_info['models']:
                        channel['suggested_model'] = model_info['models'][0]

                    # éªŒè¯é…ç½®çš„æ¨¡å‹æ˜¯å¦åœ¨å‘ç°çš„æ¨¡å‹åˆ—è¡¨ä¸­
                    config_model = channel.get('model_name')
                    if config_model and config_model not in model_info['models']:
                        channel['model_validation_warning'] = f"é…ç½®çš„æ¨¡å‹ '{config_model}' ä¸åœ¨å‘ç°çš„æ¨¡å‹åˆ—è¡¨ä¸­"

        # æ·»åŠ å‘ç°ç»Ÿè®¡ä¿¡æ¯
        merged['model_discovery'] = {
            'last_updated': datetime.now().isoformat(),
            'total_channels_discovered': len(discovered_models),
            'total_models_discovered': sum(info['model_count'] for info in discovered_models.values()),
            'discovery_summary': {
                channel_id: {
                    'model_count': info['model_count'],
                    'status': info['status'],
                    'provider': info.get('provider', 'unknown') # ç¡®ä¿providerå­˜åœ¨
                }
                for channel_id, info in discovered_models.items()
            }
        }

        return merged

    async def run_discovery_task(self, channels: list[dict[str, Any]], original_config_data: dict[str, Any]):
        """è¿è¡Œå®Œæ•´çš„æ¨¡å‹å‘ç°ä»»åŠ¡"""
        logger.info("å¯åŠ¨æ¨¡å‹å‘ç°ä»»åŠ¡")

        try:
            # å‘ç°æ¨¡å‹
            discovered_models = await self.discover_models(channels)

            # æ›´æ–°ç¼“å­˜
            self.cached_models.update(discovered_models)

            # åˆå¹¶é…ç½®
            self.merged_config = self.merge_with_config(original_config_data, discovered_models)

            # ä¿å­˜ç¼“å­˜
            self._save_cache()

            # æ›´æ–°æ—¶é—´æˆ³
            self.last_update = datetime.now()

            logger.info(f"æ¨¡å‹å‘ç°ä»»åŠ¡å®Œæˆï¼Œå…±å‘ç° {len(discovered_models)} ä¸ªæ¸ é“çš„æ¨¡å‹")

            return {
                'success': True,
                'discovered_channels': len(discovered_models),
                'total_models': sum(info['model_count'] for info in discovered_models.values()),
                'last_update': self.last_update.isoformat()
            }

        except Exception as e:
            logger.error(f"æ¨¡å‹å‘ç°ä»»åŠ¡å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'last_update': datetime.now().isoformat()
            }

    def get_cached_models(self, max_age_hours: int = 24) -> Optional[dict[str, dict]]:
        """è·å–ç¼“å­˜çš„æ¨¡å‹æ•°æ®ï¼ˆå¦‚æœæœªè¿‡æœŸï¼‰"""
        if not self.last_update:
            return None

        age = datetime.now() - self.last_update
        if age > timedelta(hours=max_age_hours):
            logger.info(f"ç¼“å­˜å·²è¿‡æœŸ (è¶…è¿‡ {max_age_hours} å°æ—¶)")
            return None

        return self.cached_models

    def _extract_model_info(self, model_data: dict[str, Any]) -> dict[str, Any]:
        """ä»æ¨¡å‹æ•°æ®ä¸­æå–è¯¦ç»†ä¿¡æ¯

        Args:
            model_data: æ¨¡å‹çš„åŸå§‹æ•°æ®

        Returns:
            åŒ…å«è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        model_info = {
            'id': model_data.get('id'),
            'object': model_data.get('object', 'model'),
            'created': model_data.get('created'),
            'owned_by': model_data.get('owned_by'),
        }

        # æå–ä¸Šä¸‹æ–‡é•¿åº¦ä¿¡æ¯
        context_length = None
        if 'context_length' in model_data:
            context_length = model_data['context_length']
        elif 'top_provider' in model_data and 'context_length' in model_data['top_provider']:
            context_length = model_data['top_provider']['context_length']

        if context_length:
            model_info['context_length'] = context_length
            model_info['max_input_tokens'] = context_length

        # æå–æœ€å¤§è¾“å‡ºä»¤ç‰Œæ•°
        max_completion_tokens = None
        if 'max_completion_tokens' in model_data:
            max_completion_tokens = model_data['max_completion_tokens']
        elif 'top_provider' in model_data and 'max_completion_tokens' in model_data['top_provider']:
            max_completion_tokens = model_data['top_provider']['max_completion_tokens']

        if max_completion_tokens:
            model_info['max_output_tokens'] = max_completion_tokens

        # æå–å‚æ•°ä¿¡æ¯ï¼ˆä»æè¿°æˆ–å…¶ä»–å­—æ®µï¼‰
        parameter_info = self._extract_parameter_info(model_data)
        if parameter_info:
            model_info.update(parameter_info)

        # æå–å®šä»·ä¿¡æ¯
        if 'pricing' in model_data:
            pricing = model_data['pricing']
            model_info['pricing'] = {
                'prompt': pricing.get('prompt'),
                'completion': pricing.get('completion'),
                'request': pricing.get('request'),
            }

        # æå–æ¶æ„ä¿¡æ¯
        if 'architecture' in model_data:
            model_info['architecture'] = model_data['architecture']

        return model_info

    def _extract_parameter_info(self, model_data: dict[str, Any]) -> dict[str, Any]:
        """ä»æ¨¡å‹æ•°æ®ä¸­æå–å‚æ•°ä¿¡æ¯

        Args:
            model_data: æ¨¡å‹çš„åŸå§‹æ•°æ®

        Returns:
            åŒ…å«å‚æ•°ä¿¡æ¯çš„å­—å…¸
        """
        # ä»æè¿°ä¸­æå–å‚æ•°ä¿¡æ¯
        description = model_data.get('description', '')
        name = model_data.get('name', '')
        id = model_data.get('id', '')

        # åˆå¹¶æ‰€æœ‰å¯èƒ½çš„æ–‡æœ¬æ¥æº
        all_texts = [description, name, id]

        # å‚æ•°å¤§å°æ¨¡å¼
        param_patterns = [
            r'(\d+\.?\d*)\s*[Bb](?:illion)?\s*(?:parameter|mixture-of-experts)?',
            r'(\d+\.?\d*)\s*[Mm](?:illion)?\s*parameter',
            r'(\d+\.?\d*)\s*[Bb]\s*param',
            r'(\d+\.?\d*)[Bb](?![A-Za-z])',  # é¿å…åŒ¹é… "bit", "byte"
        ]

        for text in all_texts:
            for pattern in param_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    try:
                        param_value = float(match.group(1))
                        # æ ¹æ®å•ä½è½¬æ¢
                        if 'b' in match.group(0).lower() and 'm' not in match.group(0).lower():
                            # billions
                            param_count = int(param_value * 1000)  # è½¬æ¢ä¸ºç™¾ä¸‡å•ä½
                        else:
                            # millions
                            param_count = int(param_value)

                        return {
                            'parameter_count': param_count,
                            'parameter_size_text': match.group(0)
                        }
                    except (ValueError, IndexError):
                        continue

        # ä»æ¨¡å‹IDä¸­æå–å‚æ•°ä¿¡æ¯
        id_patterns = [
            r'-(\d+\.?\d*)[Bb]-',
            r'_(\d+\.?\d*)[Bb]_',
            r'(\d+\.?\d*)[Bb](?![A-Za-z])',
        ]

        for pattern in id_patterns:
            match = re.search(pattern, id)
            if match:
                try:
                    param_value = float(match.group(1))
                    return {
                        'parameter_count': int(param_value * 1000),  # è½¬æ¢ä¸ºç™¾ä¸‡å•ä½
                        'parameter_size_text': f"{param_value}b"
                    }
                except (ValueError, IndexError):
                    continue

        return {}

    def get_merged_config(self) -> dict[str, Any]:
        """è·å–åˆå¹¶åçš„é…ç½®"""
        return self.merged_config.copy()

    def is_cache_valid(self, max_age_hours: int = 24) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not self.last_update:
            return False

        age = datetime.now() - self.last_update
        return age <= timedelta(hours=max_age_hours)

    def _extract_models_pricing(self, models_data: dict, response_data: dict, provider: str) -> dict[str, dict]:
        """æå–æ¨¡å‹å®šä»·ä¿¡æ¯ç”¨äºå¤šProviderå…è´¹ç­–ç•¥
        
        Args:
            models_data: æ¨¡å‹è¯¦ç»†æ•°æ®å­—å…¸
            response_data: APIåŸå§‹å“åº”æ•°æ®
            provider: æä¾›å•†åç§°
            
        Returns:
            æ¨¡å‹å®šä»·å­—å…¸ {model_id: {'prompt': price, 'completion': price, 'is_free': bool}}
        """
        pricing_info = {}
        
        try:
            # æ–¹æ³•1: ä»models_dataä¸­æå–è¯¦ç»†å®šä»·
            for model_id, model_data in models_data.items():
                if 'pricing' in model_data:
                    pricing = model_data['pricing']
                    prompt_price = pricing.get('prompt', 0)
                    completion_price = pricing.get('completion', 0)
                    
                    # è½¬æ¢å­—ç¬¦ä¸²ä»·æ ¼åˆ°æ•°å€¼
                    try:
                        prompt_price = float(prompt_price) if prompt_price else 0
                        completion_price = float(completion_price) if completion_price else 0
                    except (ValueError, TypeError):
                        prompt_price = completion_price = 0
                    
                    is_free = (prompt_price == 0 and completion_price == 0)
                    
                    pricing_info[model_id] = {
                        'prompt': prompt_price,
                        'completion': completion_price,
                        'is_free': is_free,
                        'provider': provider
                    }
            
            # æ–¹æ³•2: OpenRouterç‰¹æ®Šå¤„ç† - ä»dataå­—æ®µç›´æ¥æå–
            if provider == 'openrouter' and 'data' in response_data:
                for model in response_data['data']:
                    if isinstance(model, dict):
                        model_id = model.get('id')
                        if model_id and 'pricing' in model:
                            pricing = model['pricing']
                            prompt_price = pricing.get('prompt', 0)
                            completion_price = pricing.get('completion', 0)
                            
                            # è½¬æ¢å­—ç¬¦ä¸²ä»·æ ¼åˆ°æ•°å€¼
                            try:
                                prompt_price = float(prompt_price) if prompt_price else 0
                                completion_price = float(completion_price) if completion_price else 0
                            except (ValueError, TypeError):
                                prompt_price = completion_price = 0
                            
                            is_free = (prompt_price == 0 and completion_price == 0)
                            
                            # å¦‚æœè¿˜æ²¡æœ‰å®šä»·ä¿¡æ¯æˆ–è¿™æ¬¡æ£€æµ‹åˆ°å…è´¹ï¼Œæ›´æ–°ä¿¡æ¯
                            if model_id not in pricing_info or is_free:
                                pricing_info[model_id] = {
                                    'prompt': prompt_price,
                                    'completion': completion_price,
                                    'is_free': is_free,
                                    'provider': provider
                                }
            
            logger.debug(f"æå–åˆ° {len(pricing_info)} ä¸ªæ¨¡å‹çš„å®šä»·ä¿¡æ¯ (Provider: {provider})")
            free_models = [mid for mid, info in pricing_info.items() if info.get('is_free')]
            if free_models:
                logger.info(f"ğŸ†“ å‘ç°å…è´¹æ¨¡å‹ ({provider}): {len(free_models)} ä¸ª")
                
        except Exception as e:
            logger.warning(f"æå–å®šä»·ä¿¡æ¯å¤±è´¥ (Provider: {provider}): {e}")
        
        return pricing_info

    def _detect_user_level(self, models: list[str], provider: str) -> str:
        """æ£€æµ‹ç”¨æˆ·ç­‰çº§ï¼ˆåŸºäºæ¨¡å‹åˆ—è¡¨å’Œæä¾›å•†ï¼‰"""
        if not models:
            return 'unknown'

        # SiliconFlowç”¨æˆ·ç­‰çº§æ£€æµ‹
        if provider == 'siliconflow':
            pro_models = [m for m in models if 'Pro/' in m or '/Pro' in m]
            if pro_models:
                return 'pro'
            return 'free'

        # OpenRouterç”¨æˆ·ç­‰çº§æ£€æµ‹
        if provider == 'openrouter':
            model_count = len(models)
            if model_count > 100:
                return 'premium'
            elif model_count > 50:
                return 'pro'
            return 'free'

        # Groqç”¨æˆ·ç­‰çº§æ£€æµ‹
        if provider == 'groq':
            # Groqçš„å…è´¹æ¨¡å‹é€šå¸¸è¾ƒå°‘
            return 'free' if len(models) <= 10 else 'pro'

        # å…¶ä»–æä¾›å•†çš„é»˜è®¤æ£€æµ‹é€»è¾‘
        model_count = len(models)
        if model_count > 50:
            return 'premium'
        elif model_count > 20:
            return 'pro'
        elif model_count > 0:
            return 'free'

        return 'unknown'


# å…¨å±€å®ä¾‹
_model_discovery_task = None

def get_model_discovery_task() -> ModelDiscoveryTask:
    """è·å–å…¨å±€æ¨¡å‹å‘ç°ä»»åŠ¡å®ä¾‹"""
    global _model_discovery_task
    if _model_discovery_task is None:
        _model_discovery_task = ModelDiscoveryTask()
    return _model_discovery_task


async def run_model_discovery(channels: list[dict[str, Any]], config: dict[str, Any]) -> dict[str, Any]:
    """è¿è¡Œæ¨¡å‹å‘ç°ä»»åŠ¡çš„ä¾¿æ·å‡½æ•°"""
    task = get_model_discovery_task()
    return await task.run_discovery_task(channels, config)


def get_cached_models(max_age_hours: int = 24) -> Optional[dict[str, dict]]:
    """è·å–ç¼“å­˜æ¨¡å‹çš„ä¾¿æ·å‡½æ•°"""
    task = get_model_discovery_task()
    return task.get_cached_models(max_age_hours)


def get_merged_config() -> dict[str, Any]:
    """è·å–åˆå¹¶é…ç½®çš„ä¾¿æ·å‡½æ•°"""
    task = get_model_discovery_task()
    return task.get_merged_config()
