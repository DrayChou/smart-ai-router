"""
åŸºäºJSONé…ç½®çš„è½»é‡è·¯ç”±å¼•æ“
"""
import random
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
import time
import logging

from .yaml_config import YAMLConfigLoader, get_yaml_config_loader
from .config_models import Channel

logger = logging.getLogger(__name__)

class TagNotFoundError(Exception):
    """æ ‡ç­¾æœªæ‰¾åˆ°é”™è¯¯"""
    def __init__(self, tags: List[str], message: str = None):
        self.tags = tags
        if message is None:
            if len(tags) == 1:
                message = f"æ²¡æœ‰æ‰¾åˆ°åŒ¹é…æ ‡ç­¾ '{tags[0]}' çš„æ¨¡å‹"
            else:
                message = f"æ²¡æœ‰æ‰¾åˆ°åŒæ—¶åŒ¹é…æ ‡ç­¾ {tags} çš„æ¨¡å‹"
        super().__init__(message)

@dataclass
class RoutingScore:
    """è·¯ç”±è¯„åˆ†ç»“æœ"""
    channel: Channel
    total_score: float
    cost_score: float
    speed_score: float
    quality_score: float
    reliability_score: float
    reason: str
    matched_model: Optional[str] = None  # å¯¹äºæ ‡ç­¾è·¯ç”±ï¼Œè®°å½•å®é™…åŒ¹é…çš„æ¨¡å‹

@dataclass
class ChannelCandidate:
    """å€™é€‰æ¸ é“ä¿¡æ¯"""
    channel: Channel
    matched_model: Optional[str] = None  # å¯¹äºæ ‡ç­¾è·¯ç”±ï¼Œè®°å½•å®é™…åŒ¹é…çš„æ¨¡å‹

@dataclass
class RoutingRequest:
    """è·¯ç”±è¯·æ±‚"""
    model: str
    messages: List[Dict[str, Any]]
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    stream: bool = False
    functions: Optional[List[Dict[str, Any]]] = None
    required_capabilities: List[str] = None

class JSONRouter:
    """åŸºäºPydanticéªŒè¯åé…ç½®çš„è·¯ç”±å™¨"""
    
    def __init__(self, config_loader: Optional[YAMLConfigLoader] = None):
        self.config_loader = config_loader or get_yaml_config_loader()
        self.config = self.config_loader.config
        
        # æ ‡ç­¾ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
        self._tag_cache: Dict[str, List[str]] = {}
        self._available_tags_cache: Optional[set] = None
        self._available_models_cache: Optional[List[str]] = None
        
    def route_request(self, request: RoutingRequest) -> List[RoutingScore]:
        """
        è·¯ç”±è¯·æ±‚ï¼Œè¿”å›æŒ‰è¯„åˆ†æ’åºçš„å€™é€‰æ¸ é“åˆ—è¡¨ã€‚
        """
        logger.info(f"ğŸš€ ROUTING START: Processing request for model '{request.model}'")
        try:
            # ç¬¬ä¸€æ­¥ï¼šè·å–å€™é€‰æ¸ é“
            logger.info(f"ğŸ” STEP 1: Finding candidate channels...")
            candidates = self._get_candidate_channels(request)
            if not candidates:
                logger.warning(f"âŒ ROUTING FAILED: No suitable channels found for model '{request.model}'")
                return []
            
            logger.info(f"âœ… STEP 1 COMPLETE: Found {len(candidates)} candidate channels")
            
            # ç¬¬äºŒæ­¥ï¼šè¿‡æ»¤æ¸ é“
            logger.info(f"ğŸ”§ STEP 2: Filtering channels by health and availability...")
            filtered_candidates = self._filter_channels(candidates, request)
            if not filtered_candidates:
                logger.warning(f"âŒ ROUTING FAILED: No available channels after filtering for model '{request.model}'")
                return []
            
            logger.info(f"âœ… STEP 2 COMPLETE: {len(filtered_candidates)} channels passed filtering (filtered out {len(candidates) - len(filtered_candidates)})")
            
            # ç¬¬ä¸‰æ­¥ï¼šè¯„åˆ†å’Œæ’åº
            logger.info(f"ğŸ¯ STEP 3: Scoring and ranking channels...")
            scored_channels = self._score_channels(filtered_candidates, request)
            if not scored_channels:
                logger.warning(f"âŒ ROUTING FAILED: Failed to score any channels for model '{request.model}'")
                return []
            
            logger.info(f"âœ… STEP 3 COMPLETE: Scored {len(scored_channels)} channels")
            logger.info(f"ğŸ‰ ROUTING SUCCESS: Ready to attempt {len(scored_channels)} channels in ranked order for model '{request.model}'")
            
            return scored_channels
            
        except TagNotFoundError:
            # è®©TagNotFoundErrorä¼ æ’­å‡ºå»ï¼Œä»¥ä¾¿ä¸Šå±‚å¤„ç†
            raise
        except Exception as e:
            logger.error(f"âŒ ROUTING ERROR: Request failed for model '{request.model}': {e}", exc_info=True)
            return []
    
    def _get_candidate_channels(self, request: RoutingRequest) -> List[ChannelCandidate]:
        """è·å–å€™é€‰æ¸ é“ï¼Œæ”¯æŒæŒ‰æ ‡ç­¾é›†åˆæˆ–ç‰©ç†æ¨¡å‹è¿›è¡Œæ™ºèƒ½è·¯ç”±"""
        
        if request.model.startswith("tag:"):
            tag_query = request.model.split(":", 1)[1]
            
            # æ”¯æŒå¤šæ ‡ç­¾æŸ¥è¯¢ï¼Œç”¨é€—å·åˆ†éš”ï¼štag:qwen,free
            if "," in tag_query:
                tags = [tag.strip().lower() for tag in tag_query.split(",")]
                logger.info(f"ğŸ·ï¸  TAG ROUTING: Processing multi-tag query '{request.model}' -> tags: {tags}")
                candidates = self._get_candidate_channels_by_auto_tags(tags)
                if not candidates:
                    logger.error(f"âŒ TAG NOT FOUND: No models found matching all tags {tags}")
                    raise TagNotFoundError(tags)
                logger.info(f"ğŸ·ï¸  TAG ROUTING: Multi-tag query found {len(candidates)} candidate channels")
                return candidates
            else:
                # å•æ ‡ç­¾æŸ¥è¯¢ - ç›´æ¥ä»æ¨¡å‹å‘ç°ç¼“å­˜ä¸­æŸ¥æ‰¾
                tag = tag_query.strip().lower()
                logger.info(f"ğŸ·ï¸  TAG ROUTING: Processing single tag query '{request.model}' -> tag: '{tag}'")
                
                candidates = self._get_candidate_channels_by_auto_tags([tag])
                if not candidates:
                    logger.error(f"âŒ TAG NOT FOUND: No models found matching tag '{tag}'")
                    raise TagNotFoundError([tag])
                logger.info(f"ğŸ·ï¸  TAG ROUTING: Found {len(candidates)} candidate channels for tag '{tag}'")
                return candidates
        
        # étag:å‰ç¼€çš„æ¨¡å‹åç§° - é¦–å…ˆå°è¯•ç‰©ç†æ¨¡å‹ï¼Œç„¶åå°è¯•è‡ªåŠ¨æ ‡ç­¾åŒ–
        candidate_channels = []
        all_enabled_channels = self.config_loader.get_enabled_channels()
        model_cache = self.config_loader.get_model_cache()

        # 1. é¦–å…ˆå°è¯•ä½œä¸ºç‰©ç†æ¨¡å‹æŸ¥æ‰¾
        for channel in all_enabled_channels:
            if channel.id in model_cache:
                discovered_info = model_cache[channel.id]
                if request.model in discovered_info.get("models", []):
                    # å¯¹äºç‰©ç†æ¨¡å‹ï¼Œmatched_model å°±æ˜¯è¯·æ±‚çš„æ¨¡å‹
                    candidate_channels.append(ChannelCandidate(
                        channel=channel,
                        matched_model=request.model
                    ))
        
        if candidate_channels:
            logger.info(f"Found {len(candidate_channels)} candidate channels for physical model '{request.model}'")
            return candidate_channels

        # 2. å¦‚æœä½œä¸ºç‰©ç†æ¨¡å‹æ²¡æ‰¾åˆ°ï¼Œå°è¯•è‡ªåŠ¨æ ‡ç­¾åŒ–
        logger.info(f"ğŸ”„ AUTO TAGGING: No physical model found for '{request.model}', trying automatic tag extraction")
        auto_tags = self._extract_tags_from_model_name(request.model)
        if auto_tags:
            logger.info(f"ğŸ·ï¸  AUTO TAGGING: Extracted tags {auto_tags} from model name '{request.model}'")
            candidates = self._get_candidate_channels_by_auto_tags(auto_tags)
            if candidates:
                logger.info(f"ğŸ·ï¸  AUTO TAGGING: Found {len(candidates)} candidate channels using auto-extracted tags")
                return candidates
            else:
                logger.warning(f"ğŸ·ï¸  AUTO TAGGING: No channels found for auto-extracted tags {auto_tags}")
        
        # 3. æœ€åå°è¯•ä»é…ç½®ä¸­æŸ¥æ‰¾
        config_channels = self.config_loader.get_channels_by_model(request.model)
        if config_channels:
            logger.info(f"Found {len(config_channels)} channels in configuration for model '{request.model}'")
            return [ChannelCandidate(channel=ch, matched_model=request.model) for ch in config_channels]
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›ç©ºåˆ—è¡¨
        logger.warning(f"âŒ NO MATCH: No channels found for model '{request.model}' (tried physical, auto-tag, and config)")
        return []
    
    def _filter_channels(self, channels: List[ChannelCandidate], request: RoutingRequest) -> List[ChannelCandidate]:
        """è¿‡æ»¤æ¸ é“"""
        filtered = []
        for candidate in channels:
            channel = candidate.channel
            if not channel.enabled or not channel.api_key:
                continue
            
            health_score = self.config_loader.runtime_state.health_scores.get(channel.id, 1.0)
            if health_score < 0.3:
                continue
            
            # TODO: Add capability filtering when needed
            # if request.required_capabilities:
            #     if not all(cap in channel.capabilities for cap in request.required_capabilities):
            #         continue
            
            filtered.append(candidate)
        return filtered
    
    def _score_channels(self, channels: List[ChannelCandidate], request: RoutingRequest) -> List[RoutingScore]:
        """è®¡ç®—æ¸ é“è¯„åˆ†"""
        logger.info(f"ğŸ“Š SCORING: Evaluating {len(channels)} candidate channels for model '{request.model}'")
        
        scored_channels = []
        strategy = self._get_routing_strategy(request.model)
        
        logger.info(f"ğŸ“Š SCORING: Using routing strategy with {len(strategy)} rules")
        for rule in strategy:
            logger.debug(f"ğŸ“Š SCORING: Strategy rule: {rule['field']} (weight: {rule['weight']}, order: {rule['order']})")
        
        for candidate in channels:
            channel = candidate.channel
            cost_score = self._calculate_cost_score(channel, request)
            speed_score = self._calculate_speed_score(channel)
            quality_score = self._calculate_quality_score(channel, candidate.matched_model)
            reliability_score = self._calculate_reliability_score(channel)
            
            total_score = self._calculate_total_score(
                strategy, cost_score, speed_score, quality_score, reliability_score
            )
            
            # ä¸ºäº†å‡å°‘æ—¥å¿—å†—ä½™ï¼Œåªæ˜¾ç¤ºæ¨¡å‹åç§°å’Œæ€»åˆ†
            model_display = candidate.matched_model or channel.model_name
            logger.info(f"ğŸ“Š SCORE: '{channel.name}' -> '{model_display}' = {total_score:.3f} (Q:{quality_score:.2f})")
            
            scored_channels.append(RoutingScore(
                channel=channel, total_score=total_score, cost_score=cost_score,
                speed_score=speed_score, quality_score=quality_score,
                reliability_score=reliability_score, 
                reason=f"cost:{cost_score:.2f} speed:{speed_score:.2f} quality:{quality_score:.2f} reliability:{reliability_score:.2f}",
                matched_model=candidate.matched_model
            ))
        
        scored_channels.sort(key=lambda x: x.total_score, reverse=True)
        
        logger.info(f"ğŸ† SCORING RESULT: Channels ranked by score:")
        for i, scored in enumerate(scored_channels[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            logger.info(f"ğŸ†   #{i+1}: '{scored.channel.name}' (Score: {scored.total_score:.3f})")
        
        return scored_channels
    
    def _get_routing_strategy(self, model: str) -> List[Dict[str, Any]]:
        """è·å–å¹¶è§£æè·¯ç”±ç­–ç•¥ï¼Œå§‹ç»ˆè¿”å›è§„åˆ™åˆ—è¡¨"""
        strategy_name = self.config.routing.default_strategy

        predefined_strategies = {
            "cost_optimized": [
                {"field": "cost_score", "order": "desc", "weight": 0.7},
                {"field": "reliability_score", "order": "desc", "weight": 0.2},
                {"field": "speed_score", "order": "desc", "weight": 0.1}
            ],
            "speed_optimized": [
                {"field": "speed_score", "order": "desc", "weight": 0.6},
                {"field": "reliability_score", "order": "desc", "weight": 0.2},
                {"field": "cost_score", "order": "desc", "weight": 0.2}
            ],
            "quality_optimized": [
                {"field": "quality_score", "order": "desc", "weight": 0.6},
                {"field": "reliability_score", "order": "desc", "weight": 0.2},
                {"field": "cost_score", "order": "desc", "weight": 0.2}
            ],
            "balanced": [
                {"field": "cost_score", "order": "desc", "weight": 0.3},
                {"field": "speed_score", "order": "desc", "weight": 0.3},
                {"field": "quality_score", "order": "desc", "weight": 0.2},
                {"field": "reliability_score", "order": "desc", "weight": 0.2}
            ]
        }
        
        return predefined_strategies.get(strategy_name, predefined_strategies["balanced"])

    def _calculate_cost_score(self, channel: Channel, request: RoutingRequest) -> float:
        """è®¡ç®—æˆæœ¬è¯„åˆ†(0-1ï¼Œè¶Šä½æˆæœ¬è¶Šé«˜åˆ†)"""
        pricing = channel.pricing
        if not pricing:
            return 0.5
        
        # ä¼°ç®—tokenæ•°é‡
        input_tokens = self._estimate_tokens(request.messages)
        max_output_tokens = request.max_tokens or 1000
        
        # è®¡ç®—æˆæœ¬
        input_cost = pricing.get("input_cost_per_1k", 0.001) * input_tokens / 1000
        output_cost = pricing.get("output_cost_per_1k", 0.002) * max_output_tokens / 1000
        total_cost = (input_cost + output_cost) * pricing.get("effective_multiplier", 1.0)
        
        # è½¬æ¢ä¸ºè¯„åˆ† (æˆæœ¬è¶Šä½åˆ†æ•°è¶Šé«˜)
        # å‡è®¾æœ€é«˜æˆæœ¬ä¸º0.1ç¾å…ƒ
        max_cost = 0.1
        score = max(0, 1 - (total_cost / max_cost))
        
        return min(1.0, score)
    
    # å†…ç½®æ¨¡å‹è´¨é‡æ’å (åˆ†æ•°è¶Šé«˜è¶Šå¥½, æ»¡åˆ†100)
    MODEL_QUALITY_RANKING = {
        # OpenAI ç³»åˆ—
        "gpt-4o": 98, "gpt-4-turbo": 95, "gpt-4": 90, "gpt-4o-mini": 85, "gpt-3.5-turbo": 70,
        # Anthropic ç³»åˆ—
        "claude-3-5-sonnet": 99, "claude-3-opus": 97, "claude-3-sonnet": 92, "claude-3-haiku": 80,
        # Meta Llama ç³»åˆ—
        "llama-3.1-70b": 93, "llama-3.1-8b": 82, "llama-3-70b": 90, "llama-3-8b": 80,
        # Qwen ç³»åˆ—
        "qwen2.5-72b-instruct": 91, "qwen2-72b-instruct": 90, "qwen2-7b-instruct": 78,
        "qwen3-coder": 88, "qwen3-30b": 85, "qwen3-14b": 82, "qwen3-8b": 80, "qwen3-4b": 75, 
        "qwen3-1.7b": 65, "qwen3-0.6b": 60,
        # DeepSeek ç³»åˆ—
        "deepseek-r1": 89, "deepseek-v3": 87, "deepseek-coder": 85,
        # Moonshot ç³»åˆ—
        "moonshot-v1-128k": 88, "moonshot-v1-32k": 87, "moonshot-v1-8k": 86,
        # 01.AI ç³»åˆ—
        "yi-large": 89, "yi-lightning": 75,
        # Google ç³»åˆ—
        "gemma-3-12b": 78, "gemma-3-9b": 75, "gemma-3-270m": 55,
    }

    def _calculate_quality_score(self, channel: Channel, matched_model: Optional[str] = None) -> float:
        """æ ¹æ®å†…ç½®æ’ååŠ¨æ€è®¡ç®—è´¨é‡è¯„åˆ†"""
        # ä¼˜å…ˆä½¿ç”¨åŒ¹é…çš„æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ¸ é“é»˜è®¤æ¨¡å‹
        model_name = (matched_model or channel.model_name).lower()
        simple_model_name = model_name.split('/')[-1]
        quality_score = self.MODEL_QUALITY_RANKING.get(simple_model_name)
        
        if quality_score is None:
            for key, score in self.MODEL_QUALITY_RANKING.items():
                if key in model_name:
                    quality_score = score
                    break
        
        # ç»™æœ¬åœ°æ¨¡å‹ä¸€äº›è´¨é‡åˆ†æ•°è°ƒæ•´
        if any(keyword in model_name for keyword in ['qwen3', 'qwen2.5']):
            base_score = quality_score or 75
            # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´åˆ†æ•°
            if '0.6b' in model_name or '1.7b' in model_name:
                base_score = max(60, base_score - 15)  # å°æ¨¡å‹é™åˆ†
            elif '4b' in model_name:
                base_score = max(70, base_score - 10)  # ä¸­æ¨¡å‹é€‚å½“é™åˆ†
            quality_score = base_score
        
        return (quality_score or 70) / 100.0

    def _calculate_speed_score(self, channel: Channel) -> float:
        """æ ¹æ®å¹³å‡å»¶è¿ŸåŠ¨æ€è®¡ç®—é€Ÿåº¦è¯„åˆ†"""
        channel_stats = self.config_loader.runtime_state.channel_stats.get(channel.id)
        if channel_stats and "avg_latency_ms" in channel_stats:
            avg_latency = channel_stats["avg_latency_ms"]
            base_latency = 2000.0
            score = max(0.0, 1.0 - (avg_latency / base_latency))
            return 0.1 + score * 0.9
        return channel.performance.get("speed_score", 0.8)

    def _calculate_reliability_score(self, channel: Channel) -> float:
        """è®¡ç®—å¯é æ€§è¯„åˆ†"""
        health_score = self.config_loader.runtime_state.health_scores.get(channel.id, 1.0)
        return health_score
    
    def _calculate_total_score(self, strategy: List[Dict[str, Any]], 
                             cost_score: float, speed_score: float, 
                             quality_score: float, reliability_score: float) -> float:
        """æ ¹æ®ç­–ç•¥è®¡ç®—æ€»è¯„åˆ†"""
        total_score = 0.0
        score_map = {
            "cost_score": cost_score, "speed_score": speed_score,
            "quality_score": quality_score, "reliability_score": reliability_score
        }
        
        total_weight = sum(rule.get("weight", 0.0) for rule in strategy)
        if total_weight == 0: 
            return 0.5

        for rule in strategy:
            field = rule.get("field", "")
            if field in score_map:
                score = score_map[field]
                if rule.get("order") == "asc": 
                    score = 1.0 - score
                total_score += score * rule.get("weight", 0.0)
        
        return total_score / total_weight
    
    def _estimate_tokens(self, messages: List[Dict[str, Any]]) -> int:
        """ä½¿ç”¨tiktokenä¼°ç®—prompt tokens"""
        try:
            # å°è¯•ä»mainæ¨¡å—è·å–å·²ç»åˆå§‹åŒ–çš„encoder
            import main
            if hasattr(main, 'estimate_prompt_tokens'):
                return main.estimate_prompt_tokens(messages)
        except (ImportError, AttributeError):
            pass

        # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨ç®€å•å›é€€æ–¹æ³•
        logger.warning("æ— æ³•ä»mainæ¨¡å—è·å–tiktokenç¼–ç å™¨, tokenè®¡ç®—å°†ä½¿ç”¨ç®€å•æ–¹æ³•")
        total_chars = 0
        for message in messages:
            content = message.get("content", "")
            if isinstance(content, str):
                total_chars += len(content)
        return max(1, total_chars // 4)
    
    def _is_suitable_for_chat(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦é€‚åˆchatå¯¹è¯ä»»åŠ¡"""
        if not model_name:
            return False
            
        model_lower = model_name.lower()
        
        # è¿‡æ»¤embeddingæ¨¡å‹
        embedding_keywords = ['embedding', 'embed', 'text-embedding']
        if any(keyword in model_lower for keyword in embedding_keywords):
            return False
            
        # è¿‡æ»¤çº¯visionæ¨¡å‹
        vision_only_keywords = ['vision-only', 'image-only']
        if any(keyword in model_lower for keyword in vision_only_keywords):
            return False
            
        # è¿‡æ»¤å·¥å…·ç±»æ¨¡å‹
        tool_keywords = ['tokenizer', 'classifier', 'detector']
        if any(keyword in model_lower for keyword in tool_keywords):
            return False
            
        return True

    def _extract_tags_from_model_name(self, model_name: str) -> List[str]:
        """ä»æ¨¡å‹åç§°ä¸­æå–æ ‡ç­¾ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        ä¾‹å¦‚: "qwen/qwen3-30b-a3b:free" -> ["qwen", "qwen3", "30b", "a3b", "free"]
        """
        if not model_name or not isinstance(model_name, str):
            return []
        
        # æ£€æŸ¥ç¼“å­˜
        if model_name in self._tag_cache:
            return self._tag_cache[model_name]
        
        import re
        
        # ä½¿ç”¨å¤šç§åˆ†éš”ç¬¦è¿›è¡Œæ‹†åˆ†: :, /, @, -, _
        separators = r'[/:@\-_]'
        parts = re.split(separators, model_name.lower())
        
        # æ¸…ç†å’Œè¿‡æ»¤æ ‡ç­¾
        tags = []
        for part in parts:
            part = part.strip()
            if part and len(part) > 1:  # å¿½ç•¥å•å­—ç¬¦å’Œç©ºæ ‡ç­¾
                # è¿›ä¸€æ­¥åˆ†è§£æ•°å­—å’Œå­—æ¯ç»„åˆï¼Œå¦‚ "30b" -> ["30b"]
                tags.append(part)
        
        # ç¼“å­˜ç»“æœ
        self._tag_cache[model_name] = tags
        return tags

    def _get_candidate_channels_by_auto_tags(self, tags: List[str]) -> List[ChannelCandidate]:
        """æ ¹æ®è‡ªåŠ¨æå–çš„æ ‡ç­¾è·å–å€™é€‰æ¸ é“ï¼ˆæ”¯æŒæ¸è¿›å¼å›é€€ï¼‰"""
        if not tags:
            return []
        
        # æ ‡å‡†åŒ–æ ‡ç­¾
        normalized_tags = [tag.lower().strip() for tag in tags if tag and isinstance(tag, str)]
        if not normalized_tags:
            return []
        
        logger.info(f"ğŸ” TAG MATCHING: Searching for channels with tags: {normalized_tags}")
        
        model_cache = self.config_loader.get_model_cache()
        if not model_cache:
            logger.warning("ğŸ” TAG MATCHING: Model cache is empty, cannot perform tag routing")
            return []
        
        logger.info(f"ğŸ” TAG MATCHING: Searching through {len(model_cache)} cached channels")
        
        # ä¸¥æ ¼åŒ¹é…ï¼šåªè¿”å›åŒæ—¶åŒ…å«æ‰€æœ‰æ ‡ç­¾çš„æ¨¡å‹ï¼Œä¸è¿›è¡Œä»»ä½•å›é€€
        exact_candidates = self._find_channels_with_all_tags(normalized_tags, model_cache)
        
        if exact_candidates:
            logger.info(f"ğŸ¯ STRICT MATCH: Found {len(exact_candidates)} channels with ALL required tags {normalized_tags}")
            return exact_candidates
        
        logger.warning(f"âŒ NO MATCH: No channels found with ALL required tags {normalized_tags}")
        return []
    
    def _find_channels_with_all_tags(self, tags: List[str], model_cache: dict) -> List[ChannelCandidate]:
        """æŸ¥æ‰¾åŒ…å«æ‰€æœ‰æŒ‡å®šæ ‡ç­¾çš„æ¸ é“å’Œæ¨¡å‹ç»„åˆ
        
        æ ‡ç­¾åŒ¹é…è§„åˆ™ï¼š
        1. é¦–å…ˆæ£€æŸ¥æ¸ é“çº§åˆ«çš„æ ‡ç­¾ (channel.tags)
        2. å¦‚æœæ¸ é“æ ‡ç­¾åŒ¹é…ï¼Œè¯¥æ¸ é“ä¸‹æ‰€æœ‰æ¨¡å‹éƒ½è¢«è§†ä¸ºåŒ¹é…
        3. å¦åˆ™æ£€æŸ¥ä»æ¨¡å‹åç§°æå–çš„æ ‡ç­¾
        """
        candidate_channels = []
        matched_models = []
        
        # éå†æ‰€æœ‰æœ‰æ•ˆæ¸ é“
        for channel in self.config_loader.get_enabled_channels():
            if channel.id not in model_cache:
                continue
                
            discovered_info = model_cache[channel.id]
            if not isinstance(discovered_info, dict):
                continue
                
            models = discovered_info.get("models", [])
            if not models:
                continue
            
            logger.debug(f"ğŸ” TAG MATCHING: Checking channel {channel.id} ({channel.name}) with {len(models)} models")
            
            # ç»Ÿä¸€çš„æ ‡ç­¾åˆå¹¶åŒ¹é…ï¼šæ¸ é“æ ‡ç­¾ + æ¨¡å‹æ ‡ç­¾
            channel_tags = getattr(channel, 'tags', []) or []
            channel_matches = 0
            
            for model_name in models:
                if not model_name:
                    continue
                    
                # ä»æ¨¡å‹åç§°æå–æ ‡ç­¾
                model_tags = self._extract_tags_from_model_name(model_name)
                
                # åˆå¹¶æ¸ é“æ ‡ç­¾å’Œæ¨¡å‹æ ‡ç­¾
                combined_tags = list(set(channel_tags + model_tags))
                
                # éªŒè¯æ‰€æœ‰æŸ¥è¯¢æ ‡ç­¾éƒ½åœ¨åˆå¹¶åçš„æ ‡ç­¾ä¸­
                if all(tag in combined_tags for tag in tags):
                    # è¿‡æ»¤æ‰ä¸é€‚åˆchatçš„æ¨¡å‹ç±»å‹
                    if self._is_suitable_for_chat(model_name):
                        candidate_channels.append(ChannelCandidate(
                            channel=channel,
                            matched_model=model_name
                        ))
                        matched_models.append(model_name)
                        channel_matches += 1
                        logger.debug(f"âœ… MERGED TAG MATCH: Channel '{channel.name}' model '{model_name}' -> tags: {combined_tags}")
                    else:
                        logger.debug(f"âš ï¸ FILTERED: Model '{model_name}' not suitable for chat (appears to be embedding/vision model)")
            
            if channel_matches > 0:
                logger.info(f"ğŸ¯ CHANNEL SUMMARY: Found {channel_matches} matching models in channel '{channel.name}' via merged channel+model tags")
        
        if matched_models:
            logger.info(f"ğŸ¯ TOTAL MATCHED MODELS: {len(matched_models)} models found: {matched_models[:5]}{'...' if len(matched_models) > 5 else ''}")
        
        return candidate_channels

    def get_available_models(self) -> List[str]:
        """è·å–ç”¨æˆ·å¯ç›´æ¥è¯·æ±‚çš„ã€åœ¨é…ç½®ä¸­å®šä¹‰çš„æ¨¡å‹åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        if self._available_models_cache is not None:
            return self._available_models_cache
        
        models = set()
        all_tags = set()
        
        # æ·»åŠ ç‰©ç†æ¨¡å‹åç§°
        for ch in self.config.channels:
            if ch.enabled and ch.model_name:
                models.add(ch.model_name)
                
                # ä»é…ç½®çš„tagsä¸­æ·»åŠ 
                for tag in ch.tags:
                    if tag:
                        all_tags.add(f"tag:{tag}")
        
        # ä»æ¨¡å‹ç¼“å­˜ä¸­æå–è‡ªåŠ¨æ ‡ç­¾
        model_cache = self.config_loader.get_model_cache()
        if model_cache:
            for channel_id, cache_info in model_cache.items():
                if not isinstance(cache_info, dict):
                    continue
                
                models_list = cache_info.get("models", [])
                if not isinstance(models_list, list):
                    continue
                    
                for model_name in models_list:
                    if model_name:
                        models.add(model_name)
                        # æå–è‡ªåŠ¨æ ‡ç­¾
                        auto_tags = self._extract_tags_from_model_name(model_name)
                        for tag in auto_tags:
                            if tag:
                                all_tags.add(f"tag:{tag}")
        
        # ç¼“å­˜ç»“æœ
        result = sorted(list(models | all_tags))
        self._available_models_cache = result
        return result
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        self._tag_cache.clear()
        self._available_tags_cache = None
        self._available_models_cache = None
        logger.info("Router cache cleared")
    
    def update_channel_health(self, channel_id: str, success: bool, latency: Optional[float] = None):
        """æ›´æ–°æ¸ é“å¥åº·çŠ¶æ€"""
        self.config_loader.update_channel_health(channel_id, success, latency)

# å…¨å±€è·¯ç”±å™¨å®ä¾‹
_router: Optional[JSONRouter] = None

def get_router() -> JSONRouter:
    """è·å–å…¨å±€è·¯ç”±å™¨å®ä¾‹"""
    global _router
    if _router is None:
        _router = JSONRouter()
    return _router