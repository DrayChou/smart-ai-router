"""
åŸºäºJSONé…ç½®çš„è½»é‡è·¯ç”±å¼•æ“
"""
import logging
import random
import re
import time
from dataclasses import dataclass
from typing import Any, Optional

from .config_models import Channel
from .utils.capability_mapper import get_capability_mapper
from .utils.channel_cache_manager import get_channel_cache_manager
from .utils.local_model_capabilities import get_capability_detector
from .utils.model_analyzer import get_model_analyzer
from .utils.parameter_comparator import get_parameter_comparator
from .utils.request_cache import RequestFingerprint, get_request_cache
from .yaml_config import YAMLConfigLoader, get_yaml_config_loader

logger = logging.getLogger(__name__)

class TagNotFoundError(Exception):
    """æ ‡ç­¾æœªæ‰¾åˆ°é”™è¯¯"""
    def __init__(self, tags: list[str], message: str = None):
        self.tags = tags
        if message is None:
            if len(tags) == 1:
                message = f"æ²¡æœ‰æ‰¾åˆ°åŒ¹é…æ ‡ç­¾ '{tags[0]}' çš„æ¨¡å‹"
            else:
                message = f"æ²¡æœ‰æ‰¾åˆ°åŒæ—¶åŒ¹é…æ ‡ç­¾ {tags} çš„æ¨¡å‹"
        super().__init__(message)

class ParameterComparisonError(Exception):
    """å‚æ•°é‡æ¯”è¾ƒé”™è¯¯"""
    def __init__(self, query: str, message: str = None):
        self.query = query
        if message is None:
            message = f"æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³å‚æ•°é‡æ¯”è¾ƒ '{query}' çš„æ¨¡å‹"
        super().__init__(message)

@dataclass
class SizeFilter:
    """å¤§å°è¿‡æ»¤å™¨"""
    operator: str  # >, <, >=, <=, =
    value: float
    unit: str  # b, m, k for parameters; ki, ko, mi, mo for context
    type: str  # 'params', 'input_context', 'output_context'

    def matches(self, target_value: float) -> bool:
        """æ£€æŸ¥ç›®æ ‡å€¼æ˜¯å¦åŒ¹é…è¿‡æ»¤æ¡ä»¶"""
        if self.operator == ">":
            return target_value > self.value
        elif self.operator == "<":
            return target_value < self.value
        elif self.operator == ">=":
            return target_value >= self.value
        elif self.operator == "<=":
            return target_value <= self.value
        elif self.operator == "=":
            return target_value == self.value
        return False

def parse_size_filter(tag: str) -> Optional[SizeFilter]:
    """è§£æå¤§å°è¿‡æ»¤æ ‡ç­¾

    Args:
        tag: æ ‡ç­¾å­—ç¬¦ä¸²ï¼Œå¦‚ ">20b", "<8ko", ">=10ki"

    Returns:
        SizeFilter å¯¹è±¡ï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å› None
    """
    # å‚æ•°å¤§å°è¿‡æ»¤æ¨¡å¼ï¼š>20b, <7b, >=1.5b
    param_pattern = r'^([><=]+)(\d+\.?\d*)([bmk])$'
    # ä¸Šä¸‹æ–‡å¤§å°è¿‡æ»¤æ¨¡å¼ï¼š>10ki, <8ko, >=32mi
    context_pattern = r'^([><=]+)(\d+\.?\d*)([kK]?[iI]|[mM]?[oO])$'

    # å…ˆå°è¯•å‚æ•°å¤§å°è¿‡æ»¤
    match = re.match(param_pattern, tag, re.IGNORECASE)
    if match:
        operator, value_str, unit = match.groups()
        try:
            value = float(value_str)
            # è½¬æ¢å•ä½åˆ°åŸºæœ¬å•ä½
            if unit.lower() == 'b':
                pass  # 1b = 1 billion parameters
            elif unit.lower() == 'm':
                pass  # 1m = 1 million parameters
            elif unit.lower() == 'k':
                pass  # 1k = 1 thousand parameters
            else:
                return None

            return SizeFilter(
                operator=operator,
                value=value,
                unit=unit,
                type='params'
            )
        except ValueError:
            return None

    # å†å°è¯•ä¸Šä¸‹æ–‡å¤§å°è¿‡æ»¤
    match = re.match(context_pattern, tag, re.IGNORECASE)
    if match:
        operator, value_str, unit = match.groups()
        try:
            value = float(value_str)
            # è½¬æ¢å•ä½åˆ°åŸºæœ¬å•ä½ (tokens)
            unit_lower = unit.lower()
            if unit_lower in ['ki', 'i']:
                pass  # 1ki = 1000 tokens
            elif unit_lower in ['ko', 'o']:
                pass  # 1ko = 1000 tokens
            elif unit_lower in ['mi', 'm']:
                pass  # 1mi = 1M tokens
            elif unit_lower in ['mo']:
                pass  # 1mo = 1M tokens
            else:
                return None

            context_type = 'input_context' if unit_lower in ['ki', 'i', 'mi', 'm'] else 'output_context'

            return SizeFilter(
                operator=operator,
                value=value,
                unit=unit,
                type=context_type
            )
        except ValueError:
            return None

    return None


def apply_size_filters(candidates: list['ChannelCandidate'], size_filters: list[SizeFilter]) -> list['ChannelCandidate']:
    """åº”ç”¨å¤§å°è¿‡æ»¤å™¨åˆ°å€™é€‰æ¸ é“

    Args:
        candidates: å€™é€‰æ¸ é“åˆ—è¡¨
        size_filters: å¤§å°è¿‡æ»¤å™¨åˆ—è¡¨

    Returns:
        è¿‡æ»¤åçš„å€™é€‰æ¸ é“åˆ—è¡¨
    """
    if not size_filters:
        return candidates

    filtered_candidates = []
    model_analyzer = get_model_analyzer()

    logger.info(f"SIZE FILTERS: Applying {len(size_filters)} filters to {len(candidates)} candidates")

    for candidate in candidates:
        match = True
        model_name = candidate.matched_model
        logger.info(f"SIZE FILTER DEBUG: Processing candidate {candidate.channel.id} -> {model_name}")

        # è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
        config_loader = get_yaml_config_loader()
        channel_id = candidate.channel.id
        model_data = None

        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨API Keyçº§åˆ«ç¼“å­˜æŸ¥æ‰¾æ–¹æ³•
        discovered_info = config_loader.get_model_cache_by_channel(channel_id)
        if isinstance(discovered_info, dict):
            # é¦–å…ˆå°è¯•ä» models_data è·å–æ¨¡å‹è¯¦æƒ…ï¼ˆæ–°æ ¼å¼ï¼‰
            models_data = discovered_info.get("models_data", {})
            if model_name in models_data:
                model_data = models_data[model_name]
                logger.info(f"SIZE FILTER DEBUG: Found model_data for {model_name} in models_data")
            else:
                # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœ models_data ä¸ºç©ºï¼Œä» response_data.data æŸ¥æ‰¾ï¼ˆOpenRouteræ ¼å¼ï¼‰
                response_data = discovered_info.get("response_data", {})
                data_list = response_data.get("data", [])
                model_data = None
                for model_info in data_list:
                    if model_info.get("id") == model_name:
                        model_data = model_info
                        logger.info(f"SIZE FILTER DEBUG: Found model_data for {model_name} in response_data: context_length={model_info.get('context_length')}")
                        break
                if not model_data:
                    logger.info(f"SIZE FILTER DEBUG: No model_data found for {model_name} in channel {channel_id}")
                    logger.info(f"SIZE FILTER DEBUG: Available models in models_data: {list(models_data.keys())[:5]}")
                    logger.info(f"SIZE FILTER DEBUG: Available models in response_data: {len(data_list)} models")
        else:
            logger.info(f"SIZE FILTER DEBUG: No discovered_info found for channel {channel_id}")

        # ä½¿ç”¨æ¨¡å‹åˆ†æå™¨åˆ†ææ¨¡å‹
        try:
            specs = model_analyzer.analyze_model(model_name, model_data)
            if specs:
                logger.info(f"SIZE FILTER DEBUG: Model analysis for {model_name}: context_length={specs.context_length}, parameter_count={specs.parameter_count}")
            else:
                logger.info(f"SIZE FILTER DEBUG: Model analysis returned None for {model_name}")
        except Exception as e:
            logger.warning(f"Failed to analyze model {model_name}: {e}")
            specs = None

        # åº”ç”¨æ¯ä¸ªè¿‡æ»¤å™¨
        for size_filter in size_filters:
            if size_filter.type == 'params':
                # å‚æ•°å¤§å°è¿‡æ»¤
                if specs and specs.parameter_count:
                    # parameter_count ä»¥ç™¾ä¸‡ä¸ºå•ä½ï¼Œéœ€è¦è½¬æ¢ä¸º billion è¿›è¡Œæ¯”è¾ƒ
                    param_count_billions = specs.parameter_count / 1000.0
                    logger.debug(f"Model {model_name}: {specs.parameter_count}M params = {param_count_billions}B")

                    if not size_filter.matches(param_count_billions):
                        logger.debug(f"SIZE FILTER: {model_name} filtered out - {param_count_billions}B params does not match {size_filter.operator}{size_filter.value}b")
                        match = False
                        break
                else:
                    logger.debug(f"SIZE FILTER: {model_name} filtered out - no parameter count available")
                    match = False
                    break

            elif size_filter.type == 'input_context':
                # è¾“å…¥ä¸Šä¸‹æ–‡å¤§å°è¿‡æ»¤
                context_size = None
                if specs and specs.context_length:
                    context_size = specs.context_length
                elif model_data and model_data.get("max_input_tokens"):
                    context_size = model_data["max_input_tokens"]
                elif model_data and model_data.get("context_length"):
                    context_size = model_data["context_length"]

                if context_size:
                    # è½¬æ¢ä¸ºåƒä¸ºå•ä½è¿›è¡Œæ¯”è¾ƒ
                    context_size_k = context_size / 1000.0
                    logger.info(f"SIZE FILTER DEBUG: Model {model_name}: {context_size} input tokens = {context_size_k}k")

                    if not size_filter.matches(context_size_k):
                        logger.info(f"SIZE FILTER DEBUG: {model_name} filtered out - {context_size_k}ki does not match {size_filter.operator}{size_filter.value}ki")
                        match = False
                        break
                    else:
                        logger.info(f"SIZE FILTER DEBUG: {model_name} PASSED - {context_size_k}ki matches {size_filter.operator}{size_filter.value}ki")
                else:
                    logger.info(f"SIZE FILTER DEBUG: {model_name} filtered out - no input context size available")
                    match = False
                    break

            elif size_filter.type == 'output_context':
                # è¾“å‡ºä¸Šä¸‹æ–‡å¤§å°è¿‡æ»¤
                context_size = None
                if model_data and model_data.get("max_output_tokens"):
                    context_size = model_data["max_output_tokens"]
                elif specs and specs.context_length:
                    # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„è¾“å‡ºé™åˆ¶ï¼Œä½¿ç”¨æ€»ä¸Šä¸‹æ–‡é•¿åº¦ä½œä¸ºè¿‘ä¼¼
                    context_size = specs.context_length
                elif model_data and model_data.get("context_length"):
                    context_size = model_data["context_length"]

                if context_size:
                    # è½¬æ¢ä¸ºåƒä¸ºå•ä½è¿›è¡Œæ¯”è¾ƒ
                    context_size_k = context_size / 1000.0
                    logger.debug(f"Model {model_name}: {context_size} output tokens = {context_size_k}k")

                    if not size_filter.matches(context_size_k):
                        logger.debug(f"SIZE FILTER: {model_name} filtered out - {context_size_k}ko does not match {size_filter.operator}{size_filter.value}ko")
                        match = False
                        break
                else:
                    logger.debug(f"SIZE FILTER: {model_name} filtered out - no output context size available")
                    match = False
                    break

        if match:
            filtered_candidates.append(candidate)
            logger.debug(f"SIZE FILTER: {model_name} passed all filters")

    logger.info(f"SIZE FILTERS: Filtered from {len(candidates)} to {len(filtered_candidates)} candidates")
    return filtered_candidates

@dataclass
class RoutingScore:
    """è·¯ç”±è¯„åˆ†ç»“æœ"""
    channel: Channel
    total_score: float
    cost_score: float
    speed_score: float
    quality_score: float
    reliability_score: float
    reason: str
    matched_model: Optional[str] = None  # å¯¹äºæ ‡ç­¾è·¯ç”±ï¼Œè®°å½•å®é™…åŒ¹é…çš„æ¨¡å‹
    # æ€§èƒ½ä¼˜åŒ–ï¼šé¢„è®¡ç®—å±‚æ¬¡æ’åºæ‰€éœ€çš„é¢å¤–è¯„åˆ†
    parameter_score: float = 0.0
    context_score: float = 0.0
    free_score: float = 0.0

@dataclass
class ChannelCandidate:
    """å€™é€‰æ¸ é“ä¿¡æ¯"""
    channel: Channel
    matched_model: Optional[str] = None  # å¯¹äºæ ‡ç­¾è·¯ç”±ï¼Œè®°å½•å®é™…åŒ¹é…çš„æ¨¡å‹

@dataclass
class RoutingRequest:
    """è·¯ç”±è¯·æ±‚"""
    model: str
    messages: list[dict[str, Any]]
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    stream: bool = False
    functions: Optional[list[dict[str, Any]]] = None
    required_capabilities: list[str] = None
    data: Optional[dict[str, Any]] = None  # å®Œæ•´çš„è¯·æ±‚æ•°æ®ï¼Œç”¨äºèƒ½åŠ›æ£€æµ‹

class JSONRouter:
    """åŸºäºPydanticéªŒè¯åé…ç½®çš„è·¯ç”±å™¨"""

    def __init__(self, config_loader: Optional[YAMLConfigLoader] = None):
        self.config_loader = config_loader or get_yaml_config_loader()
        self.config = self.config_loader.config

        # æ ‡ç­¾ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
        self._tag_cache: dict[str, list[str]] = {}
        self._available_tags_cache: Optional[set] = None
        self._available_models_cache: Optional[list[str]] = None

        # æ¨¡å‹åˆ†æå™¨ã€ç¼“å­˜ç®¡ç†å™¨å’Œå‚æ•°é‡æ¯”è¾ƒå™¨
        self.model_analyzer = get_model_analyzer()
        self.cache_manager = get_channel_cache_manager()
        self.parameter_comparator = get_parameter_comparator()

        # èƒ½åŠ›æ£€æµ‹å™¨å’Œæ˜ å°„å™¨
        self.capability_detector = get_capability_detector()
        self.capability_mapper = get_capability_mapper()

    async def route_request(self, request: RoutingRequest) -> list[RoutingScore]:
        """
        è·¯ç”±è¯·æ±‚ï¼Œè¿”å›æŒ‰è¯„åˆ†æ’åºçš„å€™é€‰æ¸ é“åˆ—è¡¨ã€‚

        æ”¯æŒè¯·æ±‚çº§ç¼“å­˜ä»¥æé«˜æ€§èƒ½ï¼š
        - ç¼“å­˜TTL: 60ç§’
        - åŸºäºè¯·æ±‚æŒ‡çº¹çš„æ™ºèƒ½ç¼“å­˜é”®ç”Ÿæˆ
        - è‡ªåŠ¨æ•…éšœè½¬ç§»å’Œç¼“å­˜å¤±æ•ˆ
        """
        logger.info(f"ROUTING START: Processing request for model '{request.model}'")

        # ç”Ÿæˆè¯·æ±‚æŒ‡çº¹ç”¨äºç¼“å­˜
        fingerprint = RequestFingerprint(
            model=request.model,
            routing_strategy=getattr(request, 'routing_strategy', 'balanced'),
            required_capabilities=getattr(request, 'required_capabilities', None),
            min_context_length=getattr(request, 'min_context_length', None),
            max_cost_per_1k=getattr(request, 'max_cost_per_1k', None),
            prefer_local=getattr(request, 'prefer_local', False),
            exclude_providers=getattr(request, 'exclude_providers', None),
            # æ–°å¢å½±å“è·¯ç”±çš„å‚æ•°
            max_tokens=getattr(request, 'max_tokens', None),
            temperature=getattr(request, 'temperature', None),
            stream=getattr(request, 'stream', False),
            has_functions=bool(getattr(request, 'functions', None) or getattr(request, 'tools', None))
        )

        # æ£€æŸ¥ç¼“å­˜
        cache = get_request_cache()
        cache_key = fingerprint.to_cache_key()
        logger.debug(f"CACHE LOOKUP: Key={cache_key}, Model={request.model}")

        cached_result = await cache.get_cached_selection(fingerprint)

        if cached_result:
            # ç¼“å­˜å‘½ä¸­ï¼Œè½¬æ¢ä¸ºRoutingScoreåˆ—è¡¨
            logger.info(f"CACHE HIT: Using cached selection for '{request.model}' "
                       f"(cost: ${cached_result.cost_estimate:.4f})")

            # æ„å»ºRoutingScoreåˆ—è¡¨
            scores = []

            # ä¸»è¦æ¸ é“
            primary_score = RoutingScore(
                channel=cached_result.primary_channel,
                total_score=1.0,  # ç¼“å­˜çš„ç»“æœä¼˜å…ˆçº§æœ€é«˜
                cost_score=1.0 if cached_result.cost_estimate == 0.0 else 0.8,
                speed_score=0.9,  # ç¼“å­˜è®¿é—®å¾ˆå¿«
                quality_score=0.8,
                reliability_score=0.9,
                reason=f"CACHED: {cached_result.selection_reason}",
                matched_model=cached_result.primary_matched_model or request.model  # ä½¿ç”¨ç¼“å­˜ä¸­çš„çœŸå®æ¨¡å‹å
            )
            scores.append(primary_score)

            # å¤‡é€‰æ¸ é“
            for i, backup_channel in enumerate(cached_result.backup_channels):
                # è·å–å¯¹åº”çš„å¤‡é€‰æ¨¡å‹å
                backup_matched_model = None
                if cached_result.backup_matched_models and i < len(cached_result.backup_matched_models):
                    backup_matched_model = cached_result.backup_matched_models[i]

                backup_score = RoutingScore(
                    channel=backup_channel,
                    total_score=0.9 - i * 0.1,  # é€’å‡ä¼˜å…ˆçº§
                    cost_score=0.7,
                    speed_score=0.8,
                    quality_score=0.7,
                    reliability_score=0.8,
                    reason=f"CACHED_BACKUP_{i+1}",
                    matched_model=backup_matched_model or request.model  # ä½¿ç”¨ç¼“å­˜ä¸­çš„çœŸå®æ¨¡å‹å
                )
                scores.append(backup_score)

            return scores

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæ­£å¸¸è·¯ç”±é€»è¾‘
        logger.info(f"CACHE MISS: Computing fresh routing for '{request.model}'")
        try:
            # ç¬¬ä¸€æ­¥ï¼šè·å–å€™é€‰æ¸ é“
            logger.info("STEP 1: Finding candidate channels...")
            candidates = self._get_candidate_channels(request)
            if not candidates:
                logger.warning(f"ROUTING FAILED: No suitable channels found for model '{request.model}'")
                return []

            logger.info(f"STEP 1 COMPLETE: Found {len(candidates)} candidate channels")

            # ç¬¬äºŒæ­¥ï¼šè¿‡æ»¤æ¸ é“
            logger.info("STEP 2: Filtering channels by health and availability...")
            filtered_candidates = self._filter_channels(candidates, request)
            if not filtered_candidates:
                logger.warning(f"ROUTING FAILED: No available channels after filtering for model '{request.model}'")
                return []

            # ç¬¬2.5æ­¥ï¼šèƒ½åŠ›æ£€æµ‹è¿‡æ»¤ï¼ˆä»…å¯¹æœ¬åœ°æ¨¡å‹ï¼‰
            logger.info("STEP 2.5: Checking model capabilities...")
            capability_filtered = await self._filter_by_capabilities(filtered_candidates, request)
            if not capability_filtered:
                logger.warning(f"ROUTING FAILED: No channels with required capabilities for model '{request.model}'")
                return []

            logger.info(f"STEP 2.5 COMPLETE: {len(capability_filtered)} channels passed capability check (filtered out {len(filtered_candidates) - len(capability_filtered)})")
            filtered_candidates = capability_filtered

            # ç¬¬2.7æ­¥ï¼šé¢„ç­›é€‰ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰- é™åˆ¶å‚ä¸è¯¦ç»†è¯„åˆ†çš„æ¸ é“æ•°é‡
            if len(filtered_candidates) > 20:  # ä»…åœ¨æ¸ é“æ•°é‡è¿‡å¤šæ—¶é¢„ç­›é€‰
                logger.info(f"STEP 2.7: Pre-filtering {len(filtered_candidates)} channels to reduce scoring overhead...")
                pre_filtered = await self._pre_filter_channels(filtered_candidates, request, max_channels=20)
                logger.info(f"STEP 2.7 COMPLETE: Pre-filtered to {len(pre_filtered)} channels for detailed scoring")
                filtered_candidates = pre_filtered

            # ç¬¬ä¸‰æ­¥ï¼šè¯„åˆ†å’Œæ’åº
            logger.info("ğŸ¯ STEP 3: Scoring and ranking channels...")
            scored_channels = await self._score_channels(filtered_candidates, request)
            if not scored_channels:
                logger.warning(f"ROUTING FAILED: Failed to score any channels for model '{request.model}'")
                return []

            logger.info(f"STEP 3 COMPLETE: Scored {len(scored_channels)} channels")

            # ç¼“å­˜ç»“æœï¼ˆå¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰
            if scored_channels:
                primary_channel = scored_channels[0].channel
                backup_channels = [score.channel for score in scored_channels[1:6]]  # æœ€å¤š5ä¸ªå¤‡é€‰
                selection_reason = scored_channels[0].reason
                cost_estimate = self._estimate_cost_for_channel(primary_channel, request)

                # æå–çœŸå®çš„åŒ¹é…æ¨¡å‹å
                primary_matched_model = scored_channels[0].matched_model
                backup_matched_models = [score.matched_model for score in scored_channels[1:6]]

                try:
                    # å¼‚æ­¥ä¿å­˜åˆ°ç¼“å­˜
                    cache_key = await cache.cache_selection(
                        fingerprint=fingerprint,
                        primary_channel=primary_channel,
                        backup_channels=backup_channels,
                        selection_reason=selection_reason,
                        cost_estimate=cost_estimate,
                        ttl_seconds=60,  # 1åˆ†é’ŸTTL
                        primary_matched_model=primary_matched_model,
                        backup_matched_models=backup_matched_models
                    )
                    logger.debug(f"ğŸ’¾ CACHED RESULT: {cache_key} -> {primary_channel.name}")
                except Exception as cache_error:
                    logger.warning(f"âš ï¸  CACHE SAVE FAILED: {cache_error}, continuing without caching")

            logger.info(f"ğŸ‰ ROUTING SUCCESS: Ready to attempt {len(scored_channels)} channels in ranked order for model '{request.model}'")

            return scored_channels

        except TagNotFoundError:
            # è®©TagNotFoundErrorä¼ æ’­å‡ºå»ï¼Œä»¥ä¾¿ä¸Šå±‚å¤„ç†
            raise
        except ParameterComparisonError:
            # è®©ParameterComparisonErrorä¼ æ’­å‡ºå»ï¼Œä»¥ä¾¿ä¸Šå±‚å¤„ç†
            raise
        except Exception as e:
            logger.error(f"ROUTING ERROR: Request failed for model '{request.model}': {e}", exc_info=True)
            return []

    def _get_candidate_channels(self, request: RoutingRequest) -> list[ChannelCandidate]:
        """è·å–å€™é€‰æ¸ é“ï¼Œæ”¯æŒæŒ‰æ ‡ç­¾é›†åˆã€ç‰©ç†æ¨¡å‹æˆ–å‚æ•°é‡æ¯”è¾ƒè¿›è¡Œæ™ºèƒ½è·¯ç”±"""

        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºå‚æ•°é‡æ¯”è¾ƒæŸ¥è¯¢ï¼ˆqwen3->8b, qwen3-<72b ç­‰ï¼‰
        if self.parameter_comparator.is_parameter_comparison(request.model):
            logger.info(f"ğŸ”¢ PARAMETER COMPARISON: Processing query '{request.model}'")
            comparison = self.parameter_comparator.parse_comparison(request.model)
            if not comparison:
                logger.error(f"PARAM PARSE FAILED: Could not parse parameter comparison '{request.model}'")
                raise ParameterComparisonError(request.model)

            # è·å–æ‰€æœ‰æ¨¡å‹ç¼“å­˜
            model_cache = self.config_loader.get_model_cache()
            if not model_cache:
                logger.error("NO MODEL CACHE: Model cache is empty for parameter comparison")
                raise ParameterComparisonError(request.model, "æ¨¡å‹ç¼“å­˜ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå‚æ•°é‡æ¯”è¾ƒ")

            # æŒ‰å‚æ•°é‡æ¯”è¾ƒç­›é€‰æ¨¡å‹
            matching_models = self.parameter_comparator.filter_models_by_comparison(comparison, model_cache)
            if not matching_models:
                logger.error(f"PARAM COMPARISON FAILED: No models found matching '{request.model}'")
                raise ParameterComparisonError(request.model)
            else:
                logger.info("ğŸ“ First 5 matched models:")
                for i, (channel_id, model_name, model_params) in enumerate(matching_models[:5]):
                    logger.info(f"  {i+1}. {channel_id} -> {model_name} ({model_params:.3f}B)")

            # è½¬æ¢ä¸ºå€™é€‰æ¸ é“åˆ—è¡¨
            candidates = []
            disabled_count = 0
            not_found_count = 0

            logger.debug(f"Processing {len(matching_models)} matching models for channel lookup...")

            for channel_id, model_name, model_params in matching_models:
                # ä» API key-level cache key ä¸­æå–çœŸå®çš„ channel ID
                # æ ¼å¼: "channel_id_apikeyash" -> "channel_id"
                real_channel_id = channel_id
                if '_' in channel_id:
                    # å°è¯•å»æ‰ API key hash åç¼€
                    parts = channel_id.split('_')
                    if len(parts) >= 2:
                        # æ£€æŸ¥æœ€åä¸€éƒ¨åˆ†æ˜¯å¦ä¸º hash æ ¼å¼ï¼ˆé•¿åº¦ä¸º8çš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
                        potential_hash = parts[-1]
                        if len(potential_hash) == 8 and all(c in '0123456789abcdef' for c in potential_hash.lower()):
                            real_channel_id = '_'.join(parts[:-1])

                logger.debug(f"Channel ID mapping: '{channel_id}' -> '{real_channel_id}'")

                # æŸ¥æ‰¾å¯¹åº”çš„æ¸ é“å¯¹è±¡
                channel = self.config_loader.get_channel_by_id(real_channel_id)
                if channel:
                    if channel.enabled:
                        candidates.append(ChannelCandidate(
                            channel=channel,
                            matched_model=model_name
                        ))
                        logger.debug(f"Added channel: {channel.name} -> {model_name} ({model_params:.3f}B)")
                    else:
                        disabled_count += 1
                        logger.debug(f"Disabled channel: {real_channel_id} -> {model_name}")
                else:
                    not_found_count += 1
                    logger.debug(f"Channel not found: {real_channel_id} (from {channel_id}) -> {model_name}")

            logger.info(f"CHANNEL LOOKUP: Found {len(candidates)} enabled channels, "
                       f"disabled: {disabled_count}, not_found: {not_found_count}")

            logger.info(f"PARAMETER COMPARISON: Found {len(candidates)} candidate channels "
                       f"for '{comparison.model_prefix}' models {comparison.operator} {comparison.target_params}B")

            # æ˜¾ç¤ºå‰å‡ ä¸ªåŒ¹é…çš„æ¸ é“
            if candidates:
                logger.info("ğŸ“ Top matched channels:")
                for i, candidate in enumerate(candidates[:5]):
                    logger.info(f"  {i+1}. {candidate.channel.name} -> {candidate.matched_model}")
            return candidates

        # 2. æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡ç­¾æŸ¥è¯¢
        if request.model.startswith("tag:") or request.model.startswith("tags:"):
            # ç»Ÿä¸€å¤„ç† tag: å’Œ tags: å‰ç¼€
            prefix = "tag:" if request.model.startswith("tag:") else "tags:"
            tag_query = request.model.split(":", 1)[1]

            # æ”¯æŒå¤šæ ‡ç­¾æŸ¥è¯¢ï¼Œç”¨é€—å·åˆ†éš”ï¼štag:qwen,free,!local
            if "," in tag_query:
                tag_parts = [tag.strip() for tag in tag_query.split(",")]
                positive_tags = []
                negative_tags = []

                for tag_part in tag_parts:
                    if tag_part.startswith("!"):
                        # è´Ÿæ ‡ç­¾ï¼š!local
                        negative_tags.append(tag_part[1:].lower())
                    else:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤§å°è¿‡æ»¤æ ‡ç­¾
                        size_filter = parse_size_filter(tag_part)
                        if size_filter:
                            # è¿™æ˜¯å¤§å°è¿‡æ»¤æ ‡ç­¾ï¼Œä½†ä¹Ÿè¦å½“ä½œæ­£æ ‡ç­¾å¤„ç†æŸ¥æ‰¾å€™é€‰è€…
                            pass  # Size filter will be applied later
                        else:
                            # æ­£æ ‡ç­¾ï¼šfree, qwen3
                            positive_tags.append(tag_part.lower())

                logger.info(f"TAG ROUTING: Processing multi-tag query '{request.model}' -> positive: {positive_tags}, negative: {negative_tags} (prefix: {prefix})")
                candidates = self._get_candidate_channels_by_auto_tags(positive_tags, negative_tags)
                if not candidates:
                    logger.error(f"TAG NOT FOUND: No models found matching tags {positive_tags} excluding {negative_tags}")
                    raise TagNotFoundError(positive_tags + [f"!{tag}" for tag in negative_tags])

                # æå–å¹¶åº”ç”¨å¤§å°è¿‡æ»¤å™¨
                size_filters = []
                for tag_part in tag_parts:
                    if not tag_part.startswith("!"):
                        size_filter = parse_size_filter(tag_part)
                        if size_filter:
                            size_filters.append(size_filter)

                if size_filters:
                    logger.info(f"SIZE FILTERS: Applying {len(size_filters)} size filters: {[f'{sf.operator}{sf.value}{sf.unit}' for sf in size_filters]}")
                    filtered_candidates = apply_size_filters(candidates, size_filters)
                    logger.info(f"SIZE FILTERS: Filtered from {len(candidates)} to {len(filtered_candidates)} candidates")
                    candidates = filtered_candidates

                if not candidates:
                    logger.error("SIZE FILTERS: No candidates left after applying size filters")
                    raise TagNotFoundError(positive_tags + [f"!{tag}" for tag in negative_tags])

                logger.info(f"TAG ROUTING: Multi-tag query found {len(candidates)} candidate channels")
                return candidates
            else:
                # å•æ ‡ç­¾æŸ¥è¯¢ - æ”¯æŒè´Ÿæ ‡ç­¾ï¼štag:!local
                tag_part = tag_query.strip()
                if tag_part.startswith("!"):
                    # è´Ÿæ ‡ç­¾å•ç‹¬æŸ¥è¯¢ï¼štag:!local (åŒ¹é…æ‰€æœ‰ä¸åŒ…å«localçš„æ¨¡å‹)
                    negative_tag = tag_part[1:].lower()
                    logger.info(f"TAG ROUTING: Processing negative tag query '{request.model}' -> excluding: '{negative_tag}' (prefix: {prefix})")
                    candidates = self._get_candidate_channels_by_auto_tags([], [negative_tag])
                else:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºå¤§å°è¿‡æ»¤æ ‡ç­¾
                    size_filter = parse_size_filter(tag_part)
                    if size_filter:
                        # è¿™æ˜¯å¤§å°è¿‡æ»¤æ ‡ç­¾ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                        logger.info(f"TAG ROUTING: Processing size filter query '{request.model}' -> filter: '{tag_part}' (prefix: {prefix})")
                        # è·å–æ‰€æœ‰å€™é€‰æ¸ é“ï¼Œç„¶ååº”ç”¨å¤§å°è¿‡æ»¤å™¨
                        candidates = self._get_candidate_channels_by_auto_tags([], [])
                        filtered_candidates = apply_size_filters(candidates, [size_filter])
                        logger.info(f"SIZE FILTERS: Filtered from {len(candidates)} to {len(filtered_candidates)} candidates")
                        candidates = filtered_candidates
                    else:
                        # æ­£å¸¸å•æ ‡ç­¾æŸ¥è¯¢
                        tag = tag_part.lower()
                        logger.info(f"TAG ROUTING: Processing single tag query '{request.model}' -> tag: '{tag}' (prefix: {prefix})")
                        candidates = self._get_candidate_channels_by_auto_tags([tag], [])

                if not candidates:
                    logger.error(f"TAG NOT FOUND: No models found for query '{request.model}'")
                    raise TagNotFoundError([tag_query])
                logger.info(f"TAG ROUTING: Found {len(candidates)} candidate channels")
                return candidates

        # étag:å‰ç¼€çš„æ¨¡å‹åç§° - ç²¾ç¡®åŒ¹é…ç”¨æˆ·è¾“å…¥çš„å®Œæ•´åç§°
        all_enabled_channels = self.config_loader.get_enabled_channels()
        model_cache = self.config_loader.get_model_cache()

        # 1. é¦–å…ˆå°è¯•ä½œä¸ºç‰©ç†æ¨¡å‹æŸ¥æ‰¾ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        physical_candidates = []
        for channel in all_enabled_channels:
            if channel.id in model_cache:
                discovered_info = model_cache[channel.id]
                models_data = discovered_info.get("models_data", {}) if isinstance(discovered_info, dict) else {}

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç²¾ç¡®åŒ¹é…çš„æ¨¡å‹
                if request.model in discovered_info.get("models", []):
                    # è·å–çœŸå®çš„æ¨¡å‹ID - å¯èƒ½ä¸è¯·æ±‚çš„ä¸åŒï¼ˆåˆ«åæ˜ å°„ï¼‰
                    real_model_id = request.model
                    if models_data and request.model in models_data:
                        model_info = models_data[request.model]
                        real_model_id = model_info.get("id", request.model)

                    logger.debug(f"PHYSICAL MODEL: Found '{request.model}' -> '{real_model_id}' in channel '{channel.name}'")
                    physical_candidates.append(ChannelCandidate(
                        channel=channel,
                        matched_model=real_model_id  # ä½¿ç”¨çœŸå®çš„æ¨¡å‹ID
                    ))

        # 2. å°è¯•å®Œæ•´å­æ®µæ ‡ç­¾åŒ¹é…ï¼ˆä¸æ‹†åˆ†ç”¨æˆ·è¾“å…¥ï¼‰
        complete_segment_candidates = []
        logger.info(f"COMPLETE SEGMENT MATCHING: Searching for exact match of '{request.model}' as complete segment")

        # ä½¿ç”¨ç”¨æˆ·è¾“å…¥ä½œä¸ºå®Œæ•´æ ‡ç­¾è¿›è¡ŒåŒ¹é…
        complete_segment_candidates = self._get_candidate_channels_by_complete_segment([request.model.lower()])
        if complete_segment_candidates:
            logger.info(f"COMPLETE SEGMENT MATCHING: Found {len(complete_segment_candidates)} candidate channels using complete segment match")
        else:
            logger.info(f"COMPLETE SEGMENT MATCHING: No channels found for complete segment '{request.model}'")

        # 3. åˆå¹¶ç‰©ç†æ¨¡å‹å’Œå®Œæ•´å­æ®µåŒ¹é…çš„ç»“æœï¼Œå»é‡
        all_candidates = physical_candidates.copy()

        # æ·»åŠ å®Œæ•´å­æ®µåŒ¹é…çš„å€™é€‰ï¼Œé¿å…é‡å¤
        for segment_candidate in complete_segment_candidates:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ç›¸åŒçš„ channel + model ç»„åˆ
            duplicate_found = False
            for existing in all_candidates:
                if (existing.channel.id == segment_candidate.channel.id and
                    existing.matched_model == segment_candidate.matched_model):
                    duplicate_found = True
                    break

            if not duplicate_found:
                all_candidates.append(segment_candidate)

        if all_candidates:
            physical_count = len(physical_candidates)
            segment_count = len(complete_segment_candidates)
            total_count = len(all_candidates)
            logger.info(f"COMPREHENSIVE SEARCH: Found {total_count} total candidates "
                       f"(physical: {physical_count}, complete-segment: {segment_count}, merged without duplicates)")
            return all_candidates

        # 4. æœ€åå°è¯•ä»é…ç½®ä¸­æŸ¥æ‰¾
        config_channels = self.config_loader.get_channels_by_model(request.model)
        if config_channels:
            logger.info(f"CONFIG FALLBACK: Found {len(config_channels)} channels in configuration for model '{request.model}'")
            # å¯¹äºé…ç½®æŸ¥æ‰¾ï¼Œå°è¯•è·å–çœŸå®çš„æ¨¡å‹ID
            config_candidates = []
            for ch in config_channels:
                real_model_id = request.model
                if ch.id in model_cache:
                    discovered_info = model_cache[ch.id]
                    models_data = discovered_info.get("models_data", {}) if isinstance(discovered_info, dict) else {}
                    if models_data and request.model in models_data:
                        model_info = models_data[request.model]
                        real_model_id = model_info.get("id", request.model)

                config_candidates.append(ChannelCandidate(channel=ch, matched_model=real_model_id))
            return config_candidates

        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›ç©ºåˆ—è¡¨
        logger.warning(f"NO MATCH: No channels found for model '{request.model}' (tried physical, auto-tag, and config)")
        return []

    def _filter_channels(self, channels: list[ChannelCandidate], request: RoutingRequest) -> list[ChannelCandidate]:
        """è¿‡æ»¤æ¸ é“ï¼ŒåŒ…å«èƒ½åŠ›æ£€æµ‹"""
        filtered = []

        # è·å–è·¯ç”±é…ç½®ä¸­çš„è¿‡æ»¤æ¡ä»¶
        routing_config = self.config.routing if hasattr(self.config, 'routing') else None
        if routing_config and hasattr(routing_config, 'model_filters'):
            model_filters = routing_config.model_filters or {}
        else:
            model_filters = {}

        # å®‰å…¨è·å–model_filtersä¸­çš„å€¼
        min_context_length = getattr(model_filters, 'min_context_length', 0) if hasattr(model_filters, 'min_context_length') else model_filters.get('min_context_length', 0) if isinstance(model_filters, dict) else 0
        min_parameter_count = getattr(model_filters, 'min_parameter_count', 0) if hasattr(model_filters, 'min_parameter_count') else model_filters.get('min_parameter_count', 0) if isinstance(model_filters, dict) else 0
        exclude_embedding = getattr(model_filters, 'exclude_embedding_models', True) if hasattr(model_filters, 'exclude_embedding_models') else model_filters.get('exclude_embedding_models', True) if isinstance(model_filters, dict) else True
        exclude_vision_only = getattr(model_filters, 'exclude_vision_only_models', True) if hasattr(model_filters, 'exclude_vision_only_models') else model_filters.get('exclude_vision_only_models', True) if isinstance(model_filters, dict) else True

        for candidate in channels:
            channel = candidate.channel
            if not channel.enabled or not channel.api_key:
                continue

            # ğŸš€ ä¼˜åŒ–å¥åº·åˆ†æ•°æ£€æŸ¥ï¼ˆé¿å…é‡å¤è®¿é—®ï¼‰
            if not hasattr(self, '_cached_health_scores'):
                self._cached_health_scores = self.config_loader.runtime_state.health_scores.copy()

            health_score = self._cached_health_scores.get(channel.id, 1.0)
            if health_score < 0.3:
                continue

            # æ¨¡å‹è§„æ ¼è¿‡æ»¤
            if candidate.matched_model and (min_context_length > 0 or min_parameter_count > 0 or exclude_embedding or exclude_vision_only):
                model_name = candidate.matched_model

                # æ£€æŸ¥æ˜¯å¦ä¸ºembeddingæ¨¡å‹
                if exclude_embedding and self._is_embedding_model(model_name):
                    logger.debug(f"Filtered out embedding model: {model_name}")
                    continue

                # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯è§†è§‰æ¨¡å‹
                if exclude_vision_only and self._is_vision_only_model(model_name):
                    logger.debug(f"Filtered out vision-only model: {model_name}")
                    continue

                # è·å–æ¨¡å‹è§„æ ¼
                model_specs = self._get_model_specs(channel.id, model_name)

                # ä¸Šä¸‹æ–‡é•¿åº¦è¿‡æ»¤
                if min_context_length > 0:
                    context_length = model_specs.get('context_length', 0) if model_specs else 0
                    if context_length < min_context_length:
                        logger.debug(f"Filtered out model {model_name}: context {context_length} < {min_context_length}")
                        continue

                # å‚æ•°æ•°é‡è¿‡æ»¤
                if min_parameter_count > 0:
                    param_count = model_specs.get('parameter_count', 0) if model_specs else 0
                    if param_count < min_parameter_count:
                        logger.debug(f"Filtered out model {model_name}: params {param_count}M < {min_parameter_count}M")
                        continue

            # TODO: Add capability filtering when needed
            # if request.required_capabilities:
            #     if not all(cap in channel.capabilities for cap in request.required_capabilities):
            #         continue

            filtered.append(candidate)
        return filtered

    async def _score_channels(self, channels: list[ChannelCandidate], request: RoutingRequest) -> list[RoutingScore]:
        """è®¡ç®—æ¸ é“è¯„åˆ† - æ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒæ…¢æŸ¥è¯¢æ£€æµ‹"""
        start_time = time.time()
        channel_count = len(channels)

        logger.info(f"ğŸ“Š SCORING: Evaluating {channel_count} candidate channels for model '{request.model}'")

        # å¦‚æœæ¸ é“æ•°é‡è¾ƒå°‘ï¼Œä½¿ç”¨åŸæœ‰çš„å•ä¸ªè¯„åˆ†æ–¹å¼
        if channel_count < 5:
            result = await self._score_channels_individual(channels, request)
            elapsed_ms = (time.time() - start_time) * 1000
            self._log_performance_metrics(channel_count, elapsed_ms, "individual")
            return result

        # ä½¿ç”¨æ‰¹é‡è¯„åˆ†å™¨è¿›è¡Œä¼˜åŒ–
        if not hasattr(self, '_batch_scorer'):
            from core.utils.batch_scorer import BatchScorer
            self._batch_scorer = BatchScorer(self)

        # æ‰¹é‡è®¡ç®—æ‰€æœ‰è¯„åˆ†ï¼Œè·å–æ€§èƒ½æŒ‡æ ‡
        batch_result, metrics = await self._batch_scorer.batch_score_channels(channels, request)


        # æ„å»ºè¯„åˆ†ç»“æœ
        scored_channels = []
        strategy = self._get_routing_strategy(request.model)

        logger.info(f"ğŸ“Š SCORING: Using routing strategy with {len(strategy)} rules")
        for rule in strategy:
            logger.debug(f"ğŸ“Š SCORING: Strategy rule: {rule['field']} (weight: {rule['weight']}, order: {rule['order']})")

        for candidate in channels:
            # ä»æ‰¹é‡ç»“æœä¸­è·å–è¯„åˆ†
            scores = self._batch_scorer.get_score_for_channel(batch_result, candidate)

            total_score = self._calculate_total_score(
                strategy,
                scores['cost_score'], scores['speed_score'], scores['quality_score'],
                scores['reliability_score'], scores['parameter_score'], scores['context_score'],
                scores['free_score'], scores['local_score']
            )

            # ç®€åŒ–æ—¥å¿—è¾“å‡º
            model_display = candidate.matched_model or candidate.channel.model_name
            # åªåœ¨è°ƒè¯•æ¨¡å¼è®°å½•è¯¦ç»†è¯„åˆ†ï¼Œç”Ÿäº§æ¨¡å¼å‡å°‘æ—¥å¿—å™ªéŸ³
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ğŸ“Š SCORE: '{candidate.channel.name}' -> '{model_display}' = {total_score:.3f} (Q:{scores['quality_score']:.2f})")

            scored_channels.append(RoutingScore(
                channel=candidate.channel, total_score=total_score,
                cost_score=scores['cost_score'], speed_score=scores['speed_score'],
                quality_score=scores['quality_score'], reliability_score=scores['reliability_score'],
                reason=f"cost:{scores['cost_score']:.2f} speed:{scores['speed_score']:.2f} quality:{scores['quality_score']:.2f} reliability:{scores['reliability_score']:.2f}",
                matched_model=candidate.matched_model,
                # é¢„è®¡ç®—å±‚æ¬¡æ’åºæ‰€éœ€çš„é¢å¤–è¯„åˆ†
                parameter_score=scores['parameter_score'],
                context_score=scores['context_score'],
                free_score=scores['free_score']
            ))

        # ä½¿ç”¨åˆ†å±‚ä¼˜å…ˆçº§æ’åº
        scored_channels = self._hierarchical_sort(scored_channels)

        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        total_elapsed_ms = (time.time() - start_time) * 1000
        self._log_performance_metrics(channel_count, total_elapsed_ms, "batch", metrics)

        logger.info(f"ğŸ† SCORING RESULT: Channels ranked by score (computed in {total_elapsed_ms:.1f}ms):")
        for i, scored in enumerate(scored_channels[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            logger.info(f"ğŸ†   #{i+1}: '{scored.channel.name}' (Score: {scored.total_score:.3f})")

        return scored_channels

    async def _score_channels_individual(self, channels: list[ChannelCandidate], request: RoutingRequest) -> list[RoutingScore]:
        """å•ä¸ªæ¸ é“è¯„åˆ†æ–¹å¼ï¼ˆç”¨äºå°æ•°é‡æ¸ é“ï¼‰"""
        logger.info(f"ğŸ“Š SCORING: Using individual scoring for {len(channels)} channels")

        scored_channels = []
        strategy = self._get_routing_strategy(request.model)

        for candidate in channels:
            channel = candidate.channel
            cost_score = self._calculate_cost_score(channel, request)
            speed_score = self._calculate_speed_score(channel)
            quality_score = self._calculate_quality_score(channel, candidate.matched_model)
            reliability_score = self._calculate_reliability_score(channel)
            parameter_score = self._calculate_parameter_score(channel, candidate.matched_model)
            context_score = self._calculate_context_score(channel, candidate.matched_model)
            free_score = self._calculate_free_score(channel, candidate.matched_model)
            local_score = self._calculate_local_score(channel, candidate.matched_model)

            total_score = self._calculate_total_score(
                strategy, cost_score, speed_score, quality_score, reliability_score,
                parameter_score, context_score, free_score, local_score
            )

            model_display = candidate.matched_model or channel.model_name
            # åªåœ¨è°ƒè¯•æ¨¡å¼è®°å½•è¯¦ç»†è¯„åˆ†ï¼Œç”Ÿäº§æ¨¡å¼å‡å°‘æ—¥å¿—å™ªéŸ³
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ğŸ“Š SCORE: '{channel.name}' -> '{model_display}' = {total_score:.3f} (Q:{quality_score:.2f})")

            scored_channels.append(RoutingScore(
                channel=channel, total_score=total_score, cost_score=cost_score,
                speed_score=speed_score, quality_score=quality_score,
                reliability_score=reliability_score,
                reason=f"cost:{cost_score:.2f} speed:{speed_score:.2f} quality:{quality_score:.2f} reliability:{reliability_score:.2f}",
                matched_model=candidate.matched_model,
                # é¢„è®¡ç®—å±‚æ¬¡æ’åºæ‰€éœ€çš„é¢å¤–è¯„åˆ†
                parameter_score=parameter_score,
                context_score=context_score,
                free_score=free_score
            ))

        scored_channels = self._hierarchical_sort(scored_channels)

        # è®°å½•è¯„åˆ†ç»“æœæ‘˜è¦
        logger.info(f"ğŸ† INDIVIDUAL SCORING RESULT: Processed {len(scored_channels)} channels")
        for i, scored in enumerate(scored_channels[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            logger.info(f"ğŸ†   #{i+1}: '{scored.channel.name}' (Score: {scored.total_score:.3f})")

        return scored_channels

    def _get_routing_strategy(self, model: str) -> list[dict[str, Any]]:
        """è·å–å¹¶è§£æè·¯ç”±ç­–ç•¥ï¼Œå§‹ç»ˆè¿”å›è§„åˆ™åˆ—è¡¨"""
        routing_config = self.config.routing if hasattr(self.config, 'routing') else None
        if routing_config and hasattr(routing_config, 'default_strategy'):
            strategy_name = routing_config.default_strategy or 'balanced'
        else:
            strategy_name = 'balanced'

        # å°è¯•ä»é…ç½®ä¸­è·å–è‡ªå®šä¹‰ç­–ç•¥
        if routing_config and hasattr(routing_config, 'sorting_strategies'):
            custom_strategies = routing_config.sorting_strategies or {}
        else:
            custom_strategies = {}
        if strategy_name in custom_strategies:
            return custom_strategies[strategy_name]

        # å›é€€åˆ°é¢„å®šä¹‰ç­–ç•¥
        predefined_strategies = {
            "cost_first": [
                {"field": "cost_score", "order": "desc", "weight": 0.4},
                {"field": "parameter_score", "order": "desc", "weight": 0.25},
                {"field": "context_score", "order": "desc", "weight": 0.2},
                {"field": "speed_score", "order": "desc", "weight": 0.15}
            ],
            "free_first": [
                {"field": "free_score", "order": "desc", "weight": 0.5},
                {"field": "cost_score", "order": "desc", "weight": 0.3},
                {"field": "speed_score", "order": "desc", "weight": 0.15},
                {"field": "reliability_score", "order": "desc", "weight": 0.05}
            ],
            "local_first": [
                {"field": "local_score", "order": "desc", "weight": 0.6},
                {"field": "speed_score", "order": "desc", "weight": 0.25},
                {"field": "cost_score", "order": "desc", "weight": 0.1},
                {"field": "reliability_score", "order": "desc", "weight": 0.05}
            ],
            "cost_optimized": [
                {"field": "cost_score", "order": "desc", "weight": 0.7},
                {"field": "reliability_score", "order": "desc", "weight": 0.2},
                {"field": "speed_score", "order": "desc", "weight": 0.1}
            ],
            "speed_optimized": [
                {"field": "speed_score", "order": "desc", "weight": 0.4},
                {"field": "cost_score", "order": "desc", "weight": 0.3},
                {"field": "parameter_score", "order": "desc", "weight": 0.2},
                {"field": "context_score", "order": "desc", "weight": 0.1}
            ],
            "quality_optimized": [
                {"field": "parameter_score", "order": "desc", "weight": 0.4},
                {"field": "context_score", "order": "desc", "weight": 0.3},
                {"field": "quality_score", "order": "desc", "weight": 0.2},
                {"field": "cost_score", "order": "desc", "weight": 0.1}
            ],
            "balanced": [
                {"field": "cost_score", "order": "desc", "weight": 0.3},
                {"field": "parameter_score", "order": "desc", "weight": 0.25},
                {"field": "context_score", "order": "desc", "weight": 0.2},
                {"field": "speed_score", "order": "desc", "weight": 0.15},
                {"field": "reliability_score", "order": "desc", "weight": 0.1}
            ]
        }

        return predefined_strategies.get(strategy_name, predefined_strategies["cost_first"])

    def _calculate_cost_score(self, channel: Channel, request: RoutingRequest) -> float:
        """è®¡ç®—æˆæœ¬è¯„åˆ†(0-1ï¼Œè¶Šä½æˆæœ¬è¶Šé«˜åˆ†)"""
        pricing = channel.pricing
        if not pricing:
            return 0.5

        # ä¼°ç®—tokenæ•°é‡
        input_tokens = self._estimate_tokens(request.messages)
        max_output_tokens = request.max_tokens or 1000

        # è®¡ç®—æˆæœ¬
        input_cost = pricing.get("input_cost_per_1k", 0.001) * input_tokens / 1000
        output_cost = pricing.get("output_cost_per_1k", 0.002) * max_output_tokens / 1000
        total_cost = (input_cost + output_cost) * pricing.get("effective_multiplier", 1.0)

        # è½¬æ¢ä¸ºè¯„åˆ† (æˆæœ¬è¶Šä½åˆ†æ•°è¶Šé«˜)
        # å‡è®¾æœ€é«˜æˆæœ¬ä¸º0.1ç¾å…ƒ
        max_cost = 0.1
        score = max(0, 1 - (total_cost / max_cost))

        return min(1.0, score)

    # å†…ç½®æ¨¡å‹è´¨é‡æ’å (åˆ†æ•°è¶Šé«˜è¶Šå¥½, æ»¡åˆ†100)
    MODEL_QUALITY_RANKING = {
        # OpenAI ç³»åˆ—
        "gpt-4o": 98, "gpt-4-turbo": 95, "gpt-4": 90, "gpt-4o-mini": 85, "gpt-3.5-turbo": 70,
        # Anthropic ç³»åˆ—
        "claude-3-5-sonnet": 99, "claude-3-opus": 97, "claude-3-sonnet": 92, "claude-3-haiku": 80,
        # Meta Llama ç³»åˆ—
        "llama-3.1-70b": 93, "llama-3.1-8b": 82, "llama-3-70b": 90, "llama-3-8b": 80,
        # Qwen ç³»åˆ—
        "qwen2.5-72b-instruct": 91, "qwen2-72b-instruct": 90, "qwen2-7b-instruct": 78,
        "qwen3-coder": 88, "qwen3-30b": 85, "qwen3-14b": 82, "qwen3-8b": 80, "qwen3-4b": 75,
        "qwen3-1.7b": 65, "qwen3-0.6b": 60,
        # DeepSeek ç³»åˆ—
        "deepseek-r1": 89, "deepseek-v3": 87, "deepseek-coder": 85,
        # Moonshot ç³»åˆ—
        "moonshot-v1-128k": 88, "moonshot-v1-32k": 87, "moonshot-v1-8k": 86,
        # 01.AI ç³»åˆ—
        "yi-large": 89, "yi-lightning": 75,
        # Google ç³»åˆ—
        "gemma-3-12b": 78, "gemma-3-9b": 75, "gemma-3-270m": 55,
    }

    def _calculate_quality_score(self, channel: Channel, matched_model: Optional[str] = None) -> float:
        """æ ¹æ®å†…ç½®æ’åå’Œæ¨¡å‹è§„æ ¼åŠ¨æ€è®¡ç®—è´¨é‡è¯„åˆ†"""
        # ä¼˜å…ˆä½¿ç”¨åŒ¹é…çš„æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ¸ é“é»˜è®¤æ¨¡å‹
        model_name = matched_model or channel.model_name
        model_name_lower = model_name.lower()
        simple_model_name = model_name_lower.split('/')[-1]

        # 1. åŸºç¡€è´¨é‡è¯„åˆ†ï¼ˆä»å†…ç½®æ’åï¼‰
        base_quality_score = self.MODEL_QUALITY_RANKING.get(simple_model_name)

        if base_quality_score is None:
            for key, score in self.MODEL_QUALITY_RANKING.items():
                if key in model_name_lower:
                    base_quality_score = score
                    break

        # 2. è·å–æ¨¡å‹è§„æ ¼ä¿¡æ¯
        model_specs = None
        try:
            # ä»æ¸ é“ç¼“å­˜ä¸­è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
            channel_cache = self.cache_manager.load_channel_models(channel.id)
            if channel_cache and 'models' in channel_cache:
                model_specs = channel_cache['models'].get(model_name)
        except Exception as e:
            logger.debug(f"Failed to load model specs for {model_name}: {e}")

        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œä½¿ç”¨åˆ†æå™¨åˆ†ææ¨¡å‹åç§°
        if not model_specs:
            analyzed_specs = self.model_analyzer.analyze_model(model_name)
            model_specs = {
                'parameter_count': analyzed_specs.parameter_count,
                'context_length': analyzed_specs.context_length
            }

        # 3. æ ¹æ®å‚æ•°æ•°é‡è°ƒæ•´è¯„åˆ†
        param_multiplier = 1.0
        if model_specs and 'parameter_count' in model_specs:
            param_count = model_specs['parameter_count']
            if param_count:
                # å‚æ•°æ•°é‡è¯„åˆ†æ›²çº¿ï¼ˆè¶Šå¤§è¶Šå¥½ï¼Œä½†æœ‰è¾¹é™…é€’å‡æ•ˆåº”ï¼‰
                if param_count >= 100000:      # 100B+
                    param_multiplier = 1.3
                elif param_count >= 70000:     # 70B+
                    param_multiplier = 1.25
                elif param_count >= 30000:     # 30B+
                    param_multiplier = 1.2
                elif param_count >= 8000:      # 8B+
                    param_multiplier = 1.1
                elif param_count >= 4000:      # 4B+
                    param_multiplier = 1.05
                elif param_count >= 1000:      # 1B+
                    param_multiplier = 1.0
                elif param_count >= 500:       # 500M+
                    param_multiplier = 0.9
                else:                          # <500M
                    param_multiplier = 0.8

                logger.debug(f"Model {model_name}: {param_count}M params -> multiplier {param_multiplier}")

        # 4. æ ¹æ®ä¸Šä¸‹æ–‡é•¿åº¦è°ƒæ•´è¯„åˆ†
        context_bonus = 0.0
        if model_specs and 'context_length' in model_specs:
            context_length = model_specs['context_length']
            if context_length:
                # ä¸Šä¸‹æ–‡é•¿åº¦å¥–åŠ±ï¼ˆæ›´é•¿çš„ä¸Šä¸‹æ–‡åœ¨åŒç­‰æ¡ä»¶ä¸‹æ›´å¥½ï¼‰
                if context_length >= 200000:     # 200k+
                    context_bonus = 15
                elif context_length >= 128000:   # 128k+
                    context_bonus = 10
                elif context_length >= 32000:    # 32k+
                    context_bonus = 5
                elif context_length >= 16000:    # 16k+
                    context_bonus = 2

                logger.debug(f"Model {model_name}: {context_length} context -> bonus {context_bonus}")

        # 5. è®¡ç®—æœ€ç»ˆè¯„åˆ†
        final_score = (base_quality_score or 70) * param_multiplier + context_bonus

        # 6. å½’ä¸€åŒ–åˆ°0-1èŒƒå›´ï¼Œæœ€é«˜åˆ†è®¾ä¸º120åˆ†
        normalized_score = min(1.0, final_score / 120.0)

        logger.debug(f"Quality scoring for {model_name}: base={base_quality_score}, "
                    f"param_mult={param_multiplier}, context_bonus={context_bonus}, "
                    f"final={final_score}, normalized={normalized_score:.3f}")

        return normalized_score

    def _calculate_parameter_score(self, channel: Channel, matched_model: Optional[str] = None) -> float:
        """è®¡ç®—å‚æ•°æ•°é‡è¯„åˆ†"""
        model_name = matched_model or channel.model_name
        model_specs = self._get_model_specs(channel.id, model_name)

        if not model_specs or not model_specs.get('parameter_count'):
            return 0.5  # é»˜è®¤ä¸­ç­‰è¯„åˆ†

        param_count = model_specs['parameter_count']

        # å‚æ•°æ•°é‡è¯„åˆ†æ›²çº¿ï¼ˆå¯¹æ•°å¼é€’å¢ï¼Œæœ‰è¾¹é™…é€’å‡æ•ˆåº”ï¼‰
        if param_count >= 500000:      # 500B+
            score = 1.0
        elif param_count >= 100000:    # 100B+
            score = 0.95
        elif param_count >= 70000:     # 70B+
            score = 0.9
        elif param_count >= 30000:     # 30B+
            score = 0.85
        elif param_count >= 8000:      # 8B+
            score = 0.8
        elif param_count >= 4000:      # 4B+
            score = 0.75
        elif param_count >= 1000:      # 1B+
            score = 0.7
        elif param_count >= 500:       # 500M+
            score = 0.6
        else:                          # <500M
            score = 0.4

        logger.debug(f"Parameter score for {model_name}: {param_count}M -> {score}")
        return score

    def _calculate_context_score(self, channel: Channel, matched_model: Optional[str] = None) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡é•¿åº¦è¯„åˆ†"""
        model_name = matched_model or channel.model_name
        model_specs = self._get_model_specs(channel.id, model_name)

        if not model_specs or not model_specs.get('context_length'):
            return 0.5  # é»˜è®¤ä¸­ç­‰è¯„åˆ†

        context_length = model_specs['context_length']

        # ä¸Šä¸‹æ–‡é•¿åº¦è¯„åˆ†æ›²çº¿
        if context_length >= 1000000:    # 1M+
            score = 1.0
        elif context_length >= 200000:   # 200k+
            score = 0.95
        elif context_length >= 128000:   # 128k+
            score = 0.9
        elif context_length >= 32000:    # 32k+
            score = 0.8
        elif context_length >= 16000:    # 16k+
            score = 0.7
        elif context_length >= 8000:     # 8k+
            score = 0.6
        elif context_length >= 4000:     # 4k+
            score = 0.5
        else:                            # <4k
            score = 0.3

        logger.debug(f"Context score for {model_name}: {context_length} -> {score}")
        return score

    def _get_model_specs(self, channel_id: str, model_name: str) -> Optional[dict[str, Any]]:
        """è·å–æ¨¡å‹è§„æ ¼ä¿¡æ¯ï¼ˆå†…å­˜ç´¢å¼•ä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            # ğŸš€ ä¼˜å…ˆä½¿ç”¨å†…å­˜ç´¢å¼•ï¼ˆæ¶ˆé™¤æ–‡ä»¶I/Oç“¶é¢ˆï¼‰
            from core.utils.memory_index import get_memory_index
            memory_index = get_memory_index()
            specs = memory_index.get_model_specs(channel_id, model_name)
            if specs:
                return specs

        except Exception as e:
            logger.debug(f"Memory index specs lookup failed for {model_name}: {e}")

        try:
            # å›é€€ï¼šä»æ¸ é“ç¼“å­˜ä¸­è·å–
            channel_cache = self.cache_manager.load_channel_models(channel_id)
            if channel_cache and 'models' in channel_cache:
                return channel_cache['models'].get(model_name)
        except Exception as e:
            logger.debug(f"Failed to load model specs for {model_name}: {e}")

        # æœ€åå›é€€åˆ°åˆ†æå™¨åˆ†æ
        analyzed_specs = self.model_analyzer.analyze_model(model_name)
        return {
            'parameter_count': analyzed_specs.parameter_count,
            'context_length': analyzed_specs.context_length
        }

    def _is_embedding_model(self, model_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºembeddingæ¨¡å‹"""
        name_lower = model_name.lower()
        embedding_keywords = ['embedding', 'embed', 'text-embedding', 'bge-', 'gte-', 'e5-']
        return any(keyword in name_lower for keyword in embedding_keywords)

    def _is_vision_only_model(self, model_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºçº¯è§†è§‰æ¨¡å‹"""
        name_lower = model_name.lower()
        vision_keywords = ['vision-only', 'image-only', 'ocr-only']
        return any(keyword in name_lower for keyword in vision_keywords)

    def _calculate_speed_score(self, channel: Channel) -> float:
        """æ ¹æ®å¹³å‡å»¶è¿ŸåŠ¨æ€è®¡ç®—é€Ÿåº¦è¯„åˆ†"""
        channel_stats = self.config_loader.runtime_state.channel_stats.get(channel.id)
        if channel_stats and "avg_latency_ms" in channel_stats:
            avg_latency = channel_stats["avg_latency_ms"]
            base_latency = 2000.0
            score = max(0.0, 1.0 - (avg_latency / base_latency))
            return 0.1 + score * 0.9
        return channel.performance.get("speed_score", 0.8)

    def _calculate_reliability_score(self, channel: Channel) -> float:
        """è®¡ç®—å¯é æ€§è¯„åˆ†ï¼ˆå®Œå…¨åå°åŒ–ï¼šåªä½¿ç”¨ç¼“å­˜ç»“æœï¼‰"""
        # ğŸš€ å®Œå…¨ä¾èµ–åå°ä»»åŠ¡ç»´æŠ¤çš„ç¼“å­˜æ•°æ®ï¼Œä¸»çº¿ç¨‹ä¸æ‰§è¡Œä»»ä½•å¥åº·æ£€æŸ¥
        # 1. ä¼˜å…ˆä»å†…å­˜ç´¢å¼•è·å–ç¼“å­˜çš„å¥åº·è¯„åˆ†
        try:
            from core.utils.memory_index import get_memory_index
            memory_index = get_memory_index()
            cached_health = memory_index.get_health_score(channel.id, cache_ttl=300.0)  # 5åˆ†é’ŸTTL
            if cached_health is not None:
                return cached_health
        except Exception:
            pass

        # 2. å›é€€åˆ°è¿è¡Œæ—¶çŠ¶æ€ç¼“å­˜ï¼ˆç”±åå°å¥åº·æ£€æŸ¥ä»»åŠ¡æ›´æ–°ï¼‰
        health_score = self.config_loader.runtime_state.health_scores.get(channel.id, 1.0)

        # 3. ğŸš€ å¼‚æ­¥æ›´æ–°å†…å­˜ç¼“å­˜ï¼ˆéé˜»å¡ï¼‰
        try:
            from core.utils.memory_index import get_memory_index
            memory_index = get_memory_index()
            memory_index.set_health_score(channel.id, health_score)
        except Exception:
            pass  # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»è·¯ç”±æ€§èƒ½

        return health_score

    def _calculate_free_score(self, channel: Channel, model_name: str = None) -> float:
        """è®¡ç®—å…è´¹ä¼˜å…ˆè¯„åˆ† - ä¸¥æ ¼éªŒè¯çœŸæ­£å…è´¹çš„æ¸ é“"""
        free_tags = {"free", "å…è´¹", "0cost", "nocost", "trial"}

        # ğŸ”¥ ä¼˜å…ˆæ£€æŸ¥æ¨¡å‹çº§åˆ«çš„å®šä»·ä¿¡æ¯ï¼ˆä»ç¼“å­˜ä¸­è·å–ï¼‰
        if model_name:
            model_specs = self._get_model_specs(channel.id, model_name)
            if model_specs and 'raw_data' in model_specs:
                raw_data = model_specs['raw_data']
                if 'pricing' in raw_data:
                    pricing = raw_data['pricing']
                    prompt_cost = float(pricing.get('prompt', '0'))
                    completion_cost = float(pricing.get('completion', '0'))
                    if prompt_cost == 0.0 and completion_cost == 0.0:
                        logger.debug(f"FREE SCORE: Model '{model_name}' confirmed free via model-level pricing (prompt={prompt_cost}, completion={completion_cost})")
                        return 1.0
                    else:
                        logger.debug(f"FREE SCORE: Model '{model_name}' has non-zero costs (prompt={prompt_cost}, completion={completion_cost}), not free")
                        return 0.1

        # ğŸ”¥ æ£€æŸ¥æ¨¡å‹åç§°æ¨¡å¼ - å¯¹äºæ˜ç¡®çš„å…è´¹æ¨¡å‹åç¼€
        if model_name:
            model_lower = model_name.lower()
            # æ˜ç¡®çš„å…è´¹æ¨¡å‹æ ‡è¯†
            if (":free" in model_lower or
                "-free" in model_lower or
                "_free" in model_lower):
                logger.debug(f"FREE SCORE: Model '{model_name}' has explicit :free suffix, assumed free")
                return 1.0

            # æ£€æŸ¥å…¶ä»–å…è´¹æ¨¡å‹æ¨¡å¼
            model_tags = self._extract_tags_from_model_name(model_name)
            model_tags_lower = [tag.lower() for tag in model_tags]
            if any(tag in free_tags for tag in model_tags_lower):
                # å¼€æºæ¨¡å‹é€šå¸¸å…è´¹
                if ("oss" in model_lower or
                    "huggingface" in model_lower or
                    "hf" in model_lower):
                    logger.debug(f"FREE SCORE: Model '{model_name}' is open source model, assumed free")
                    return 1.0
                else:
                    # æ¨¡å‹åç§°åŒ…å«freeä½†éœ€è¦è¿›ä¸€æ­¥éªŒè¯
                    logger.debug(f"FREE SCORE: Model '{model_name}' has 'free' in name, needs verification")
                    # ç»§ç»­æ£€æŸ¥æ¸ é“çº§åˆ«ä¿¡æ¯

        # ğŸ”¥ æ£€æŸ¥æ¸ é“çº§åˆ«çš„å®šä»·ä¿¡æ¯
        cost_per_token = getattr(channel, 'cost_per_token', None)
        if cost_per_token:
            input_cost = cost_per_token.get("input", 0.0)
            output_cost = cost_per_token.get("output", 0.0)
            if input_cost <= 0.0 and output_cost <= 0.0:
                logger.debug(f"FREE SCORE: Channel '{channel.name}' confirmed free via cost_per_token (input={input_cost}, output={output_cost})")
                return 1.0
            # å¯¹äºæœ‰cost_per_tokenä½†éé›¶çš„æƒ…å†µï¼Œå¦‚æœæ¨¡å‹åæ˜ç¡®åŒ…å«:freeï¼Œä»è®¤ä¸ºè¯¥ç‰¹å®šæ¨¡å‹å…è´¹
            elif model_name and (":free" in model_name.lower() or "-free" in model_name.lower()):
                logger.debug(f"FREE SCORE: Channel '{channel.name}' has costs but model '{model_name}' explicitly marked as free")
                return 1.0
            else:
                # æ¸ é“æœ‰æˆæœ¬ä¸”æ¨¡å‹æœªæ˜ç¡®æ ‡è®°ä¸ºå…è´¹
                logger.debug(f"FREE SCORE: Channel '{channel.name}' has non-zero costs (input={input_cost}, output={output_cost}), not free")
                return 0.1

        # æ£€æŸ¥ä¼ ç»Ÿçš„pricingé…ç½®
        pricing = getattr(channel, 'pricing', None)
        if pricing:
            input_cost = pricing.get("input_cost_per_1k", 0.001)
            output_cost = pricing.get("output_cost_per_1k", 0.002)
            avg_cost = (input_cost + output_cost) / 2
            if avg_cost <= 0.0001:  # æä½æˆæœ¬
                logger.debug(f"FREE SCORE: Channel '{channel.name}' very low cost (avg={avg_cost:.6f}), scored as near-free")
                return 0.9
            elif avg_cost <= 0.001:  # ä½æˆæœ¬
                logger.debug(f"FREE SCORE: Channel '{channel.name}' low cost (avg={avg_cost:.6f}), scored as affordable")
                return 0.7
            else:
                # æœ‰æ˜ç¡®pricingä¸”ä¸ä¾¿å®œçš„ï¼Œä½†å¦‚æœæ¨¡å‹æ˜ç¡®æ ‡è®°ä¸º:freeï¼Œä»è®¤ä¸ºå…è´¹
                if model_name and (":free" in model_name.lower() or "-free" in model_name.lower()):
                    logger.debug(f"FREE SCORE: Channel has costs but model '{model_name}' explicitly marked as free")
                    return 1.0
                else:
                    logger.debug(f"FREE SCORE: Channel '{channel.name}' has significant costs (avg={avg_cost:.6f}), not free")
                    return 0.1

        # ğŸ”¥ æ£€æŸ¥æ¸ é“æ ‡ç­¾
        channel_tags_lower = [tag.lower() for tag in getattr(channel, 'tags', [])]
        if any(tag in free_tags for tag in channel_tags_lower):
            logger.debug(f"FREE SCORE: Channel '{channel.name}' has free tag in channel tags, assumed free")
            return 1.0

        # é»˜è®¤æƒ…å†µ - æ²¡æœ‰å…è´¹è¯æ®
        logger.debug(f"FREE SCORE: Channel '{channel.name}' no evidence of being free")
        return 0.1

    def _calculate_local_score(self, channel: Channel, model_name: str = None) -> float:
        """è®¡ç®—æœ¬åœ°ä¼˜å…ˆè¯„åˆ†"""
        # æ£€æŸ¥æ¸ é“æ ‡ç­¾ä¸­æ˜¯å¦åŒ…å«æœ¬åœ°ç›¸å…³æ ‡ç­¾
        local_tags = {"local", "æœ¬åœ°", "localhost", "127.0.0.1", "offline", "edge"}

        # ä»æ¸ é“æ ‡ç­¾æ£€æŸ¥
        channel_tags_lower = [tag.lower() for tag in channel.tags]
        if any(tag in local_tags for tag in channel_tags_lower):
            return 1.0

        # ä»æ¨¡å‹åç§°æå–æ ‡ç­¾æ£€æŸ¥
        if model_name:
            model_tags = self._extract_tags_from_model_name(model_name)
            model_tags_lower = [tag.lower() for tag in model_tags]
            if any(tag in local_tags for tag in model_tags_lower):
                return 1.0

        # æ£€æŸ¥base_urlæ˜¯å¦æŒ‡å‘æœ¬åœ°åœ°å€
        base_url = getattr(channel, 'base_url', None)
        if base_url:
            base_url_lower = base_url.lower()
            local_indicators = ["localhost", "127.0.0.1", "0.0.0.0", "::1"]
            if any(indicator in base_url_lower for indicator in local_indicators):
                return 1.0

        # æ£€æŸ¥provideræ˜¯å¦æŒ‡å‘æœ¬åœ°åœ°å€
        provider_config = None
        if hasattr(self.config, 'providers'):
            provider_config = next((p for p in self.config.providers.values() if p.name == channel.provider), None)

        if provider_config and hasattr(provider_config, 'base_url'):
            provider_url_lower = provider_config.base_url.lower()
            local_indicators = ["localhost", "127.0.0.1", "0.0.0.0", "::1"]
            if any(indicator in provider_url_lower for indicator in local_indicators):
                return 1.0

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§æœ¬åœ°æ¨¡å‹åç§°
        if model_name:
            local_model_patterns = ["ollama", "llama.cpp", "local", "è‡ªå·±çš„", "ç§æœ‰"]
            model_lower = model_name.lower()
            if any(pattern in model_lower for pattern in local_model_patterns):
                return 0.8

        return 0.1  # é»˜è®¤è¾ƒä½è¯„åˆ†

    def _calculate_total_score(self, strategy: list[dict[str, Any]],
                             cost_score: float, speed_score: float,
                             quality_score: float, reliability_score: float,
                             parameter_score: float = 0.5, context_score: float = 0.5,
                             free_score: float = 0.1, local_score: float = 0.1) -> float:
        """æ ¹æ®ç­–ç•¥è®¡ç®—æ€»è¯„åˆ†"""
        total_score = 0.0
        score_map = {
            "cost_score": cost_score, "speed_score": speed_score,
            "quality_score": quality_score, "reliability_score": reliability_score,
            "parameter_score": parameter_score, "context_score": context_score,
            "free_score": free_score, "local_score": local_score
        }

        total_weight = sum(rule.get("weight", 0.0) for rule in strategy)
        if total_weight == 0:
            return 0.5

        for rule in strategy:
            field = rule.get("field", "")
            if field in score_map:
                score = score_map[field]
                if rule.get("order") == "asc":
                    score = 1.0 - score
                total_score += score * rule.get("weight", 0.0)

        return total_score / total_weight

    def _hierarchical_sort(self, scored_channels: list[RoutingScore]) -> list[RoutingScore]:
        """åˆ†å±‚ä¼˜å…ˆçº§æ’åºï¼šä½¿ç”¨6ä½æ•°å­—è¯„åˆ†ç³»ç»Ÿ (æˆæœ¬|ä¸Šä¸‹æ–‡|å‚æ•°|é€Ÿåº¦|è´¨é‡|å¯é æ€§)

        æ³¨æ„ï¼šç§»é™¤äº†è‡ªåŠ¨æœ¬åœ°ä¼˜å…ˆé€»è¾‘ï¼Œåªæœ‰åœ¨ç”¨æˆ·æ˜ç¡®æŒ‡å®š local æ ‡ç­¾æˆ– local_first ç­–ç•¥æ—¶æ‰ä¼šä¼˜å…ˆæœ¬åœ°
        """
        def sorting_key(score: RoutingScore):
            # ä½¿ç”¨é¢„è®¡ç®—çš„è¯„åˆ†ä¿¡æ¯ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
            free_score = score.free_score
            parameter_score = score.parameter_score
            context_score = score.context_score

            # å°†æ¯ä¸ªç»´åº¦çš„è¯„åˆ†è½¬æ¢ä¸º0-9çš„æ•´æ•°è¯„åˆ†
            # ç¬¬1ä½ï¼šæˆæœ¬ä¼˜åŒ–ç¨‹åº¦ (9=å®Œå…¨å…è´¹, 8=å¾ˆä¾¿å®œ, 0=å¾ˆæ˜‚è´µ)
            if free_score >= 0.9:
                cost_tier = 9  # å…è´¹æ¨¡å‹å›ºå®šä¸º9åˆ†
            else:
                cost_tier = min(8, int(score.cost_score * 8))  # ä»˜è´¹æ¨¡å‹æœ€é«˜8åˆ†

            # ç¬¬2ä½ï¼šä¸Šä¸‹æ–‡é•¿åº¦ç¨‹åº¦ (9=å¾ˆé•¿, 0=å¾ˆçŸ­) - ä¼˜å…ˆçº§é«˜äºå‚æ•°é‡
            context_tier = min(9, int(context_score * 9))

            # ç¬¬3ä½ï¼šå‚æ•°é‡ç¨‹åº¦ (9=å¾ˆå¤§, 0=å¾ˆå°) - åœ¨å‚æ•°é‡æ¯”è¾ƒæŸ¥è¯¢ä¸­å…³é”®
            parameter_tier = min(9, int(parameter_score * 9))

            # ç¬¬4ä½ï¼šé€Ÿåº¦ç¨‹åº¦ (9=å¾ˆå¿«, 0=å¾ˆæ…¢)
            speed_tier = min(9, int(score.speed_score * 9))

            # ç¬¬5ä½ï¼šè´¨é‡ç¨‹åº¦ (9=å¾ˆé«˜, 0=å¾ˆä½)
            quality_tier = min(9, int(score.quality_score * 9))

            # ç¬¬6ä½ï¼šå¯é æ€§ç¨‹åº¦ (9=å¾ˆå¯é , 0=å¾ˆä¸å¯é )
            reliability_tier = min(9, int(score.reliability_score * 9))

            # ç»„æˆ6ä½æ•°å­—ï¼Œæ•°å­—è¶Šå¤§æ’åºè¶Šé å‰
            hierarchical_score = (
                cost_tier * 100000 +        # ç¬¬1ä½ï¼šæˆæœ¬(å…è´¹=9,ä»˜è´¹æœ€é«˜=8)
                context_tier * 10000 +      # ç¬¬2ä½ï¼šä¸Šä¸‹æ–‡(ä¼˜å…ˆçº§é«˜äºå‚æ•°é‡)
                parameter_tier * 1000 +     # ç¬¬3ä½ï¼šå‚æ•°é‡(åœ¨å‚æ•°é‡æ¯”è¾ƒæŸ¥è¯¢ä¸­å…³é”®)
                speed_tier * 100 +          # ç¬¬4ä½ï¼šé€Ÿåº¦
                quality_tier * 10 +         # ç¬¬5ä½ï¼šè´¨é‡
                reliability_tier            # ç¬¬6ä½ï¼šå¯é æ€§
            )

            return (-hierarchical_score, score.channel.name)

        logger.info("HIERARCHICAL SORTING: 6-digit scoring system (Cost|Context|Param|Speed|Quality|Reliability)")

        # æŒ‰åˆ†å±‚è¯„åˆ†æ’åº
        sorted_channels = sorted(scored_channels, key=sorting_key)

        # æ‰“å°æ’åºç»“æœç”¨äºè°ƒè¯•
        for i, scored in enumerate(sorted_channels[:5]):
            # ä½¿ç”¨é¢„è®¡ç®—çš„è¯„åˆ†ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
            free_score = scored.free_score
            parameter_score = scored.parameter_score
            context_score = scored.context_score

            # è®¡ç®—6ä½æ•°å­—è¯„åˆ†
            if free_score >= 0.9:
                cost_tier = 9  # å…è´¹æ¨¡å‹å›ºå®šä¸º9åˆ†
            else:
                cost_tier = min(8, int(scored.cost_score * 8))  # ä»˜è´¹æ¨¡å‹æœ€é«˜8åˆ†

            context_tier = min(9, int(context_score * 9))
            parameter_tier = min(9, int(parameter_score * 9))
            speed_tier = min(9, int(scored.speed_score * 9))
            quality_tier = min(9, int(scored.quality_score * 9))
            reliability_tier = min(9, int(scored.reliability_score * 9))

            hierarchical_score = (
                cost_tier * 100000 + context_tier * 10000 + parameter_tier * 1000 +
                speed_tier * 100 + quality_tier * 10 + reliability_tier
            )

            score_display = f"{cost_tier}{context_tier}{parameter_tier}{speed_tier}{quality_tier}{reliability_tier}"
            is_free = "FREE" if free_score >= 0.9 else "PAID"
            logger.info(f"   #{i+1}: '{scored.channel.name}' [{is_free}] Score: {score_display} (Total: {hierarchical_score:,})")

        return sorted_channels

    def _estimate_tokens(self, messages: list[dict[str, Any]]) -> int:
        """æ™ºèƒ½ä¼°ç®—prompt tokens"""
        try:
            # å°è¯•ä½¿ç”¨tiktokenåº“
            import tiktoken
            encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4ä½¿ç”¨çš„ç¼–ç 

            total_tokens = 0
            for message in messages:
                content = message.get("content", "")
                if isinstance(content, str):
                    # åŠ ä¸Šè§’è‰²æ ‡è®°çš„å¼€é”€
                    role_overhead = 4  # æ¯æ¡æ¶ˆæ¯çš„roleç­‰ç»“æ„å¼€é”€
                    content_tokens = len(encoding.encode(content))
                    total_tokens += content_tokens + role_overhead

            return max(1, total_tokens)

        except ImportError:
            # tiktokenä¸å¯ç”¨æ—¶çš„æ™ºèƒ½å›é€€
            logger.debug("tiktokenä¸å¯ç”¨ï¼Œä½¿ç”¨æ”¹è¿›çš„å­—ç¬¦ä¼°ç®—æ–¹æ³•")

        # æ”¹è¿›çš„å­—ç¬¦è®¡ç®—æ–¹æ³•
        total_chars = 0
        all_content = ""
        for message in messages:
            content = message.get("content", "")
            if isinstance(content, str):
                # è§’è‰²æ ‡è®°å¼€é”€
                total_chars += len(f"role: {message.get('role', 'user')}")
                # å†…å®¹å­—ç¬¦æ•°
                total_chars += len(content)
                all_content += content

        # æ”¹è¿›çš„å­—ç¬¦åˆ°tokenè½¬æ¢æ¯”ç‡ï¼ˆæ›´å‡†ç¡®ï¼‰
        # ä¸­æ–‡å­—ç¬¦é€šå¸¸1ä¸ªå­—ç¬¦=1ä¸ªtokenï¼Œè‹±æ–‡çº¦4ä¸ªå­—ç¬¦=1ä¸ªtoken
        chinese_chars = sum(1 for char in all_content if '\u4e00' <= char <= '\u9fff')
        english_chars = len(all_content) - chinese_chars
        estimated_tokens = chinese_chars + (english_chars // 4) + (len(messages) * 4)  # æ·»åŠ æ¶ˆæ¯ç»“æ„å¼€é”€

        return max(1, estimated_tokens)

    def _estimate_cost_for_channel(self, channel: Channel, request: RoutingRequest) -> float:
        """ä¼°ç®—ç‰¹å®šæ¸ é“çš„è¯·æ±‚æˆæœ¬"""
        try:
            # ä¼°ç®—tokenæ•°é‡
            input_tokens = self._estimate_tokens(request.messages)
            estimated_output_tokens = max(50, input_tokens // 4)  # é¢„ä¼°è¾“å‡ºtoken

            # è·å–æ¸ é“çš„å®šä»·ä¿¡æ¯
            model_cache = self.config_loader.get_model_cache()
            if channel.id in model_cache:
                discovered_info = model_cache[channel.id]
                models_pricing = discovered_info.get("models_pricing", {})

                # æŸ¥æ‰¾æ¨¡å‹çš„å®šä»·ä¿¡æ¯
                model_pricing = None
                for model_name, pricing in models_pricing.items():
                    if model_name == request.model or request.model in model_name:
                        model_pricing = pricing
                        break

                if model_pricing:
                    input_cost = model_pricing.get("input_cost_per_token", 0.0)
                    output_cost = model_pricing.get("output_cost_per_token", 0.0)

                    total_cost = (input_tokens * input_cost +
                                estimated_output_tokens * output_cost)

                    return total_cost

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®šä»·ä¿¡æ¯ï¼Œä½¿ç”¨å…è´¹è¯„åˆ†æ¥ä¼°ç®—
            free_score = self._calculate_free_score(channel, request.model)
            if free_score >= 0.9:
                return 0.0  # å…è´¹æ¨¡å‹
            else:
                return 0.001  # é»˜è®¤ä¼°ç®—ä¸ºå¾ˆä½çš„æˆæœ¬

        except Exception as e:
            logger.warning(f"æˆæœ¬ä¼°ç®—å¤±è´¥: {e}")
            return 0.001

    def _is_suitable_for_chat(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦é€‚åˆchatå¯¹è¯ä»»åŠ¡"""
        if not model_name:
            return False

        model_lower = model_name.lower()

        # è¿‡æ»¤embeddingæ¨¡å‹
        embedding_keywords = ['embedding', 'embed', 'text-embedding']
        if any(keyword in model_lower for keyword in embedding_keywords):
            return False

        # è¿‡æ»¤çº¯visionæ¨¡å‹
        vision_only_keywords = ['vision-only', 'image-only']
        if any(keyword in model_lower for keyword in vision_only_keywords):
            return False

        # è¿‡æ»¤å·¥å…·ç±»æ¨¡å‹
        tool_keywords = ['tokenizer', 'classifier', 'detector']
        if any(keyword in model_lower for keyword in tool_keywords):
            return False

        return True

    def _extract_tags_from_model_name(self, model_name: str) -> list[str]:
        """ä»æ¨¡å‹åç§°ä¸­æå–æ ‡ç­¾ï¼ˆå¸¦ç¼“å­˜ï¼‰- æ”¯æŒå¤šé¢—ç²’åº¦åŒ¹é…

        ä¾‹å¦‚: "qwen/qwen3-235b-a22b:free" ->
        - æ‹†åˆ†æ ‡ç­¾: ["qwen", "qwen3", "235b", "a22b", "free"]
        - å®Œæ•´å­æ®µ: ["qwen3-235b-a22b"] (å»æ‰å‰ç¼€å’Œåç¼€çš„æ ¸å¿ƒåç§°)
        - åˆå¹¶ç»“æœ: ["qwen", "qwen3", "235b", "a22b", "free", "qwen3-235b-a22b"]
        """
        if not model_name or not isinstance(model_name, str):
            return []

        # æ£€æŸ¥ç¼“å­˜
        if model_name in self._tag_cache:
            return self._tag_cache[model_name]

        import re

        # 1. æå–å®Œæ•´å­æ®µï¼ˆæ ¸å¿ƒæ¨¡å‹åç§°ï¼‰
        complete_segments = self._extract_complete_segments(model_name)

        # 2. ä½¿ç”¨å¤šç§åˆ†éš”ç¬¦è¿›è¡Œæ‹†åˆ†: :, /, @, -, _, ,
        separators = r'[/:@\-_,]'
        parts = re.split(separators, model_name.lower())

        # 3. æ¸…ç†å’Œè¿‡æ»¤æ‹†åˆ†æ ‡ç­¾
        split_tags = []
        for part in parts:
            part = part.strip()
            if part and len(part) > 1:  # å¿½ç•¥å•å­—ç¬¦å’Œç©ºæ ‡ç­¾
                split_tags.append(part)

        # 4. åˆå¹¶æ‹†åˆ†æ ‡ç­¾å’Œå®Œæ•´å­æ®µæ ‡ç­¾ï¼Œå»é‡
        all_tags = list(dict.fromkeys(split_tags + complete_segments))  # ä¿æŒé¡ºåºå»é‡

        # ç¼“å­˜ç»“æœ
        self._tag_cache[model_name] = all_tags
        return all_tags

    def _extract_complete_segments(self, model_name: str) -> list[str]:
        """ä»æ¨¡å‹åç§°ä¸­æå–å®Œæ•´å­æ®µï¼ˆæ ¸å¿ƒæ¨¡å‹åç§°ï¼‰

        ä¾‹å¦‚:
        - "qwen/qwen3-235b-a22b:free" -> ["qwen3-235b-a22b"]
        - "deepseek/deepseek-v3-1:pro" -> ["deepseek-v3-1"]
        - "anthropic/claude-3-haiku@free" -> ["claude-3-haiku"]
        - "anthropic/claude-3-haiku-20240307:free" -> ["claude-3-haiku-20240307", "claude-3-haiku"]
        """
        if not model_name or not isinstance(model_name, str):
            return []

        import re

        # ä½¿ç”¨ä¸»è¦åˆ†éš”ç¬¦ï¼ˆ/, :, @ï¼‰åˆ†å‰²ï¼Œä¿ç•™ä¸­é—´éƒ¨åˆ†çš„å®Œæ•´æ€§
        main_separators = r'[/:@]'
        segments = re.split(main_separators, model_name)

        complete_segments = []
        for segment in segments:
            segment = segment.strip()
            if not segment:
                continue

            # è·³è¿‡æ˜æ˜¾çš„å‰ç¼€ï¼ˆæä¾›å•†åç§°ï¼‰å’Œåç¼€ï¼ˆä»·æ ¼æ ‡ç­¾ç­‰ï¼‰
            segment_lower = segment.lower()

            # è·³è¿‡å¸¸è§çš„æä¾›å•†å‰ç¼€
            provider_prefixes = [
                'openai', 'anthropic', 'qwen', 'deepseek', 'google', 'meta',
                'mistral', 'cohere', 'groq', 'together', 'fireworks',
                'siliconflow', 'moonshot', 'ollama', 'lmstudio'
            ]
            if segment_lower in provider_prefixes:
                continue

            # è·³è¿‡å¸¸è§çš„åç¼€æ ‡ç­¾
            suffix_tags = [
                'free', 'pro', 'premium', 'paid', 'api', 'chat', 'instruct',
                'base', 'tuned', 'finetune', 'ft', 'sft', 'rlhf', 'dpo'
            ]
            if segment_lower in suffix_tags:
                continue

            # è·³è¿‡å•å­—ç¬¦æ®µ
            if len(segment) <= 1:
                continue

            # ä¿ç•™å¯èƒ½æ˜¯æ¨¡å‹æ ¸å¿ƒåç§°çš„æ®µï¼ˆé•¿åº¦>=3ï¼ŒåŒ…å«å­—æ¯æ•°å­—ç»„åˆï¼‰
            if len(segment) >= 3 and re.search(r'[a-zA-Z]', segment) and re.search(r'[\d\-]', segment):
                # é¦–å…ˆæ·»åŠ å®Œæ•´çš„æ®µå
                complete_segments.append(segment.lower())
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ—¥æœŸåç¼€ï¼ˆæ ¼å¼: -YYYYMMDD æˆ– -YYYYMMDD å˜ç§ï¼‰
                # æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼: -20240307, -202403, -2024-03-07 ç­‰
                date_pattern = r'-(\d{8}|\d{6}|\d{4}-\d{2}-\d{2}|\d{4}\d{2}\d{2})$'
                match = re.search(date_pattern, segment_lower)
                
                if match:
                    # å¦‚æœæ‰¾åˆ°æ—¥æœŸåç¼€ï¼Œç”Ÿæˆå»æ‰æ—¥æœŸçš„ç‰ˆæœ¬
                    segment_without_date = segment_lower[:match.start()]
                    if len(segment_without_date) >= 3 and segment_without_date not in complete_segments:
                        complete_segments.append(segment_without_date)
                        logger.debug(f"DATE EXTRACTION: '{segment}' -> added both '{segment.lower()}' and '{segment_without_date}'")

        return complete_segments

    def _resolve_model_aliases(self, model_name: str, channel) -> str:
        """è§£ææ¨¡å‹åˆ«åæ˜ å°„
        
        å°†æ ‡å‡†æ¨¡å‹åç§°è½¬æ¢ä¸ºæ¸ é“ç‰¹å®šçš„æ¨¡å‹åç§°
        
        Args:
            model_name: æ ‡å‡†æ¨¡å‹åç§°ï¼ˆå¦‚ "deepseek-v3.1", "doubao-1.5-pro-256k"ï¼‰
            channel: æ¸ é“é…ç½®å¯¹è±¡
            
        Returns:
            str: æ¸ é“ç‰¹å®šçš„æ¨¡å‹åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ«ååˆ™è¿”å›åŸåç§°
        """
        if not hasattr(channel, 'model_aliases') or not channel.model_aliases:
            return model_name
            
        # ç›´æ¥åŒ¹é…
        if model_name in channel.model_aliases:
            resolved_name = channel.model_aliases[model_name]
            logger.debug(f"ALIAS RESOLVED: '{model_name}' -> '{resolved_name}' for channel {channel.id}")
            return resolved_name
        
        # å°è¯•åŒ¹é…ä¸åŒºåˆ†å¤§å°å†™
        model_lower = model_name.lower()
        for alias_key, alias_value in channel.model_aliases.items():
            if alias_key.lower() == model_lower:
                logger.debug(f"ALIAS RESOLVED (case-insensitive): '{model_name}' -> '{alias_value}' for channel {channel.id}")
                return alias_value
        
        # å°è¯•å‰ç¼€åŒ¹é…ï¼ˆç”¨äºå¸¦providerå‰ç¼€çš„æƒ…å†µï¼‰
        # ä¾‹å¦‚ "doubao/deepseek-v3" åŒ¹é… "deepseek-v3"
        if '/' in model_name:
            _, base_name = model_name.rsplit('/', 1)
            if base_name in channel.model_aliases:
                resolved_name = channel.model_aliases[base_name]
                logger.debug(f"ALIAS RESOLVED (prefix): '{model_name}' -> '{resolved_name}' for channel {channel.id}")
                return resolved_name
        
        # æ²¡æœ‰æ‰¾åˆ°åˆ«åï¼Œè¿”å›åŸåç§°
        return model_name

    def _extract_tags_with_aliases(self, model_name: str, channel) -> list[str]:
        """æå–æ¨¡å‹æ ‡ç­¾ï¼ŒåŒ…æ‹¬æ¥è‡ªæ¸ é“åˆ«åé…ç½®çš„æ ‡ç­¾
        
        Args:
            model_name: æ¨¡å‹åç§°
            channel: æ¸ é“é…ç½®å¯¹è±¡
            
        Returns:
            list[str]: åŒ…å«åŸå§‹æ ‡ç­¾å’Œåˆ«åæ ‡ç­¾çš„å®Œæ•´æ ‡ç­¾åˆ—è¡¨
        """
        # è·å–åŸºç¡€æ ‡ç­¾
        base_tags = self._extract_tags_from_model_name(model_name)
        
        if not hasattr(channel, 'model_aliases') or not channel.model_aliases:
            return base_tags
        
        # æ”¶é›†åˆ«åæ ‡ç­¾ï¼šéå†åˆ«åæ˜ å°„ï¼Œå¦‚æœå½“å‰æ¨¡å‹åç§°æ˜¯æ˜ å°„çš„ç›®æ ‡å€¼ï¼Œåˆ™å°†æ ‡å‡†åç§°ä½œä¸ºæ ‡ç­¾æ·»åŠ 
        alias_tags = []
        
        for standard_name, channel_specific_name in channel.model_aliases.items():
            # å¦‚æœå½“å‰æ¨¡å‹åç§°åŒ¹é…æ¸ é“ç‰¹å®šåç§°ï¼Œå°†æ ‡å‡†åç§°ä½œä¸ºæ ‡ç­¾æ·»åŠ 
            if model_name.lower() == channel_specific_name.lower():
                # ä»æ ‡å‡†åç§°ä¸­æå–æ ‡ç­¾
                standard_tags = self._extract_tags_from_model_name(standard_name)
                alias_tags.extend(standard_tags)
                logger.debug(f"ALIAS TAGS: '{model_name}' matched '{channel_specific_name}', adding tags from '{standard_name}': {standard_tags}")
                
            # ä¹Ÿæ”¯æŒåå‘åŒ¹é…ï¼šå¦‚æœæ¨¡å‹åç§°åŒ…å«æ ‡å‡†åç§°çš„æ ‡ç­¾ï¼Œä¹Ÿæ·»åŠ æ ‡å‡†åç§°ä½œä¸ºæ ‡ç­¾
            elif any(tag in model_name.lower() for tag in self._extract_tags_from_model_name(standard_name)):
                standard_tags = self._extract_tags_from_model_name(standard_name) 
                alias_tags.extend(standard_tags)
                logger.debug(f"ALIAS TAGS (reverse): '{model_name}' contains tags from '{standard_name}', adding: {standard_tags}")
        
        # åˆå¹¶æ‰€æœ‰æ ‡ç­¾å¹¶å»é‡
        all_tags = list(dict.fromkeys(base_tags + alias_tags))
        
        if alias_tags:
            logger.debug(f"ALIAS ENRICHED TAGS: '{model_name}' -> base: {base_tags}, aliases: {alias_tags}, total: {all_tags}")
        
        return all_tags

    def _get_candidate_channels_by_auto_tags(self, positive_tags: list[str], negative_tags: list[str] = None) -> list[ChannelCandidate]:
        """æ ¹æ®æ­£è´Ÿæ ‡ç­¾è·å–å€™é€‰æ¸ é“ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰

        Args:
            positive_tags: å¿…é¡»åŒ…å«çš„æ ‡ç­¾åˆ—è¡¨
            negative_tags: å¿…é¡»æ’é™¤çš„æ ‡ç­¾åˆ—è¡¨
        """
        if negative_tags is None:
            negative_tags = []

        # å¦‚æœæ²¡æœ‰ä»»ä½•æ ‡ç­¾æ¡ä»¶ï¼Œè¿”å›ç©ºï¼ˆé¿å…è¿”å›æ‰€æœ‰æ¨¡å‹ï¼‰
        if not positive_tags and not negative_tags:
            return []

        # æ ‡å‡†åŒ–æ ‡ç­¾
        normalized_positive = [tag.lower().strip() for tag in positive_tags if tag and isinstance(tag, str)]
        normalized_negative = [tag.lower().strip() for tag in negative_tags if tag and isinstance(tag, str)]

        logger.info(f"TAG MATCHING: Searching for channels with positive tags: {normalized_positive}, excluding: {normalized_negative}")

        model_cache = self.config_loader.get_model_cache()
        if not model_cache:
            logger.warning("TAG MATCHING: Model cache is empty, cannot perform tag routing")
            return []

        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæ£€æŸ¥ç¼“å­˜æ˜¯å¦åŒ…å«API Keyçº§åˆ«çš„æ ¼å¼
        # API Keyçº§åˆ«ç¼“å­˜æ ¼å¼ç¤ºä¾‹: {"channel_id_apikeyhash": {...}}
        # æ—§æ ¼å¼: {"channel_id": {...}}
        has_api_key_format = any('_' in key and len(key.split('_')[-1]) == 8 for key in model_cache.keys())
        if has_api_key_format:
            logger.info(f"TAG MATCHING: Using API key-level cache format with {len(model_cache)} entries")
            # API Keyçº§åˆ«ç¼“å­˜æ ¼å¼éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä½†å¯ä»¥ç»§ç»­è·¯ç”±
        else:
            logger.info(f"TAG MATCHING: Using legacy cache format with {len(model_cache)} entries")

        logger.info(f"TAG MATCHING: Searching through {len(model_cache)} cached channels")

        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨å†…å­˜ç´¢å¼•è¿›è¡Œè¶…é«˜é€Ÿæ ‡ç­¾æŸ¥è¯¢
        try:
            from core.utils.memory_index import (
                get_memory_index,
                rebuild_index_if_needed,
            )

            # ğŸš€ æ™ºèƒ½æ£€æŸ¥ï¼šä»…åœ¨å¿…è¦æ—¶é‡å»ºå†…å­˜ç´¢å¼•
            memory_index = get_memory_index()
            current_stats = memory_index.get_stats()
            cache_size = len(model_cache)

            logger.debug(f"ğŸ” INDEX CHECK: Current index has {current_stats.total_models} models, {current_stats.total_channels} channels")
            logger.debug(f"ğŸ” INDEX CHECK: Cache has {cache_size} entries")

            needs_rebuild = current_stats.total_models == 0 or memory_index.needs_rebuild(model_cache)
            if needs_rebuild:
                logger.info(f"ğŸ”¨ REBUILDING MEMORY INDEX: Cache structure changed or index empty (cache: {cache_size}, indexed: {current_stats.total_channels})")
                stats = rebuild_index_if_needed(model_cache, force_rebuild=True)
                logger.info(f"INDEX REBUILT: {stats.total_models} models, {stats.total_tags} tags in {stats.build_time_ms:.1f}ms")
            else:
                logger.debug("MEMORY INDEX: Using existing index (no rebuild needed)")

            # ä½¿ç”¨å†…å­˜ç´¢å¼•è¿›è¡Œè¶…é«˜é€ŸæŸ¥è¯¢
            start_time = time.time()
            matching_models = memory_index.find_models_by_tags(normalized_positive, normalized_negative)
            index_time_ms = (time.time() - start_time) * 1000

            logger.info(f"MEMORY INDEX QUERY: Found {len(matching_models)} models in {index_time_ms:.2f}ms")

            # å°†ç»“æœè½¬æ¢ä¸º ChannelCandidate æ ¼å¼
            memory_candidates = []
            for channel_id, model_name in matching_models:
                # æŸ¥æ‰¾å¯¹åº”çš„æ¸ é“å¯¹è±¡
                channel = self.config_loader.get_channel_by_id(channel_id)
                if channel and channel.enabled:
                    memory_candidates.append(ChannelCandidate(
                        channel=channel, matched_model=model_name
                    ))
                    logger.debug(f"FOUND: {channel.name} -> {model_name}")

            logger.info(f"ğŸ¯ MEMORY INDEX RESULT: {len(memory_candidates)} enabled channels found")
            return memory_candidates

        except Exception as e:
            logger.warning(f"âš ï¸ MEMORY INDEX FAILED: {e}, falling back to legacy file-based search")
            # ç»§ç»­ä½¿ç”¨åŸæœ‰çš„æ–‡ä»¶æœç´¢é€»è¾‘ä½œä¸ºåå¤‡

        # ä¸¥æ ¼åŒ¹é…ï¼šæ”¯æŒæ­£è´Ÿæ ‡ç­¾çš„ä¸¥æ ¼åŒ¹é…
        if not normalized_negative:
            # å¦‚æœæ²¡æœ‰è´Ÿæ ‡ç­¾ï¼Œä½¿ç”¨åŸæœ‰çš„æ–¹æ³•
            exact_candidates = self._find_channels_with_all_tags(normalized_positive, model_cache)
        else:
            # æœ‰è´Ÿæ ‡ç­¾ï¼Œä½¿ç”¨æ–°çš„æ­£è´Ÿæ ‡ç­¾åŒ¹é…æ–¹æ³•
            exact_candidates = self._find_channels_with_positive_negative_tags(normalized_positive, normalized_negative, model_cache)

        if exact_candidates:
            logger.info(f"ğŸ¯ STRICT MATCH: Found {len(exact_candidates)} channels matching positive: {normalized_positive}, excluding: {normalized_negative}")
            return exact_candidates

        logger.warning(f"NO MATCH: No channels found matching positive: {normalized_positive}, excluding: {normalized_negative}")
        return []

    def _get_candidate_channels_by_complete_segment(self, complete_segments: list[str]) -> list[ChannelCandidate]:
        """æ ¹æ®å®Œæ•´å­æ®µï¼ˆä¸æ‹†åˆ†ï¼‰è·å–å€™é€‰æ¸ é“

        Args:
            complete_segments: å®Œæ•´å­æ®µæ ‡ç­¾åˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼š["qwen3-30b-a3b"]ï¼‰
        """
        if not complete_segments:
            return []

        # æ ‡å‡†åŒ–è¾“å…¥
        normalized_segments = [segment.lower().strip() for segment in complete_segments if segment and isinstance(segment, str)]

        logger.info(f"COMPLETE SEGMENT MATCHING: Searching for exact segments: {normalized_segments}")

        model_cache = self.config_loader.get_model_cache()
        if not model_cache:
            logger.warning("COMPLETE SEGMENT MATCHING: Model cache is empty")
            return []

        candidates = []

        # éå†æ‰€æœ‰ç¼“å­˜çš„æ¸ é“
        for channel_id, discovery_data in model_cache.items():
            if not isinstance(discovery_data, dict):
                continue

            # å¤„ç†API Keyçº§åˆ«çš„ç¼“å­˜æ ¼å¼
            real_channel_id = channel_id
            if '_' in channel_id:
                # å°è¯•å»æ‰ API key hash åç¼€
                parts = channel_id.split('_')
                if len(parts) >= 2:
                    # æ£€æŸ¥æœ€åä¸€éƒ¨åˆ†æ˜¯å¦ä¸º hash æ ¼å¼ï¼ˆé•¿åº¦ä¸º8çš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
                    potential_hash = parts[-1]
                    if len(potential_hash) == 8 and all(c in '0123456789abcdef' for c in potential_hash.lower()):
                        real_channel_id = '_'.join(parts[:-1])

            # æŸ¥æ‰¾æ¸ é“å¯¹è±¡
            channel = self.config_loader.get_channel_by_id(real_channel_id)
            if not channel or not channel.enabled:
                continue

            # æ£€æŸ¥æ¸ é“ä¸­çš„æ¨¡å‹
            models = discovery_data.get("models", [])
            models_data = discovery_data.get("models_data", {})

            for model_name in models:
                # ä¸ºæ¯ä¸ªæ¨¡å‹æå–å®Œæ•´å­æ®µæ ‡ç­¾
                model_tags = self._extract_tags_from_model_name(model_name)

                # æ£€æŸ¥æ˜¯å¦åŒ…å«æˆ‘ä»¬è¦æŸ¥æ‰¾çš„å®Œæ•´å­æ®µ
                for segment in normalized_segments:
                    if segment in [tag.lower() for tag in model_tags]:
                        logger.debug(f"COMPLETE SEGMENT MATCH: Found '{segment}' in model '{model_name}' from channel '{channel.name}'")

                        # è·å–çœŸå®çš„æ¨¡å‹ID
                        real_model_id = model_name
                        if models_data and model_name in models_data:
                            model_info = models_data[model_name]
                            real_model_id = model_info.get("id", model_name)

                        candidates.append(ChannelCandidate(
                            channel=channel,
                            matched_model=real_model_id
                        ))
                        break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±å¤Ÿäº†ï¼Œé¿å…é‡å¤æ·»åŠ åŒä¸€ä¸ªæ¨¡å‹

        logger.info(f"COMPLETE SEGMENT MATCHING: Found {len(candidates)} candidates for segments {normalized_segments}")
        return candidates

    def _find_channels_with_all_tags(self, tags: list[str], model_cache: dict) -> list[ChannelCandidate]:
        """æŸ¥æ‰¾åŒ…å«æ‰€æœ‰æŒ‡å®šæ ‡ç­¾çš„æ¸ é“å’Œæ¨¡å‹ç»„åˆ

        æ ‡ç­¾åŒ¹é…è§„åˆ™ï¼š
        1. é¦–å…ˆæ£€æŸ¥æ¸ é“çº§åˆ«çš„æ ‡ç­¾ (channel.tags)
        2. å¦‚æœæ¸ é“æ ‡ç­¾åŒ¹é…ï¼Œè¯¥æ¸ é“ä¸‹æ‰€æœ‰æ¨¡å‹éƒ½è¢«è§†ä¸ºåŒ¹é…
        3. å¦åˆ™æ£€æŸ¥ä»æ¨¡å‹åç§°æå–çš„æ ‡ç­¾
        4. å¯¹äº 'free' æ ‡ç­¾ï¼Œéœ€è¦ä¸¥æ ¼éªŒè¯æ¸ é“æ˜¯å¦çœŸçš„å…è´¹
        """
        candidate_channels = []
        matched_models = []

        # æ£€æŸ¥æ˜¯å¦åŒ…å«freeæ ‡ç­¾ - éœ€è¦ä¸¥æ ¼éªŒè¯
        has_free_tag = any(tag.lower() in {"free", "å…è´¹", "0cost", "nocost"} for tag in tags)

        # éå†æ‰€æœ‰æœ‰æ•ˆæ¸ é“
        for channel in self.config_loader.get_enabled_channels():
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨API Keyçº§åˆ«ç¼“å­˜æŸ¥æ‰¾æ–¹æ³•
            discovered_info = self.config_loader.get_model_cache_by_channel(channel.id)
            if not isinstance(discovered_info, dict):
                continue

            models = discovered_info.get("models", [])
            if not models:
                continue

            logger.debug(f"TAG MATCHING: Checking channel {channel.id} ({channel.name}) with {len(models)} models")

            # ç»Ÿä¸€çš„æ ‡ç­¾åˆå¹¶åŒ¹é…ï¼šæ¸ é“æ ‡ç­¾ + æ¨¡å‹æ ‡ç­¾
            channel_tags = getattr(channel, 'tags', []) or []
            channel_matches = 0

            for model_name in models:
                if not model_name:
                    continue

                # ä»æ¨¡å‹åç§°æå–æ ‡ç­¾ï¼ˆåŒ…æ‹¬åˆ«åæ ‡ç­¾ï¼‰
                model_tags = self._extract_tags_with_aliases(model_name, channel)

                # åˆå¹¶æ¸ é“æ ‡ç­¾å’Œæ¨¡å‹æ ‡ç­¾ï¼Œå¹¶è§„èŒƒåŒ–ä¸ºå°å†™
                combined_tags = list(set([tag.lower() for tag in channel_tags] + [tag.lower() for tag in model_tags]))

                # éªŒè¯æ‰€æœ‰æŸ¥è¯¢æ ‡ç­¾éƒ½åœ¨åˆå¹¶åçš„æ ‡ç­¾ä¸­ï¼ˆcase-insensitiveåŒ¹é…ï¼‰
                normalized_query_tags = [tag.lower() for tag in tags]
                if all(tag in combined_tags for tag in normalized_query_tags):
                    # ğŸ”¥ ä¸¥æ ¼éªŒè¯ free æ ‡ç­¾ - ç¡®ä¿æ¸ é“çœŸçš„å…è´¹
                    if has_free_tag:
                        free_score = self._calculate_free_score(channel, model_name)
                        # åªæœ‰çœŸæ­£å…è´¹çš„æ¸ é“æ‰ä¼šè¢«åŒ¹é… (free_score >= 0.9)
                        if free_score < 0.9:
                            logger.debug(f"FREE TAG VALIDATION FAILED: Channel '{channel.name}' model '{model_name}' has free_score={free_score:.2f} < 0.9, not truly free")
                            continue
                        else:
                            logger.debug(f"FREE TAG VALIDATED: Channel '{channel.name}' model '{model_name}' confirmed as truly free (score={free_score:.2f})")

                    # è¿‡æ»¤æ‰ä¸é€‚åˆchatçš„æ¨¡å‹ç±»å‹
                    if self._is_suitable_for_chat(model_name):
                        candidate_channels.append(ChannelCandidate(
                            channel=channel,
                            matched_model=model_name
                        ))
                        matched_models.append(model_name)
                        channel_matches += 1
                        logger.debug(f"MERGED TAG MATCH: Channel '{channel.name}' model '{model_name}' -> tags: {combined_tags}")
                    else:
                        logger.debug(f"âš ï¸ FILTERED: Model '{model_name}' not suitable for chat (appears to be embedding/vision model)")

            if channel_matches > 0:
                logger.info(f"ğŸ¯ CHANNEL SUMMARY: Found {channel_matches} matching models in channel '{channel.name}' via merged channel+model tags")

        if matched_models:
            logger.info(f"ğŸ¯ TOTAL MATCHED MODELS: {len(matched_models)} models found: {matched_models[:5]}{'...' if len(matched_models) > 5 else ''}")

        return candidate_channels

    def _find_channels_with_positive_negative_tags(self, positive_tags: list[str], negative_tags: list[str], model_cache: dict) -> list[ChannelCandidate]:
        """æŸ¥æ‰¾åŒ¹é…æ­£æ ‡ç­¾ä½†æ’é™¤è´Ÿæ ‡ç­¾çš„æ¸ é“å’Œæ¨¡å‹ç»„åˆ

        Args:
            positive_tags: å¿…é¡»åŒ…å«çš„æ ‡ç­¾åˆ—è¡¨
            negative_tags: å¿…é¡»æ’é™¤çš„æ ‡ç­¾åˆ—è¡¨
            model_cache: æ¨¡å‹ç¼“å­˜

        Returns:
            ç¬¦åˆæ¡ä»¶çš„å€™é€‰æ¸ é“åˆ—è¡¨
        """
        candidate_channels = []
        matched_models = []

        # éå†æ‰€æœ‰æœ‰æ•ˆæ¸ é“
        for channel in self.config_loader.get_enabled_channels():
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨API Keyçº§åˆ«ç¼“å­˜æŸ¥æ‰¾æ–¹æ³•
            discovered_info = self.config_loader.get_model_cache_by_channel(channel.id)
            if not isinstance(discovered_info, dict):
                continue

            models = discovered_info.get("models", [])
            if not models:
                continue

            logger.debug(f"POSITIVE/NEGATIVE TAG MATCHING: Checking channel {channel.id} ({channel.name}) with {len(models)} models")

            # ç»Ÿä¸€çš„æ ‡ç­¾åˆå¹¶åŒ¹é…ï¼šæ¸ é“æ ‡ç­¾ + æ¨¡å‹æ ‡ç­¾
            channel_tags = getattr(channel, 'tags', []) or []
            channel_matches = 0

            for model_name in models:
                if not model_name:
                    continue

                # ä»æ¨¡å‹åç§°æå–æ ‡ç­¾ï¼ˆåŒ…æ‹¬åˆ«åæ ‡ç­¾ï¼‰
                model_tags = self._extract_tags_with_aliases(model_name, channel)

                # åˆå¹¶æ¸ é“æ ‡ç­¾å’Œæ¨¡å‹æ ‡ç­¾ï¼Œå¹¶è§„èŒƒåŒ–ä¸ºå°å†™
                combined_tags = list(set([tag.lower() for tag in channel_tags] + [tag.lower() for tag in model_tags]))

                # æ£€æŸ¥æ­£æ ‡ç­¾ï¼šæ‰€æœ‰æ­£æ ‡ç­¾éƒ½å¿…é¡»åœ¨åˆå¹¶åçš„æ ‡ç­¾ä¸­ï¼ˆcase-insensitiveï¼‰
                positive_match = True
                if positive_tags:
                    normalized_positive = [tag.lower() for tag in positive_tags]
                    positive_match = all(tag in combined_tags for tag in normalized_positive)

                # æ£€æŸ¥è´Ÿæ ‡ç­¾ï¼šä»»ä½•è´Ÿæ ‡ç­¾éƒ½ä¸èƒ½åœ¨åˆå¹¶åçš„æ ‡ç­¾ä¸­ï¼ˆcase-insensitiveï¼‰
                negative_match = True
                if negative_tags:
                    normalized_negative = [tag.lower() for tag in negative_tags]
                    negative_match = not any(tag in combined_tags for tag in normalized_negative)

                # åªæœ‰åŒæ—¶æ»¡è¶³æ­£æ ‡ç­¾å’Œè´Ÿæ ‡ç­¾æ¡ä»¶çš„æ¨¡å‹æ‰è¢«é€‰ä¸­
                if positive_match and negative_match:
                    # è¿‡æ»¤æ‰ä¸é€‚åˆchatçš„æ¨¡å‹ç±»å‹
                    if self._is_suitable_for_chat(model_name):
                        candidate_channels.append(ChannelCandidate(
                            channel=channel,
                            matched_model=model_name
                        ))
                        matched_models.append(model_name)
                        channel_matches += 1
                        logger.debug(f"POSITIVE/NEGATIVE TAG MATCH: Channel '{channel.name}' model '{model_name}' -> tags: {combined_tags}")
                    else:
                        logger.debug(f"âš ï¸ FILTERED: Model '{model_name}' not suitable for chat (appears to be embedding/vision model)")
                else:
                    if not positive_match:
                        logger.debug(f"POSITIVE MISMATCH: Model '{model_name}' missing required tags from {positive_tags}")
                    if not negative_match:
                        logger.debug(f"NEGATIVE MISMATCH: Model '{model_name}' contains excluded tags from {negative_tags}")

            if channel_matches > 0:
                logger.info(f"ğŸ¯ CHANNEL SUMMARY: Found {channel_matches} matching models in channel '{channel.name}' via positive/negative tag filtering")

        if matched_models:
            logger.info(f"ğŸ¯ TOTAL MATCHED MODELS: {len(matched_models)} models found: {matched_models[:5]}{'...' if len(matched_models) > 5 else ''}")

        return candidate_channels

    def get_available_models(self) -> list[str]:
        """è·å–ç”¨æˆ·å¯ç›´æ¥è¯·æ±‚çš„ã€åœ¨é…ç½®ä¸­å®šä¹‰çš„æ¨¡å‹åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        if self._available_models_cache is not None:
            return self._available_models_cache

        models = set()
        all_tags = set()

        # æ·»åŠ ç‰©ç†æ¨¡å‹åç§°
        for ch in self.config.channels:
            if ch.enabled and ch.model_name:
                models.add(ch.model_name)

                # ä»é…ç½®çš„tagsä¸­æ·»åŠ 
                for tag in ch.tags:
                    if tag:
                        all_tags.add(f"tag:{tag}")

        # ä»æ¨¡å‹ç¼“å­˜ä¸­æå–è‡ªåŠ¨æ ‡ç­¾
        model_cache = self.config_loader.get_model_cache()
        if model_cache:
            for _channel_id, cache_info in model_cache.items():
                if not isinstance(cache_info, dict):
                    continue

                models_list = cache_info.get("models", [])
                if not isinstance(models_list, list):
                    continue

                for model_name in models_list:
                    if model_name:
                        models.add(model_name)
                        # æå–è‡ªåŠ¨æ ‡ç­¾
                        auto_tags = self._extract_tags_from_model_name(model_name)
                        for tag in auto_tags:
                            if tag:
                                all_tags.add(f"tag:{tag}")

        # ç¼“å­˜ç»“æœ
        result = sorted(models | all_tags)
        self._available_models_cache = result
        return result

    def get_all_available_tags(self) -> list[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ ‡ç­¾ï¼ˆä¸å¸¦tag:å‰ç¼€ï¼‰"""
        models = self.get_available_models()
        tags = [model[4:] for model in models if model.startswith("tag:")]
        return sorted(tags)

    async def _filter_by_capabilities(self, channels: list[ChannelCandidate], request: RoutingRequest) -> list[ChannelCandidate]:
        """åŸºäºæ¨¡å‹èƒ½åŠ›è¿‡æ»¤æ¸ é“"""
        if not hasattr(request, 'data') or not request.data:
            # æ²¡æœ‰è¯·æ±‚æ•°æ®ï¼Œè·³è¿‡èƒ½åŠ›æ£€æµ‹
            return channels

        # åˆ†æè¯·æ±‚éœ€è¦çš„èƒ½åŠ›
        capability_requirements = self.capability_mapper.get_capability_requirements(request.data)

        # å¦‚æœè¯·æ±‚ä¸éœ€è¦ç‰¹æ®Šèƒ½åŠ›ï¼Œè·³è¿‡æ£€æµ‹
        if not any(capability_requirements.values()):
            logger.debug("Request doesn't require special capabilities, skipping capability check")
            return channels

        logger.info(f"CAPABILITY REQUIREMENTS: {capability_requirements}")

        capability_filtered = []
        fallback_channels = []

        for candidate in channels:
            channel = candidate.channel
            model_name = candidate.matched_model or channel.model_name

            try:
                # æ£€æµ‹æ¨¡å‹èƒ½åŠ›
                capabilities = await self.capability_detector.detect_model_capabilities(
                    model_name=model_name,
                    provider=channel.provider,
                    base_url=channel.base_url or "",
                    api_key=channel.api_key
                )

                # æ£€æŸ¥æ˜¯å¦èƒ½å¤„ç†å½“å‰è¯·æ±‚
                can_handle = self.capability_detector.can_handle_request(capabilities, request.data)

                if can_handle:
                    capability_filtered.append(candidate)
                    logger.debug(f"CAPABILITY MATCH: {channel.name} can handle request")
                else:
                    # å¦‚æœæ˜¯æœ¬åœ°æ¨¡å‹ä¸æ”¯æŒï¼Œè®°å½•ä¸ºå¤‡ç”¨é€‰é¡¹
                    if capabilities.is_local:
                        fallback_channels.append((candidate, capabilities))
                        logger.debug(f"âš ï¸ LOCAL LIMITATION: {channel.name} lacks required capabilities, marked for fallback")
                    else:
                        logger.debug(f"CAPABILITY MISMATCH: {channel.name} cannot handle request")

            except Exception as e:
                logger.warning(f"Error checking capabilities for {channel.name}: {e}")
                # æ£€æµ‹å¤±è´¥æ—¶ï¼Œä¿ç•™æ¸ é“ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
                capability_filtered.append(candidate)

        # å¦‚æœæ²¡æœ‰åˆé€‚çš„æ¸ é“ä¸”æœ‰æœ¬åœ°æ¨¡å‹æ— æ³•å¤„ç†ï¼Œå°è¯•æ·»åŠ äº‘ç«¯å¤‡ç”¨æ¸ é“
        if not capability_filtered and fallback_channels:
            logger.info("FALLBACK SEARCH: Looking for cloud alternatives for local model limitations...")

            # è·å–æ‰€æœ‰å¯ç”¨æ¸ é“è¿›è¡Œå¤‡ç”¨æœç´¢
            all_channels = []
            for provider_config in self.config.providers:
                for channel_config in provider_config.channels:
                    if channel_config.enabled:
                        all_channels.append({
                            "id": channel_config.id,
                            "name": channel_config.name,
                            "provider": provider_config.name,
                            "model_name": channel_config.model_name,
                            "base_url": channel_config.base_url or provider_config.base_url,
                            "api_key": channel_config.api_key,
                            "priority": getattr(channel_config, 'priority', 1)
                        })

            # å¯¹æ¯ä¸ªå¤±è´¥çš„æœ¬åœ°æ¨¡å‹ï¼Œå¯»æ‰¾å¤‡ç”¨æ¸ é“
            for failed_candidate, _failed_capabilities in fallback_channels[:1]:  # åªä¸ºç¬¬ä¸€ä¸ªå¤±è´¥çš„æœ¬åœ°æ¨¡å‹å¯»æ‰¾å¤‡ç”¨
                fallback_candidates = await self.capability_detector.get_fallback_channels(
                    original_channel=failed_candidate.channel.id,
                    request_data=request.data,
                    available_channels=all_channels
                )

                if fallback_candidates:
                    logger.info(f"FOUND FALLBACK: {len(fallback_candidates)} alternative channels for {failed_candidate.channel.name}")

                    # å°†å‰3ä¸ªå¤‡ç”¨æ¸ é“è½¬æ¢ä¸ºChannelCandidate
                    for fallback_channel_config in fallback_candidates[:3]:
                        # åˆ›å»ºChannelå¯¹è±¡
                        fallback_channel = Channel(
                            id=fallback_channel_config["id"],
                            name=fallback_channel_config["name"],
                            provider=fallback_channel_config["provider"],
                            model_name=fallback_channel_config["model_name"],
                            api_key=fallback_channel_config["api_key"],
                            base_url=fallback_channel_config["base_url"],
                            enabled=True,
                            priority=fallback_channel_config.get("priority", 1)
                        )

                        fallback_candidate = ChannelCandidate(
                            channel=fallback_channel,
                            matched_model=fallback_channel_config["model_name"]
                        )

                        capability_filtered.append(fallback_candidate)
                    break  # åªä¸ºç¬¬ä¸€ä¸ªå¤±è´¥çš„æ¸ é“å¯»æ‰¾å¤‡ç”¨

        return capability_filtered

    async def _pre_filter_channels(self, channels: list[ChannelCandidate], request: RoutingRequest, max_channels: int = 20) -> list[ChannelCandidate]:
        """
        æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨å¿«é€Ÿå¯å‘å¼æ–¹æ³•é¢„ç­›é€‰æ¸ é“ï¼Œå‡å°‘è¯¦ç»†è¯„åˆ†çš„å¼€é”€

        Args:
            channels: å€™é€‰æ¸ é“åˆ—è¡¨
            request: è·¯ç”±è¯·æ±‚
            max_channels: æœ€å¤§ä¿ç•™æ¸ é“æ•°é‡

        Returns:
            é¢„ç­›é€‰åçš„æ¸ é“åˆ—è¡¨
        """
        if len(channels) <= max_channels:
            return channels

        logger.info(f"PRE-FILTER: Fast pre-filtering {len(channels)} channels to top {max_channels}")

        # ä½¿ç”¨å¿«é€Ÿå¯å‘å¼è¯„åˆ†
        channel_scores = []
        for candidate in channels:
            channel = candidate.channel

            # å¿«é€Ÿè¯„åˆ†å› å­ï¼ˆé¿å…å¤æ‚è®¡ç®—ï¼‰
            score = 0.0

            # 1. å…è´¹ä¼˜å…ˆï¼ˆæœ€é‡è¦ï¼‰
            if self._is_free_channel(channel, candidate.matched_model):
                score += 1000  # å…è´¹æ¸ é“è·å¾—æœ€é«˜ä¼˜å…ˆçº§

            # 2. å¯é æ€§è¯„åˆ†ï¼ˆåŸºäºç®€å•æŒ‡æ ‡ï¼‰
            if hasattr(channel, 'priority') and channel.priority:
                score += (10 - channel.priority) * 10  # ä¼˜å…ˆçº§è¶Šé«˜åˆ†æ•°è¶Šé«˜

            # 3. æœ¬åœ°æ¨¡å‹ä¼˜å…ˆ
            if self._is_local_channel(channel):
                score += 100

            # 4. å¥åº·çŠ¶æ€
            if getattr(channel, 'enabled', True):
                score += 50

            # 5. éšæœºå› å­ï¼ˆé¿å…æ€»æ˜¯é€‰æ‹©ç›¸åŒæ¸ é“ï¼‰
            score += random.uniform(0, 10)

            channel_scores.append((score, candidate))

        # æŒ‰è¯„åˆ†æ’åºå¹¶é€‰æ‹©top N
        channel_scores.sort(key=lambda x: x[0], reverse=True)
        selected = [candidate for score, candidate in channel_scores[:max_channels]]

        # è®°å½•é¢„ç­›é€‰ç»“æœ
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug("PRE-FILTER RESULTS:")
            for i, (score, candidate) in enumerate(channel_scores[:max_channels]):
                logger.debug(f"  #{i+1}: {candidate.channel.name} (score: {score:.1f})")

        logger.info(f"PRE-FILTER COMPLETE: Selected {len(selected)}/{len(channels)} channels for detailed scoring")
        return selected

    def _is_free_channel(self, channel: Channel, model_name: str) -> bool:
        """å¿«é€Ÿåˆ¤æ–­æ˜¯å¦ä¸ºå…è´¹æ¸ é“"""
        # ç®€å•çš„å…è´¹åˆ¤æ–­è§„åˆ™
        if hasattr(channel, 'cost') and channel.cost == 0:
            return True
        if model_name and 'free' in model_name.lower():
            return True
        if hasattr(channel, 'name') and 'free' in channel.name.lower():
            return True
        return False

    def _is_local_channel(self, channel: Channel) -> bool:
        """å¿«é€Ÿåˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°æ¸ é“"""
        if hasattr(channel, 'base_url') and channel.base_url:
            url = channel.base_url.lower()
            return any(local_indicator in url for local_indicator in [
                'localhost', '127.0.0.1', ':11434', ':1234', 'ollama', 'lmstudio'
            ])
        return False

    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        self._tag_cache.clear()
        self._available_tags_cache = None
        self._available_models_cache = None
        logger.info("Router cache cleared")

    def update_channel_health(self, channel_id: str, success: bool, latency: Optional[float] = None):
        """æ›´æ–°æ¸ é“å¥åº·çŠ¶æ€"""
        self.config_loader.update_channel_health(channel_id, success, latency)

    def _log_performance_metrics(self, channel_count: int, elapsed_ms: float,
                                scoring_type: str, metrics=None):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡å’Œæ…¢æŸ¥è¯¢æ£€æµ‹"""
        avg_time_per_channel = elapsed_ms / max(channel_count, 1)

        # æ…¢æŸ¥è¯¢æ£€æµ‹é˜ˆå€¼
        slow_threshold_ms = 1000  # 1ç§’
        very_slow_threshold_ms = 2000  # 2ç§’

        if elapsed_ms > very_slow_threshold_ms:
            logger.warning(f"ğŸŒ VERY SLOW SCORING: {channel_count} channels took {elapsed_ms:.1f}ms "
                          f"(avg: {avg_time_per_channel:.1f}ms/channel) - Consider optimization")
        elif elapsed_ms > slow_threshold_ms:
            logger.warning(f"âš ï¸ SLOW SCORING: {channel_count} channels took {elapsed_ms:.1f}ms "
                          f"(avg: {avg_time_per_channel:.1f}ms/channel)")
        else:
            logger.info(f"SCORING PERFORMANCE: {channel_count} channels in {elapsed_ms:.1f}ms "
                       f"(avg: {avg_time_per_channel:.1f}ms/channel)")

        # å¦‚æœæ˜¯æ‰¹é‡è¯„åˆ†ä¸”æœ‰æ€§èƒ½æŒ‡æ ‡
        if metrics and scoring_type == "batch":
            if metrics.slow_threshold_exceeded:
                logger.warning(f"BATCH SCORER ANALYSIS: {metrics.optimization_applied}")

            if metrics.cache_hit:
                logger.info("ğŸ’¾ CACHE HIT: Scoring completed with cached results")

        # ä¼˜åŒ–å»ºè®®
        if channel_count > 50 and elapsed_ms > 1500:
            logger.info(f"ğŸ’¡ OPTIMIZATION TIP: Consider implementing channel pre-filtering for {channel_count}+ channels")
        elif channel_count > 20 and elapsed_ms > 800:
            logger.info("ğŸ’¡ OPTIMIZATION TIP: Performance could benefit from caching strategies")

# å…¨å±€è·¯ç”±å™¨å®ä¾‹
_router: Optional[JSONRouter] = None

def get_router() -> JSONRouter:
    """è·å–å…¨å±€è·¯ç”±å™¨å®ä¾‹"""
    global _router
    if _router is None:
        _router = JSONRouter()
    return _router
