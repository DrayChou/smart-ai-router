"""
åŸºäºJSONé…ç½®çš„è½»é‡è·¯ç”±å¼•æ“
"""
import random
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
import time
import logging

from .yaml_config import YAMLConfigLoader, get_yaml_config_loader
from .config_models import Channel
from .utils.model_analyzer import get_model_analyzer
from .utils.channel_cache_manager import get_channel_cache_manager
from .utils.request_cache import get_request_cache, RequestFingerprint

logger = logging.getLogger(__name__)

class TagNotFoundError(Exception):
    """æ ‡ç­¾æœªæ‰¾åˆ°é”™è¯¯"""
    def __init__(self, tags: List[str], message: str = None):
        self.tags = tags
        if message is None:
            if len(tags) == 1:
                message = f"æ²¡æœ‰æ‰¾åˆ°åŒ¹é…æ ‡ç­¾ '{tags[0]}' çš„æ¨¡å‹"
            else:
                message = f"æ²¡æœ‰æ‰¾åˆ°åŒæ—¶åŒ¹é…æ ‡ç­¾ {tags} çš„æ¨¡å‹"
        super().__init__(message)

@dataclass
class RoutingScore:
    """è·¯ç”±è¯„åˆ†ç»“æœ"""
    channel: Channel
    total_score: float
    cost_score: float
    speed_score: float
    quality_score: float
    reliability_score: float
    reason: str
    matched_model: Optional[str] = None  # å¯¹äºæ ‡ç­¾è·¯ç”±ï¼Œè®°å½•å®é™…åŒ¹é…çš„æ¨¡å‹

@dataclass
class ChannelCandidate:
    """å€™é€‰æ¸ é“ä¿¡æ¯"""
    channel: Channel
    matched_model: Optional[str] = None  # å¯¹äºæ ‡ç­¾è·¯ç”±ï¼Œè®°å½•å®é™…åŒ¹é…çš„æ¨¡å‹

@dataclass
class RoutingRequest:
    """è·¯ç”±è¯·æ±‚"""
    model: str
    messages: List[Dict[str, Any]]
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    stream: bool = False
    functions: Optional[List[Dict[str, Any]]] = None
    required_capabilities: List[str] = None

class JSONRouter:
    """åŸºäºPydanticéªŒè¯åé…ç½®çš„è·¯ç”±å™¨"""
    
    def __init__(self, config_loader: Optional[YAMLConfigLoader] = None):
        self.config_loader = config_loader or get_yaml_config_loader()
        self.config = self.config_loader.config
        
        # æ ‡ç­¾ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
        self._tag_cache: Dict[str, List[str]] = {}
        self._available_tags_cache: Optional[set] = None
        self._available_models_cache: Optional[List[str]] = None
        
        # æ¨¡å‹åˆ†æå™¨å’Œç¼“å­˜ç®¡ç†å™¨
        self.model_analyzer = get_model_analyzer()
        self.cache_manager = get_channel_cache_manager()
        
    async def route_request(self, request: RoutingRequest) -> List[RoutingScore]:
        """
        è·¯ç”±è¯·æ±‚ï¼Œè¿”å›æŒ‰è¯„åˆ†æ’åºçš„å€™é€‰æ¸ é“åˆ—è¡¨ã€‚
        
        æ”¯æŒè¯·æ±‚çº§ç¼“å­˜ä»¥æé«˜æ€§èƒ½ï¼š
        - ç¼“å­˜TTL: 60ç§’
        - åŸºäºè¯·æ±‚æŒ‡çº¹çš„æ™ºèƒ½ç¼“å­˜é”®ç”Ÿæˆ
        - è‡ªåŠ¨æ•…éšœè½¬ç§»å’Œç¼“å­˜å¤±æ•ˆ
        """
        logger.info(f"ğŸš€ ROUTING START: Processing request for model '{request.model}'")
        
        # ç”Ÿæˆè¯·æ±‚æŒ‡çº¹ç”¨äºç¼“å­˜
        fingerprint = RequestFingerprint(
            model=request.model,
            routing_strategy=getattr(request, 'routing_strategy', 'balanced'),
            required_capabilities=getattr(request, 'required_capabilities', None),
            min_context_length=getattr(request, 'min_context_length', None),
            max_cost_per_1k=getattr(request, 'max_cost_per_1k', None),
            prefer_local=getattr(request, 'prefer_local', False),
            exclude_providers=getattr(request, 'exclude_providers', None),
            # æ–°å¢å½±å“è·¯ç”±çš„å‚æ•°
            max_tokens=getattr(request, 'max_tokens', None),
            temperature=getattr(request, 'temperature', None),
            stream=getattr(request, 'stream', False),
            has_functions=bool(getattr(request, 'functions', None) or getattr(request, 'tools', None))
        )
        
        # æ£€æŸ¥ç¼“å­˜
        cache = get_request_cache()
        cached_result = await cache.get_cached_selection(fingerprint)
        
        if cached_result:
            # ç¼“å­˜å‘½ä¸­ï¼Œè½¬æ¢ä¸ºRoutingScoreåˆ—è¡¨
            logger.info(f"âš¡ CACHE HIT: Using cached selection for '{request.model}' "
                       f"(cost: ${cached_result.cost_estimate:.4f})")
            
            # æ„å»ºRoutingScoreåˆ—è¡¨
            scores = []
            
            # ä¸»è¦æ¸ é“
            primary_score = RoutingScore(
                channel=cached_result.primary_channel,
                total_score=1.0,  # ç¼“å­˜çš„ç»“æœä¼˜å…ˆçº§æœ€é«˜
                cost_score=1.0 if cached_result.cost_estimate == 0.0 else 0.8,
                speed_score=0.9,  # ç¼“å­˜è®¿é—®å¾ˆå¿«
                quality_score=0.8,
                reliability_score=0.9,
                reason=f"CACHED: {cached_result.selection_reason}",
                matched_model=cached_result.primary_matched_model or request.model  # ä½¿ç”¨ç¼“å­˜ä¸­çš„çœŸå®æ¨¡å‹å
            )
            scores.append(primary_score)
            
            # å¤‡é€‰æ¸ é“  
            for i, backup_channel in enumerate(cached_result.backup_channels):
                # è·å–å¯¹åº”çš„å¤‡é€‰æ¨¡å‹å
                backup_matched_model = None
                if cached_result.backup_matched_models and i < len(cached_result.backup_matched_models):
                    backup_matched_model = cached_result.backup_matched_models[i]
                
                backup_score = RoutingScore(
                    channel=backup_channel,
                    total_score=0.9 - i * 0.1,  # é€’å‡ä¼˜å…ˆçº§
                    cost_score=0.7,
                    speed_score=0.8,
                    quality_score=0.7,
                    reliability_score=0.8,
                    reason=f"CACHED_BACKUP_{i+1}",
                    matched_model=backup_matched_model or request.model  # ä½¿ç”¨ç¼“å­˜ä¸­çš„çœŸå®æ¨¡å‹å
                )
                scores.append(backup_score)
            
            return scores
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæ­£å¸¸è·¯ç”±é€»è¾‘
        logger.info(f"â±ï¸  CACHE MISS: Computing fresh routing for '{request.model}'")
        try:
            # ç¬¬ä¸€æ­¥ï¼šè·å–å€™é€‰æ¸ é“
            logger.info(f"ğŸ” STEP 1: Finding candidate channels...")
            candidates = self._get_candidate_channels(request)
            if not candidates:
                logger.warning(f"âŒ ROUTING FAILED: No suitable channels found for model '{request.model}'")
                return []
            
            logger.info(f"âœ… STEP 1 COMPLETE: Found {len(candidates)} candidate channels")
            
            # ç¬¬äºŒæ­¥ï¼šè¿‡æ»¤æ¸ é“
            logger.info(f"ğŸ”§ STEP 2: Filtering channels by health and availability...")
            filtered_candidates = self._filter_channels(candidates, request)
            if not filtered_candidates:
                logger.warning(f"âŒ ROUTING FAILED: No available channels after filtering for model '{request.model}'")
                return []
            
            logger.info(f"âœ… STEP 2 COMPLETE: {len(filtered_candidates)} channels passed filtering (filtered out {len(candidates) - len(filtered_candidates)})")
            
            # ç¬¬ä¸‰æ­¥ï¼šè¯„åˆ†å’Œæ’åº
            logger.info(f"ğŸ¯ STEP 3: Scoring and ranking channels...")
            scored_channels = self._score_channels(filtered_candidates, request)
            if not scored_channels:
                logger.warning(f"âŒ ROUTING FAILED: Failed to score any channels for model '{request.model}'")
                return []
            
            logger.info(f"âœ… STEP 3 COMPLETE: Scored {len(scored_channels)} channels")
            
            # ç¼“å­˜ç»“æœï¼ˆå¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰
            if scored_channels:
                primary_channel = scored_channels[0].channel
                backup_channels = [score.channel for score in scored_channels[1:6]]  # æœ€å¤š5ä¸ªå¤‡é€‰
                selection_reason = scored_channels[0].reason
                cost_estimate = self._estimate_cost_for_channel(primary_channel, request)
                
                # æå–çœŸå®çš„åŒ¹é…æ¨¡å‹å
                primary_matched_model = scored_channels[0].matched_model
                backup_matched_models = [score.matched_model for score in scored_channels[1:6]]
                
                try:
                    # å¼‚æ­¥ä¿å­˜åˆ°ç¼“å­˜
                    cache_key = await cache.cache_selection(
                        fingerprint=fingerprint,
                        primary_channel=primary_channel,
                        backup_channels=backup_channels,
                        selection_reason=selection_reason,
                        cost_estimate=cost_estimate,
                        ttl_seconds=60,  # 1åˆ†é’ŸTTL
                        primary_matched_model=primary_matched_model,
                        backup_matched_models=backup_matched_models
                    )
                    logger.debug(f"ğŸ’¾ CACHED RESULT: {cache_key} -> {primary_channel.name}")
                except Exception as cache_error:
                    logger.warning(f"âš ï¸  CACHE SAVE FAILED: {cache_error}, continuing without caching")
            
            logger.info(f"ğŸ‰ ROUTING SUCCESS: Ready to attempt {len(scored_channels)} channels in ranked order for model '{request.model}'")
            
            return scored_channels
            
        except TagNotFoundError:
            # è®©TagNotFoundErrorä¼ æ’­å‡ºå»ï¼Œä»¥ä¾¿ä¸Šå±‚å¤„ç†
            raise
        except Exception as e:
            logger.error(f"âŒ ROUTING ERROR: Request failed for model '{request.model}': {e}", exc_info=True)
            return []
    
    def _get_candidate_channels(self, request: RoutingRequest) -> List[ChannelCandidate]:
        """è·å–å€™é€‰æ¸ é“ï¼Œæ”¯æŒæŒ‰æ ‡ç­¾é›†åˆæˆ–ç‰©ç†æ¨¡å‹è¿›è¡Œæ™ºèƒ½è·¯ç”±"""
        
        if request.model.startswith("tag:"):
            tag_query = request.model.split(":", 1)[1]
            
            # æ”¯æŒå¤šæ ‡ç­¾æŸ¥è¯¢ï¼Œç”¨é€—å·åˆ†éš”ï¼štag:qwen,free,!local
            if "," in tag_query:
                tag_parts = [tag.strip() for tag in tag_query.split(",")]
                positive_tags = []
                negative_tags = []
                
                for tag_part in tag_parts:
                    if tag_part.startswith("!"):
                        # è´Ÿæ ‡ç­¾ï¼š!local
                        negative_tags.append(tag_part[1:].lower())
                    else:
                        # æ­£æ ‡ç­¾ï¼šfree, qwen3
                        positive_tags.append(tag_part.lower())
                
                logger.info(f"ğŸ·ï¸  TAG ROUTING: Processing multi-tag query '{request.model}' -> positive: {positive_tags}, negative: {negative_tags}")
                candidates = self._get_candidate_channels_by_auto_tags(positive_tags, negative_tags)
                if not candidates:
                    logger.error(f"âŒ TAG NOT FOUND: No models found matching tags {positive_tags} excluding {negative_tags}")
                    raise TagNotFoundError(positive_tags + [f"!{tag}" for tag in negative_tags])
                logger.info(f"ğŸ·ï¸  TAG ROUTING: Multi-tag query found {len(candidates)} candidate channels")
                return candidates
            else:
                # å•æ ‡ç­¾æŸ¥è¯¢ - æ”¯æŒè´Ÿæ ‡ç­¾ï¼štag:!local
                tag_part = tag_query.strip()
                if tag_part.startswith("!"):
                    # è´Ÿæ ‡ç­¾å•ç‹¬æŸ¥è¯¢ï¼štag:!local (åŒ¹é…æ‰€æœ‰ä¸åŒ…å«localçš„æ¨¡å‹)
                    negative_tag = tag_part[1:].lower()
                    logger.info(f"ğŸ·ï¸  TAG ROUTING: Processing negative tag query '{request.model}' -> excluding: '{negative_tag}'")
                    candidates = self._get_candidate_channels_by_auto_tags([], [negative_tag])
                else:
                    # æ­£å¸¸å•æ ‡ç­¾æŸ¥è¯¢
                    tag = tag_part.lower()
                    logger.info(f"ğŸ·ï¸  TAG ROUTING: Processing single tag query '{request.model}' -> tag: '{tag}'")
                    candidates = self._get_candidate_channels_by_auto_tags([tag], [])
                
                if not candidates:
                    logger.error(f"âŒ TAG NOT FOUND: No models found for query '{request.model}'")
                    raise TagNotFoundError([tag_query])
                logger.info(f"ğŸ·ï¸  TAG ROUTING: Found {len(candidates)} candidate channels")
                return candidates
        
        # étag:å‰ç¼€çš„æ¨¡å‹åç§° - é¦–å…ˆå°è¯•ç‰©ç†æ¨¡å‹ï¼Œç„¶åå°è¯•è‡ªåŠ¨æ ‡ç­¾åŒ–
        candidate_channels = []
        all_enabled_channels = self.config_loader.get_enabled_channels()
        model_cache = self.config_loader.get_model_cache()

        # 1. é¦–å…ˆå°è¯•ä½œä¸ºç‰©ç†æ¨¡å‹æŸ¥æ‰¾
        physical_candidates = []
        for channel in all_enabled_channels:
            if channel.id in model_cache:
                discovered_info = model_cache[channel.id]
                models_data = discovered_info.get("models_data", {}) if isinstance(discovered_info, dict) else {}
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç²¾ç¡®åŒ¹é…çš„æ¨¡å‹
                if request.model in discovered_info.get("models", []):
                    # è·å–çœŸå®çš„æ¨¡å‹ID - å¯èƒ½ä¸è¯·æ±‚çš„ä¸åŒï¼ˆåˆ«åæ˜ å°„ï¼‰
                    real_model_id = request.model
                    if models_data and request.model in models_data:
                        model_info = models_data[request.model]
                        real_model_id = model_info.get("id", request.model)
                    
                    logger.debug(f"ğŸ” PHYSICAL MODEL: Found '{request.model}' -> '{real_model_id}' in channel '{channel.name}'")
                    physical_candidates.append(ChannelCandidate(
                        channel=channel,
                        matched_model=real_model_id  # ä½¿ç”¨çœŸå®çš„æ¨¡å‹ID
                    ))
        
        # 2. åŒæ—¶å°è¯•è‡ªåŠ¨æ ‡ç­¾åŒ–æŸ¥æ‰¾ï¼ˆå…è´¹æ¨¡å‹ä¼˜å…ˆè€ƒè™‘ï¼‰
        auto_tag_candidates = []
        logger.info(f"ğŸ”„ AUTO TAGGING: Extracting tags from model name '{request.model}' for comprehensive search")
        auto_tags = self._extract_tags_from_model_name(request.model)
        if auto_tags:
            logger.info(f"ğŸ·ï¸  AUTO TAGGING: Extracted tags {auto_tags} from model name '{request.model}'")
            auto_tag_candidates = self._get_candidate_channels_by_auto_tags(auto_tags)
            if auto_tag_candidates:
                logger.info(f"ğŸ·ï¸  AUTO TAGGING: Found {len(auto_tag_candidates)} candidate channels using auto-extracted tags")
            else:
                logger.warning(f"ğŸ·ï¸  AUTO TAGGING: No channels found for auto-extracted tags {auto_tags}")
        
        # 3. åˆå¹¶ç‰©ç†æ¨¡å‹å’Œè‡ªåŠ¨æ ‡ç­¾åŒ–çš„ç»“æœï¼Œå»é‡
        all_candidates = physical_candidates.copy()
        
        # æ·»åŠ è‡ªåŠ¨æ ‡ç­¾åŒ–çš„å€™é€‰ï¼Œé¿å…é‡å¤
        for tag_candidate in auto_tag_candidates:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ç›¸åŒçš„ channel + model ç»„åˆ
            duplicate_found = False
            for existing in all_candidates:
                if (existing.channel.id == tag_candidate.channel.id and 
                    existing.matched_model == tag_candidate.matched_model):
                    duplicate_found = True
                    break
            
            if not duplicate_found:
                all_candidates.append(tag_candidate)
        
        if all_candidates:
            physical_count = len(physical_candidates)
            tag_count = len(auto_tag_candidates)
            total_count = len(all_candidates)
            logger.info(f"ğŸ” COMPREHENSIVE SEARCH: Found {total_count} total candidates "
                       f"(physical: {physical_count}, auto-tag: {tag_count}, merged without duplicates)")
            return all_candidates
        
        # 4. æœ€åå°è¯•ä»é…ç½®ä¸­æŸ¥æ‰¾
        config_channels = self.config_loader.get_channels_by_model(request.model)
        if config_channels:
            logger.info(f"ğŸ“‹ CONFIG FALLBACK: Found {len(config_channels)} channels in configuration for model '{request.model}'")
            # å¯¹äºé…ç½®æŸ¥æ‰¾ï¼Œå°è¯•è·å–çœŸå®çš„æ¨¡å‹ID
            config_candidates = []
            for ch in config_channels:
                real_model_id = request.model
                if ch.id in model_cache:
                    discovered_info = model_cache[ch.id]
                    models_data = discovered_info.get("models_data", {}) if isinstance(discovered_info, dict) else {}
                    if models_data and request.model in models_data:
                        model_info = models_data[request.model]
                        real_model_id = model_info.get("id", request.model)
                
                config_candidates.append(ChannelCandidate(channel=ch, matched_model=real_model_id))
            return config_candidates
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›ç©ºåˆ—è¡¨
        logger.warning(f"âŒ NO MATCH: No channels found for model '{request.model}' (tried physical, auto-tag, and config)")
        return []
    
    def _filter_channels(self, channels: List[ChannelCandidate], request: RoutingRequest) -> List[ChannelCandidate]:
        """è¿‡æ»¤æ¸ é“"""
        filtered = []
        
        # è·å–è·¯ç”±é…ç½®ä¸­çš„è¿‡æ»¤æ¡ä»¶
        routing_config = self.config.routing if hasattr(self.config, 'routing') else None
        if routing_config and hasattr(routing_config, 'model_filters'):
            model_filters = routing_config.model_filters or {}
        else:
            model_filters = {}
        
        # å®‰å…¨è·å–model_filtersä¸­çš„å€¼
        min_context_length = getattr(model_filters, 'min_context_length', 0) if hasattr(model_filters, 'min_context_length') else model_filters.get('min_context_length', 0) if isinstance(model_filters, dict) else 0
        min_parameter_count = getattr(model_filters, 'min_parameter_count', 0) if hasattr(model_filters, 'min_parameter_count') else model_filters.get('min_parameter_count', 0) if isinstance(model_filters, dict) else 0
        exclude_embedding = getattr(model_filters, 'exclude_embedding_models', True) if hasattr(model_filters, 'exclude_embedding_models') else model_filters.get('exclude_embedding_models', True) if isinstance(model_filters, dict) else True
        exclude_vision_only = getattr(model_filters, 'exclude_vision_only_models', True) if hasattr(model_filters, 'exclude_vision_only_models') else model_filters.get('exclude_vision_only_models', True) if isinstance(model_filters, dict) else True
        
        for candidate in channels:
            channel = candidate.channel
            if not channel.enabled or not channel.api_key:
                continue
            
            health_score = self.config_loader.runtime_state.health_scores.get(channel.id, 1.0)
            if health_score < 0.3:
                continue
            
            # æ¨¡å‹è§„æ ¼è¿‡æ»¤
            if candidate.matched_model and (min_context_length > 0 or min_parameter_count > 0 or exclude_embedding or exclude_vision_only):
                model_name = candidate.matched_model
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºembeddingæ¨¡å‹
                if exclude_embedding and self._is_embedding_model(model_name):
                    logger.debug(f"Filtered out embedding model: {model_name}")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯è§†è§‰æ¨¡å‹
                if exclude_vision_only and self._is_vision_only_model(model_name):
                    logger.debug(f"Filtered out vision-only model: {model_name}")
                    continue
                
                # è·å–æ¨¡å‹è§„æ ¼
                model_specs = self._get_model_specs(channel.id, model_name)
                
                # ä¸Šä¸‹æ–‡é•¿åº¦è¿‡æ»¤
                if min_context_length > 0:
                    context_length = model_specs.get('context_length', 0) if model_specs else 0
                    if context_length < min_context_length:
                        logger.debug(f"Filtered out model {model_name}: context {context_length} < {min_context_length}")
                        continue
                
                # å‚æ•°æ•°é‡è¿‡æ»¤
                if min_parameter_count > 0:
                    param_count = model_specs.get('parameter_count', 0) if model_specs else 0
                    if param_count < min_parameter_count:
                        logger.debug(f"Filtered out model {model_name}: params {param_count}M < {min_parameter_count}M")
                        continue
            
            # TODO: Add capability filtering when needed
            # if request.required_capabilities:
            #     if not all(cap in channel.capabilities for cap in request.required_capabilities):
            #         continue
            
            filtered.append(candidate)
        return filtered
    
    def _score_channels(self, channels: List[ChannelCandidate], request: RoutingRequest) -> List[RoutingScore]:
        """è®¡ç®—æ¸ é“è¯„åˆ†"""
        logger.info(f"ğŸ“Š SCORING: Evaluating {len(channels)} candidate channels for model '{request.model}'")
        
        scored_channels = []
        strategy = self._get_routing_strategy(request.model)
        
        logger.info(f"ğŸ“Š SCORING: Using routing strategy with {len(strategy)} rules")
        for rule in strategy:
            logger.debug(f"ğŸ“Š SCORING: Strategy rule: {rule['field']} (weight: {rule['weight']}, order: {rule['order']})")
        
        for candidate in channels:
            channel = candidate.channel
            cost_score = self._calculate_cost_score(channel, request)
            speed_score = self._calculate_speed_score(channel)
            quality_score = self._calculate_quality_score(channel, candidate.matched_model)
            reliability_score = self._calculate_reliability_score(channel)
            parameter_score = self._calculate_parameter_score(channel, candidate.matched_model)
            context_score = self._calculate_context_score(channel, candidate.matched_model)
            free_score = self._calculate_free_score(channel, candidate.matched_model)
            local_score = self._calculate_local_score(channel, candidate.matched_model)
            
            total_score = self._calculate_total_score(
                strategy, cost_score, speed_score, quality_score, reliability_score, 
                parameter_score, context_score, free_score, local_score
            )
            
            # ä¸ºäº†å‡å°‘æ—¥å¿—å†—ä½™ï¼Œåªæ˜¾ç¤ºæ¨¡å‹åç§°å’Œæ€»åˆ†
            model_display = candidate.matched_model or channel.model_name
            logger.info(f"ğŸ“Š SCORE: '{channel.name}' -> '{model_display}' = {total_score:.3f} (Q:{quality_score:.2f})")
            
            scored_channels.append(RoutingScore(
                channel=channel, total_score=total_score, cost_score=cost_score,
                speed_score=speed_score, quality_score=quality_score,
                reliability_score=reliability_score, 
                reason=f"cost:{cost_score:.2f} speed:{speed_score:.2f} quality:{quality_score:.2f} reliability:{reliability_score:.2f}",
                matched_model=candidate.matched_model
            ))
        
        # ä½¿ç”¨åˆ†å±‚ä¼˜å…ˆçº§æ’åºï¼Œè€Œä¸æ˜¯ç®€å•çš„æ€»åˆ†æ’åº
        scored_channels = self._hierarchical_sort(scored_channels)
        
        logger.info(f"ğŸ† SCORING RESULT: Channels ranked by score:")
        for i, scored in enumerate(scored_channels[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            logger.info(f"ğŸ†   #{i+1}: '{scored.channel.name}' (Score: {scored.total_score:.3f})")
        
        return scored_channels
    
    def _get_routing_strategy(self, model: str) -> List[Dict[str, Any]]:
        """è·å–å¹¶è§£æè·¯ç”±ç­–ç•¥ï¼Œå§‹ç»ˆè¿”å›è§„åˆ™åˆ—è¡¨"""
        routing_config = self.config.routing if hasattr(self.config, 'routing') else None
        if routing_config and hasattr(routing_config, 'default_strategy'):
            strategy_name = routing_config.default_strategy or 'balanced'
        else:
            strategy_name = 'balanced'
        
        # å°è¯•ä»é…ç½®ä¸­è·å–è‡ªå®šä¹‰ç­–ç•¥
        if routing_config and hasattr(routing_config, 'sorting_strategies'):
            custom_strategies = routing_config.sorting_strategies or {}
        else:
            custom_strategies = {}
        if strategy_name in custom_strategies:
            return custom_strategies[strategy_name]

        # å›é€€åˆ°é¢„å®šä¹‰ç­–ç•¥
        predefined_strategies = {
            "cost_first": [
                {"field": "cost_score", "order": "desc", "weight": 0.4},
                {"field": "parameter_score", "order": "desc", "weight": 0.25},
                {"field": "context_score", "order": "desc", "weight": 0.2},
                {"field": "speed_score", "order": "desc", "weight": 0.15}
            ],
            "free_first": [
                {"field": "free_score", "order": "desc", "weight": 0.5},
                {"field": "cost_score", "order": "desc", "weight": 0.3},
                {"field": "speed_score", "order": "desc", "weight": 0.15},
                {"field": "reliability_score", "order": "desc", "weight": 0.05}
            ],
            "local_first": [
                {"field": "local_score", "order": "desc", "weight": 0.6},
                {"field": "speed_score", "order": "desc", "weight": 0.25},
                {"field": "cost_score", "order": "desc", "weight": 0.1},
                {"field": "reliability_score", "order": "desc", "weight": 0.05}
            ],
            "cost_optimized": [
                {"field": "cost_score", "order": "desc", "weight": 0.7},
                {"field": "reliability_score", "order": "desc", "weight": 0.2},
                {"field": "speed_score", "order": "desc", "weight": 0.1}
            ],
            "speed_optimized": [
                {"field": "speed_score", "order": "desc", "weight": 0.4},
                {"field": "cost_score", "order": "desc", "weight": 0.3},
                {"field": "parameter_score", "order": "desc", "weight": 0.2},
                {"field": "context_score", "order": "desc", "weight": 0.1}
            ],
            "quality_optimized": [
                {"field": "parameter_score", "order": "desc", "weight": 0.4},
                {"field": "context_score", "order": "desc", "weight": 0.3},
                {"field": "quality_score", "order": "desc", "weight": 0.2},
                {"field": "cost_score", "order": "desc", "weight": 0.1}
            ],
            "balanced": [
                {"field": "cost_score", "order": "desc", "weight": 0.3},
                {"field": "parameter_score", "order": "desc", "weight": 0.25},
                {"field": "context_score", "order": "desc", "weight": 0.2},
                {"field": "speed_score", "order": "desc", "weight": 0.15},
                {"field": "reliability_score", "order": "desc", "weight": 0.1}
            ]
        }
        
        return predefined_strategies.get(strategy_name, predefined_strategies["cost_first"])

    def _calculate_cost_score(self, channel: Channel, request: RoutingRequest) -> float:
        """è®¡ç®—æˆæœ¬è¯„åˆ†(0-1ï¼Œè¶Šä½æˆæœ¬è¶Šé«˜åˆ†)"""
        pricing = channel.pricing
        if not pricing:
            return 0.5
        
        # ä¼°ç®—tokenæ•°é‡
        input_tokens = self._estimate_tokens(request.messages)
        max_output_tokens = request.max_tokens or 1000
        
        # è®¡ç®—æˆæœ¬
        input_cost = pricing.get("input_cost_per_1k", 0.001) * input_tokens / 1000
        output_cost = pricing.get("output_cost_per_1k", 0.002) * max_output_tokens / 1000
        total_cost = (input_cost + output_cost) * pricing.get("effective_multiplier", 1.0)
        
        # è½¬æ¢ä¸ºè¯„åˆ† (æˆæœ¬è¶Šä½åˆ†æ•°è¶Šé«˜)
        # å‡è®¾æœ€é«˜æˆæœ¬ä¸º0.1ç¾å…ƒ
        max_cost = 0.1
        score = max(0, 1 - (total_cost / max_cost))
        
        return min(1.0, score)
    
    # å†…ç½®æ¨¡å‹è´¨é‡æ’å (åˆ†æ•°è¶Šé«˜è¶Šå¥½, æ»¡åˆ†100)
    MODEL_QUALITY_RANKING = {
        # OpenAI ç³»åˆ—
        "gpt-4o": 98, "gpt-4-turbo": 95, "gpt-4": 90, "gpt-4o-mini": 85, "gpt-3.5-turbo": 70,
        # Anthropic ç³»åˆ—
        "claude-3-5-sonnet": 99, "claude-3-opus": 97, "claude-3-sonnet": 92, "claude-3-haiku": 80,
        # Meta Llama ç³»åˆ—
        "llama-3.1-70b": 93, "llama-3.1-8b": 82, "llama-3-70b": 90, "llama-3-8b": 80,
        # Qwen ç³»åˆ—
        "qwen2.5-72b-instruct": 91, "qwen2-72b-instruct": 90, "qwen2-7b-instruct": 78,
        "qwen3-coder": 88, "qwen3-30b": 85, "qwen3-14b": 82, "qwen3-8b": 80, "qwen3-4b": 75, 
        "qwen3-1.7b": 65, "qwen3-0.6b": 60,
        # DeepSeek ç³»åˆ—
        "deepseek-r1": 89, "deepseek-v3": 87, "deepseek-coder": 85,
        # Moonshot ç³»åˆ—
        "moonshot-v1-128k": 88, "moonshot-v1-32k": 87, "moonshot-v1-8k": 86,
        # 01.AI ç³»åˆ—
        "yi-large": 89, "yi-lightning": 75,
        # Google ç³»åˆ—
        "gemma-3-12b": 78, "gemma-3-9b": 75, "gemma-3-270m": 55,
    }

    def _calculate_quality_score(self, channel: Channel, matched_model: Optional[str] = None) -> float:
        """æ ¹æ®å†…ç½®æ’åå’Œæ¨¡å‹è§„æ ¼åŠ¨æ€è®¡ç®—è´¨é‡è¯„åˆ†"""
        # ä¼˜å…ˆä½¿ç”¨åŒ¹é…çš„æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ¸ é“é»˜è®¤æ¨¡å‹
        model_name = matched_model or channel.model_name
        model_name_lower = model_name.lower()
        simple_model_name = model_name_lower.split('/')[-1]
        
        # 1. åŸºç¡€è´¨é‡è¯„åˆ†ï¼ˆä»å†…ç½®æ’åï¼‰
        base_quality_score = self.MODEL_QUALITY_RANKING.get(simple_model_name)
        
        if base_quality_score is None:
            for key, score in self.MODEL_QUALITY_RANKING.items():
                if key in model_name_lower:
                    base_quality_score = score
                    break
        
        # 2. è·å–æ¨¡å‹è§„æ ¼ä¿¡æ¯
        model_specs = None
        try:
            # ä»æ¸ é“ç¼“å­˜ä¸­è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
            channel_cache = self.cache_manager.load_channel_models(channel.id)
            if channel_cache and 'models' in channel_cache:
                model_specs = channel_cache['models'].get(model_name)
        except Exception as e:
            logger.debug(f"Failed to load model specs for {model_name}: {e}")
        
        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œä½¿ç”¨åˆ†æå™¨åˆ†ææ¨¡å‹åç§°
        if not model_specs:
            analyzed_specs = self.model_analyzer.analyze_model(model_name)
            model_specs = {
                'parameter_count': analyzed_specs.parameter_count,
                'context_length': analyzed_specs.context_length
            }
        
        # 3. æ ¹æ®å‚æ•°æ•°é‡è°ƒæ•´è¯„åˆ†
        param_multiplier = 1.0
        if model_specs and 'parameter_count' in model_specs:
            param_count = model_specs['parameter_count']
            if param_count:
                # å‚æ•°æ•°é‡è¯„åˆ†æ›²çº¿ï¼ˆè¶Šå¤§è¶Šå¥½ï¼Œä½†æœ‰è¾¹é™…é€’å‡æ•ˆåº”ï¼‰
                if param_count >= 100000:      # 100B+
                    param_multiplier = 1.3
                elif param_count >= 70000:     # 70B+
                    param_multiplier = 1.25
                elif param_count >= 30000:     # 30B+
                    param_multiplier = 1.2
                elif param_count >= 8000:      # 8B+
                    param_multiplier = 1.1
                elif param_count >= 4000:      # 4B+
                    param_multiplier = 1.05
                elif param_count >= 1000:      # 1B+
                    param_multiplier = 1.0
                elif param_count >= 500:       # 500M+
                    param_multiplier = 0.9
                else:                          # <500M
                    param_multiplier = 0.8
                
                logger.debug(f"Model {model_name}: {param_count}M params -> multiplier {param_multiplier}")
        
        # 4. æ ¹æ®ä¸Šä¸‹æ–‡é•¿åº¦è°ƒæ•´è¯„åˆ†
        context_bonus = 0.0
        if model_specs and 'context_length' in model_specs:
            context_length = model_specs['context_length']
            if context_length:
                # ä¸Šä¸‹æ–‡é•¿åº¦å¥–åŠ±ï¼ˆæ›´é•¿çš„ä¸Šä¸‹æ–‡åœ¨åŒç­‰æ¡ä»¶ä¸‹æ›´å¥½ï¼‰
                if context_length >= 200000:     # 200k+
                    context_bonus = 15
                elif context_length >= 128000:   # 128k+
                    context_bonus = 10
                elif context_length >= 32000:    # 32k+
                    context_bonus = 5
                elif context_length >= 16000:    # 16k+
                    context_bonus = 2
                
                logger.debug(f"Model {model_name}: {context_length} context -> bonus {context_bonus}")
        
        # 5. è®¡ç®—æœ€ç»ˆè¯„åˆ†
        final_score = (base_quality_score or 70) * param_multiplier + context_bonus
        
        # 6. å½’ä¸€åŒ–åˆ°0-1èŒƒå›´ï¼Œæœ€é«˜åˆ†è®¾ä¸º120åˆ†
        normalized_score = min(1.0, final_score / 120.0)
        
        logger.debug(f"Quality scoring for {model_name}: base={base_quality_score}, "
                    f"param_mult={param_multiplier}, context_bonus={context_bonus}, "
                    f"final={final_score}, normalized={normalized_score:.3f}")
        
        return normalized_score
    
    def _calculate_parameter_score(self, channel: Channel, matched_model: Optional[str] = None) -> float:
        """è®¡ç®—å‚æ•°æ•°é‡è¯„åˆ†"""
        model_name = matched_model or channel.model_name
        model_specs = self._get_model_specs(channel.id, model_name)
        
        if not model_specs or not model_specs.get('parameter_count'):
            return 0.5  # é»˜è®¤ä¸­ç­‰è¯„åˆ†
        
        param_count = model_specs['parameter_count']
        
        # å‚æ•°æ•°é‡è¯„åˆ†æ›²çº¿ï¼ˆå¯¹æ•°å¼é€’å¢ï¼Œæœ‰è¾¹é™…é€’å‡æ•ˆåº”ï¼‰
        if param_count >= 500000:      # 500B+
            score = 1.0
        elif param_count >= 100000:    # 100B+
            score = 0.95
        elif param_count >= 70000:     # 70B+
            score = 0.9
        elif param_count >= 30000:     # 30B+
            score = 0.85
        elif param_count >= 8000:      # 8B+
            score = 0.8
        elif param_count >= 4000:      # 4B+
            score = 0.75
        elif param_count >= 1000:      # 1B+
            score = 0.7
        elif param_count >= 500:       # 500M+
            score = 0.6
        else:                          # <500M
            score = 0.4
        
        logger.debug(f"Parameter score for {model_name}: {param_count}M -> {score}")
        return score
    
    def _calculate_context_score(self, channel: Channel, matched_model: Optional[str] = None) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡é•¿åº¦è¯„åˆ†"""
        model_name = matched_model or channel.model_name
        model_specs = self._get_model_specs(channel.id, model_name)
        
        if not model_specs or not model_specs.get('context_length'):
            return 0.5  # é»˜è®¤ä¸­ç­‰è¯„åˆ†
        
        context_length = model_specs['context_length']
        
        # ä¸Šä¸‹æ–‡é•¿åº¦è¯„åˆ†æ›²çº¿
        if context_length >= 1000000:    # 1M+
            score = 1.0
        elif context_length >= 200000:   # 200k+
            score = 0.95
        elif context_length >= 128000:   # 128k+
            score = 0.9
        elif context_length >= 32000:    # 32k+
            score = 0.8
        elif context_length >= 16000:    # 16k+
            score = 0.7
        elif context_length >= 8000:     # 8k+
            score = 0.6
        elif context_length >= 4000:     # 4k+
            score = 0.5
        else:                            # <4k
            score = 0.3
        
        logger.debug(f"Context score for {model_name}: {context_length} -> {score}")
        return score
    
    def _get_model_specs(self, channel_id: str, model_name: str) -> Optional[Dict[str, Any]]:
        """è·å–æ¨¡å‹è§„æ ¼ä¿¡æ¯"""
        try:
            # ä»æ¸ é“ç¼“å­˜ä¸­è·å–
            channel_cache = self.cache_manager.load_channel_models(channel_id)
            if channel_cache and 'models' in channel_cache:
                return channel_cache['models'].get(model_name)
        except Exception as e:
            logger.debug(f"Failed to load model specs for {model_name}: {e}")
        
        # å›é€€åˆ°åˆ†æå™¨åˆ†æ
        analyzed_specs = self.model_analyzer.analyze_model(model_name)
        return {
            'parameter_count': analyzed_specs.parameter_count,
            'context_length': analyzed_specs.context_length
        }
    
    def _is_embedding_model(self, model_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºembeddingæ¨¡å‹"""
        name_lower = model_name.lower()
        embedding_keywords = ['embedding', 'embed', 'text-embedding', 'bge-', 'gte-', 'e5-']
        return any(keyword in name_lower for keyword in embedding_keywords)
    
    def _is_vision_only_model(self, model_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºçº¯è§†è§‰æ¨¡å‹"""
        name_lower = model_name.lower()
        vision_keywords = ['vision-only', 'image-only', 'ocr-only']
        return any(keyword in name_lower for keyword in vision_keywords)

    def _calculate_speed_score(self, channel: Channel) -> float:
        """æ ¹æ®å¹³å‡å»¶è¿ŸåŠ¨æ€è®¡ç®—é€Ÿåº¦è¯„åˆ†"""
        channel_stats = self.config_loader.runtime_state.channel_stats.get(channel.id)
        if channel_stats and "avg_latency_ms" in channel_stats:
            avg_latency = channel_stats["avg_latency_ms"]
            base_latency = 2000.0
            score = max(0.0, 1.0 - (avg_latency / base_latency))
            return 0.1 + score * 0.9
        return channel.performance.get("speed_score", 0.8)

    def _calculate_reliability_score(self, channel: Channel) -> float:
        """è®¡ç®—å¯é æ€§è¯„åˆ†"""
        health_score = self.config_loader.runtime_state.health_scores.get(channel.id, 1.0)
        return health_score

    def _calculate_free_score(self, channel: Channel, model_name: str = None) -> float:
        """è®¡ç®—å…è´¹ä¼˜å…ˆè¯„åˆ† - ä¸¥æ ¼éªŒè¯çœŸæ­£å…è´¹çš„æ¸ é“"""
        free_tags = {"free", "å…è´¹", "0cost", "nocost", "trial"}
        
        # ğŸ”¥ ä¼˜å…ˆæ£€æŸ¥æ¨¡å‹çº§åˆ«çš„å®šä»·ä¿¡æ¯ï¼ˆä»ç¼“å­˜ä¸­è·å–ï¼‰
        if model_name:
            model_specs = self._get_model_specs(channel.id, model_name)
            if model_specs and 'raw_data' in model_specs:
                raw_data = model_specs['raw_data']
                if 'pricing' in raw_data:
                    pricing = raw_data['pricing']
                    prompt_cost = float(pricing.get('prompt', '0'))
                    completion_cost = float(pricing.get('completion', '0'))
                    if prompt_cost == 0.0 and completion_cost == 0.0:
                        logger.debug(f"FREE SCORE: Model '{model_name}' confirmed free via model-level pricing (prompt={prompt_cost}, completion={completion_cost})")
                        return 1.0
                    else:
                        logger.debug(f"FREE SCORE: Model '{model_name}' has non-zero costs (prompt={prompt_cost}, completion={completion_cost}), not free")
                        return 0.1
        
        # ğŸ”¥ æ£€æŸ¥æ¨¡å‹åç§°æ¨¡å¼ - å¯¹äºæ˜ç¡®çš„å…è´¹æ¨¡å‹åç¼€
        if model_name:
            model_lower = model_name.lower()
            # æ˜ç¡®çš„å…è´¹æ¨¡å‹æ ‡è¯†
            if (":free" in model_lower or 
                "-free" in model_lower or 
                "_free" in model_lower):
                logger.debug(f"FREE SCORE: Model '{model_name}' has explicit :free suffix, assumed free")
                return 1.0
            
            # æ£€æŸ¥å…¶ä»–å…è´¹æ¨¡å‹æ¨¡å¼
            model_tags = self._extract_tags_from_model_name(model_name)
            model_tags_lower = [tag.lower() for tag in model_tags]
            if any(tag in free_tags for tag in model_tags_lower):
                # å¼€æºæ¨¡å‹é€šå¸¸å…è´¹
                if ("oss" in model_lower or 
                    "huggingface" in model_lower or
                    "hf" in model_lower):
                    logger.debug(f"FREE SCORE: Model '{model_name}' is open source model, assumed free")
                    return 1.0
                else:
                    # æ¨¡å‹åç§°åŒ…å«freeä½†éœ€è¦è¿›ä¸€æ­¥éªŒè¯
                    logger.debug(f"FREE SCORE: Model '{model_name}' has 'free' in name, needs verification")
                    # ç»§ç»­æ£€æŸ¥æ¸ é“çº§åˆ«ä¿¡æ¯
        
        # ğŸ”¥ æ£€æŸ¥æ¸ é“çº§åˆ«çš„å®šä»·ä¿¡æ¯
        cost_per_token = getattr(channel, 'cost_per_token', None)
        if cost_per_token:
            input_cost = cost_per_token.get("input", 0.0)
            output_cost = cost_per_token.get("output", 0.0)
            if input_cost <= 0.0 and output_cost <= 0.0:
                logger.debug(f"FREE SCORE: Channel '{channel.name}' confirmed free via cost_per_token (input={input_cost}, output={output_cost})")
                return 1.0
            # å¯¹äºæœ‰cost_per_tokenä½†éé›¶çš„æƒ…å†µï¼Œå¦‚æœæ¨¡å‹åæ˜ç¡®åŒ…å«:freeï¼Œä»è®¤ä¸ºè¯¥ç‰¹å®šæ¨¡å‹å…è´¹
            elif model_name and (":free" in model_name.lower() or "-free" in model_name.lower()):
                logger.debug(f"FREE SCORE: Channel '{channel.name}' has costs but model '{model_name}' explicitly marked as free")
                return 1.0
            else:
                # æ¸ é“æœ‰æˆæœ¬ä¸”æ¨¡å‹æœªæ˜ç¡®æ ‡è®°ä¸ºå…è´¹
                logger.debug(f"FREE SCORE: Channel '{channel.name}' has non-zero costs (input={input_cost}, output={output_cost}), not free")
                return 0.1
        
        # æ£€æŸ¥ä¼ ç»Ÿçš„pricingé…ç½®
        pricing = getattr(channel, 'pricing', None)
        if pricing:
            input_cost = pricing.get("input_cost_per_1k", 0.001)
            output_cost = pricing.get("output_cost_per_1k", 0.002)
            avg_cost = (input_cost + output_cost) / 2
            if avg_cost <= 0.0001:  # æä½æˆæœ¬
                logger.debug(f"FREE SCORE: Channel '{channel.name}' very low cost (avg={avg_cost:.6f}), scored as near-free")
                return 0.9
            elif avg_cost <= 0.001:  # ä½æˆæœ¬
                logger.debug(f"FREE SCORE: Channel '{channel.name}' low cost (avg={avg_cost:.6f}), scored as affordable")
                return 0.7
            else:
                # æœ‰æ˜ç¡®pricingä¸”ä¸ä¾¿å®œçš„ï¼Œä½†å¦‚æœæ¨¡å‹æ˜ç¡®æ ‡è®°ä¸º:freeï¼Œä»è®¤ä¸ºå…è´¹
                if model_name and (":free" in model_name.lower() or "-free" in model_name.lower()):
                    logger.debug(f"FREE SCORE: Channel has costs but model '{model_name}' explicitly marked as free")
                    return 1.0
                else:
                    logger.debug(f"FREE SCORE: Channel '{channel.name}' has significant costs (avg={avg_cost:.6f}), not free")
                    return 0.1
        
        # ğŸ”¥ æ£€æŸ¥æ¸ é“æ ‡ç­¾
        channel_tags_lower = [tag.lower() for tag in getattr(channel, 'tags', [])]
        if any(tag in free_tags for tag in channel_tags_lower):
            logger.debug(f"FREE SCORE: Channel '{channel.name}' has free tag in channel tags, assumed free")
            return 1.0
        
        # é»˜è®¤æƒ…å†µ - æ²¡æœ‰å…è´¹è¯æ®
        logger.debug(f"FREE SCORE: Channel '{channel.name}' no evidence of being free")
        return 0.1

    def _calculate_local_score(self, channel: Channel, model_name: str = None) -> float:
        """è®¡ç®—æœ¬åœ°ä¼˜å…ˆè¯„åˆ†"""
        # æ£€æŸ¥æ¸ é“æ ‡ç­¾ä¸­æ˜¯å¦åŒ…å«æœ¬åœ°ç›¸å…³æ ‡ç­¾
        local_tags = {"local", "æœ¬åœ°", "localhost", "127.0.0.1", "offline", "edge"}
        
        # ä»æ¸ é“æ ‡ç­¾æ£€æŸ¥
        channel_tags_lower = [tag.lower() for tag in channel.tags]
        if any(tag in local_tags for tag in channel_tags_lower):
            return 1.0
        
        # ä»æ¨¡å‹åç§°æå–æ ‡ç­¾æ£€æŸ¥
        if model_name:
            model_tags = self._extract_tags_from_model_name(model_name)
            model_tags_lower = [tag.lower() for tag in model_tags]
            if any(tag in local_tags for tag in model_tags_lower):
                return 1.0
        
        # æ£€æŸ¥base_urlæ˜¯å¦æŒ‡å‘æœ¬åœ°åœ°å€
        base_url = getattr(channel, 'base_url', None)
        if base_url:
            base_url_lower = base_url.lower()
            local_indicators = ["localhost", "127.0.0.1", "0.0.0.0", "::1"]
            if any(indicator in base_url_lower for indicator in local_indicators):
                return 1.0
        
        # æ£€æŸ¥provideræ˜¯å¦æŒ‡å‘æœ¬åœ°åœ°å€
        provider_config = None
        if hasattr(self.config, 'providers'):
            provider_config = next((p for p in self.config.providers.values() if p.name == channel.provider), None)
        
        if provider_config and hasattr(provider_config, 'base_url'):
            provider_url_lower = provider_config.base_url.lower()
            local_indicators = ["localhost", "127.0.0.1", "0.0.0.0", "::1"]
            if any(indicator in provider_url_lower for indicator in local_indicators):
                return 1.0
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§æœ¬åœ°æ¨¡å‹åç§°
        if model_name:
            local_model_patterns = ["ollama", "llama.cpp", "local", "è‡ªå·±çš„", "ç§æœ‰"]
            model_lower = model_name.lower()
            if any(pattern in model_lower for pattern in local_model_patterns):
                return 0.8
        
        return 0.1  # é»˜è®¤è¾ƒä½è¯„åˆ†
    
    def _calculate_total_score(self, strategy: List[Dict[str, Any]], 
                             cost_score: float, speed_score: float, 
                             quality_score: float, reliability_score: float,
                             parameter_score: float = 0.5, context_score: float = 0.5,
                             free_score: float = 0.1, local_score: float = 0.1) -> float:
        """æ ¹æ®ç­–ç•¥è®¡ç®—æ€»è¯„åˆ†"""
        total_score = 0.0
        score_map = {
            "cost_score": cost_score, "speed_score": speed_score,
            "quality_score": quality_score, "reliability_score": reliability_score,
            "parameter_score": parameter_score, "context_score": context_score,
            "free_score": free_score, "local_score": local_score
        }
        
        total_weight = sum(rule.get("weight", 0.0) for rule in strategy)
        if total_weight == 0: 
            return 0.5

        for rule in strategy:
            field = rule.get("field", "")
            if field in score_map:
                score = score_map[field]
                if rule.get("order") == "asc": 
                    score = 1.0 - score
                total_score += score * rule.get("weight", 0.0)
        
        return total_score / total_weight
    
    def _hierarchical_sort(self, scored_channels: List[RoutingScore]) -> List[RoutingScore]:
        """åˆ†å±‚ä¼˜å…ˆçº§æ’åºï¼šä½¿ç”¨7ä½æ•°å­—è¯„åˆ†ç³»ç»Ÿ (æˆæœ¬|æœ¬åœ°|ä¸Šä¸‹æ–‡|å‚æ•°|é€Ÿåº¦|è´¨é‡|å¯é æ€§)"""
        def sorting_key(score: RoutingScore):
            # è·å–é¢å¤–çš„è¯„åˆ†ä¿¡æ¯
            channel = score.channel
            free_score = self._calculate_free_score(channel, score.matched_model)
            local_score = self._calculate_local_score(channel, score.matched_model)
            parameter_score = self._calculate_parameter_score(channel, score.matched_model)
            context_score = self._calculate_context_score(channel, score.matched_model)
            
            # å°†æ¯ä¸ªç»´åº¦çš„è¯„åˆ†è½¬æ¢ä¸º0-9çš„æ•´æ•°è¯„åˆ†
            # ç¬¬1ä½ï¼šæˆæœ¬ä¼˜åŒ–ç¨‹åº¦ (9=å®Œå…¨å…è´¹, 8=å¾ˆä¾¿å®œ, 0=å¾ˆæ˜‚è´µ)
            # ä¼˜å…ˆä½¿ç”¨å…è´¹è¯„åˆ†ï¼Œå¦‚æœä¸å…è´¹åˆ™ä½¿ç”¨æˆæœ¬è¯„åˆ†
            if free_score >= 0.9:
                cost_tier = 9  # å…è´¹æ¨¡å‹å›ºå®šä¸º9åˆ†
            else:
                cost_tier = min(8, int(score.cost_score * 8))  # ä»˜è´¹æ¨¡å‹æœ€é«˜8åˆ†
            
            # ç¬¬2ä½ï¼šæœ¬åœ°ä¼˜å…ˆç¨‹åº¦ (9=æœ¬åœ°, 0=è¿œç¨‹)
            local_tier = min(9, int(local_score * 9))
            
            # ç¬¬3ä½ï¼šä¸Šä¸‹æ–‡é•¿åº¦ç¨‹åº¦ (9=å¾ˆé•¿, 0=å¾ˆçŸ­)
            context_tier = min(9, int(context_score * 9))
            
            # ç¬¬4ä½ï¼šå‚æ•°é‡ç¨‹åº¦ (9=å¾ˆå¤§, 0=å¾ˆå°)
            parameter_tier = min(9, int(parameter_score * 9))
            
            # ç¬¬5ä½ï¼šé€Ÿåº¦ç¨‹åº¦ (9=å¾ˆå¿«, 0=å¾ˆæ…¢)
            speed_tier = min(9, int(score.speed_score * 9))
            
            # ç¬¬6ä½ï¼šè´¨é‡ç¨‹åº¦ (9=å¾ˆé«˜, 0=å¾ˆä½)
            quality_tier = min(9, int(score.quality_score * 9))
            
            # ç¬¬7ä½ï¼šå¯é æ€§ç¨‹åº¦ (9=å¾ˆå¯é , 0=ä¸å¯é )
            reliability_tier = min(9, int(score.reliability_score * 9))
            
            # ç»„æˆ7ä½æ•°å­—ï¼Œæ•°å­—è¶Šå¤§æ’åºè¶Šé å‰
            hierarchical_score = (
                cost_tier * 1000000 +       # ç¬¬1ä½ï¼šæˆæœ¬(å…è´¹=9,ä»˜è´¹æœ€é«˜=8)
                local_tier * 100000 +       # ç¬¬2ä½ï¼šæœ¬åœ°
                context_tier * 10000 +      # ç¬¬3ä½ï¼šä¸Šä¸‹æ–‡
                parameter_tier * 1000 +     # ç¬¬4ä½ï¼šå‚æ•°é‡
                speed_tier * 100 +          # ç¬¬5ä½ï¼šé€Ÿåº¦
                quality_tier * 10 +         # ç¬¬6ä½ï¼šè´¨é‡
                reliability_tier            # ç¬¬7ä½ï¼šå¯é æ€§
            )
            
            # è¿”å›è´Ÿæ•°å®ç°é™åºæ’åˆ—ï¼ˆåˆ†æ•°è¶Šé«˜æ’åè¶Šå‰ï¼‰
            return -hierarchical_score
        
        sorted_channels = sorted(scored_channels, key=sorting_key)
        
        # è®°å½•åˆ†å±‚æ’åºçš„è¯¦ç»†ä¿¡æ¯
        logger.info("ğŸ† HIERARCHICAL SORTING: 7-digit scoring system (Cost|Local|Context|Param|Speed|Quality|Reliability)")
        for i, scored in enumerate(sorted_channels[:5]):
            free_score = self._calculate_free_score(scored.channel, scored.matched_model)
            local_score = self._calculate_local_score(scored.channel, scored.matched_model)
            parameter_score = self._calculate_parameter_score(scored.channel, scored.matched_model)
            context_score = self._calculate_context_score(scored.channel, scored.matched_model)
            
            # è®¡ç®—7ä½æ•°å­—è¯„åˆ†
            if free_score >= 0.9:
                cost_tier = 9  # å…è´¹æ¨¡å‹å›ºå®šä¸º9åˆ†
            else:
                cost_tier = min(8, int(scored.cost_score * 8))  # ä»˜è´¹æ¨¡å‹æœ€é«˜8åˆ†
            
            local_tier = min(9, int(local_score * 9))
            context_tier = min(9, int(context_score * 9))
            parameter_tier = min(9, int(parameter_score * 9))
            speed_tier = min(9, int(scored.speed_score * 9))
            quality_tier = min(9, int(scored.quality_score * 9))
            reliability_tier = min(9, int(scored.reliability_score * 9))
            
            hierarchical_score = (
                cost_tier * 1000000 + local_tier * 100000 + context_tier * 10000 + 
                parameter_tier * 1000 + speed_tier * 100 + quality_tier * 10 + reliability_tier
            )
            
            score_display = f"{cost_tier}{local_tier}{context_tier}{parameter_tier}{speed_tier}{quality_tier}{reliability_tier}"
            is_free = "FREE" if free_score >= 0.9 else "PAID"
            is_local = "LOCAL" if local_score >= 0.7 else "REMOTE"
            logger.info(f"ğŸ†   #{i+1}: '{scored.channel.name}' [{is_free}/{is_local}] "
                       f"Score: {score_display} (Total: {hierarchical_score:,})")
        
        return sorted_channels
    
    def _estimate_tokens(self, messages: List[Dict[str, Any]]) -> int:
        """ä½¿ç”¨tiktokenä¼°ç®—prompt tokens"""
        try:
            # å°è¯•ä»mainæ¨¡å—è·å–å·²ç»åˆå§‹åŒ–çš„encoder
            import main
            if hasattr(main, 'estimate_prompt_tokens'):
                return main.estimate_prompt_tokens(messages)
        except (ImportError, AttributeError):
            pass

        # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨ç®€å•å›é€€æ–¹æ³•
        logger.warning("æ— æ³•ä»mainæ¨¡å—è·å–tiktokenç¼–ç å™¨, tokenè®¡ç®—å°†ä½¿ç”¨ç®€å•æ–¹æ³•")
        total_chars = 0
        for message in messages:
            content = message.get("content", "")
            if isinstance(content, str):
                total_chars += len(content)
        return max(1, total_chars // 4)
    
    def _estimate_cost_for_channel(self, channel: Channel, request: RoutingRequest) -> float:
        """ä¼°ç®—ç‰¹å®šæ¸ é“çš„è¯·æ±‚æˆæœ¬"""
        try:
            # ä¼°ç®—tokenæ•°é‡
            input_tokens = self._estimate_tokens(request.messages)
            estimated_output_tokens = max(50, input_tokens // 4)  # é¢„ä¼°è¾“å‡ºtoken
            
            # è·å–æ¸ é“çš„å®šä»·ä¿¡æ¯
            model_cache = self.config_loader.get_model_cache()
            if channel.id in model_cache:
                discovered_info = model_cache[channel.id]
                models_pricing = discovered_info.get("models_pricing", {})
                
                # æŸ¥æ‰¾æ¨¡å‹çš„å®šä»·ä¿¡æ¯
                model_pricing = None
                for model_name, pricing in models_pricing.items():
                    if model_name == request.model or request.model in model_name:
                        model_pricing = pricing
                        break
                
                if model_pricing:
                    input_cost = model_pricing.get("input_cost_per_token", 0.0)
                    output_cost = model_pricing.get("output_cost_per_token", 0.0)
                    
                    total_cost = (input_tokens * input_cost + 
                                estimated_output_tokens * output_cost)
                    
                    return total_cost
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®šä»·ä¿¡æ¯ï¼Œä½¿ç”¨å…è´¹è¯„åˆ†æ¥ä¼°ç®—
            free_score = self._calculate_free_score(channel, request.model)
            if free_score >= 0.9:
                return 0.0  # å…è´¹æ¨¡å‹
            else:
                return 0.001  # é»˜è®¤ä¼°ç®—ä¸ºå¾ˆä½çš„æˆæœ¬
                
        except Exception as e:
            logger.warning(f"æˆæœ¬ä¼°ç®—å¤±è´¥: {e}")
            return 0.001
    
    def _is_suitable_for_chat(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦é€‚åˆchatå¯¹è¯ä»»åŠ¡"""
        if not model_name:
            return False
            
        model_lower = model_name.lower()
        
        # è¿‡æ»¤embeddingæ¨¡å‹
        embedding_keywords = ['embedding', 'embed', 'text-embedding']
        if any(keyword in model_lower for keyword in embedding_keywords):
            return False
            
        # è¿‡æ»¤çº¯visionæ¨¡å‹
        vision_only_keywords = ['vision-only', 'image-only']
        if any(keyword in model_lower for keyword in vision_only_keywords):
            return False
            
        # è¿‡æ»¤å·¥å…·ç±»æ¨¡å‹
        tool_keywords = ['tokenizer', 'classifier', 'detector']
        if any(keyword in model_lower for keyword in tool_keywords):
            return False
            
        return True

    def _extract_tags_from_model_name(self, model_name: str) -> List[str]:
        """ä»æ¨¡å‹åç§°ä¸­æå–æ ‡ç­¾ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        ä¾‹å¦‚: "qwen/qwen3-30b-a3b:free" -> ["qwen", "qwen3", "30b", "a3b", "free"]
        """
        if not model_name or not isinstance(model_name, str):
            return []
        
        # æ£€æŸ¥ç¼“å­˜
        if model_name in self._tag_cache:
            return self._tag_cache[model_name]
        
        import re
        
        # ä½¿ç”¨å¤šç§åˆ†éš”ç¬¦è¿›è¡Œæ‹†åˆ†: :, /, @, -, _, ,
        separators = r'[/:@\-_,]'
        parts = re.split(separators, model_name.lower())
        
        # æ¸…ç†å’Œè¿‡æ»¤æ ‡ç­¾
        tags = []
        for part in parts:
            part = part.strip()
            if part and len(part) > 1:  # å¿½ç•¥å•å­—ç¬¦å’Œç©ºæ ‡ç­¾
                # è¿›ä¸€æ­¥åˆ†è§£æ•°å­—å’Œå­—æ¯ç»„åˆï¼Œå¦‚ "30b" -> ["30b"]
                tags.append(part)
        
        # ç¼“å­˜ç»“æœ
        self._tag_cache[model_name] = tags
        return tags

    def _get_candidate_channels_by_auto_tags(self, positive_tags: List[str], negative_tags: List[str] = None) -> List[ChannelCandidate]:
        """æ ¹æ®æ­£è´Ÿæ ‡ç­¾è·å–å€™é€‰æ¸ é“ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰
        
        Args:
            positive_tags: å¿…é¡»åŒ…å«çš„æ ‡ç­¾åˆ—è¡¨
            negative_tags: å¿…é¡»æ’é™¤çš„æ ‡ç­¾åˆ—è¡¨
        """
        if negative_tags is None:
            negative_tags = []
            
        # å¦‚æœæ²¡æœ‰ä»»ä½•æ ‡ç­¾æ¡ä»¶ï¼Œè¿”å›ç©ºï¼ˆé¿å…è¿”å›æ‰€æœ‰æ¨¡å‹ï¼‰
        if not positive_tags and not negative_tags:
            return []
        
        # æ ‡å‡†åŒ–æ ‡ç­¾
        normalized_positive = [tag.lower().strip() for tag in positive_tags if tag and isinstance(tag, str)]
        normalized_negative = [tag.lower().strip() for tag in negative_tags if tag and isinstance(tag, str)]
        
        logger.info(f"ğŸ” TAG MATCHING: Searching for channels with positive tags: {normalized_positive}, excluding: {normalized_negative}")
        
        model_cache = self.config_loader.get_model_cache()
        if not model_cache:
            logger.warning("ğŸ” TAG MATCHING: Model cache is empty, cannot perform tag routing")
            return []
        
        logger.info(f"ğŸ” TAG MATCHING: Searching through {len(model_cache)} cached channels")
        
        # ä¸¥æ ¼åŒ¹é…ï¼šæ”¯æŒæ­£è´Ÿæ ‡ç­¾çš„ä¸¥æ ¼åŒ¹é…
        if not normalized_negative:
            # å¦‚æœæ²¡æœ‰è´Ÿæ ‡ç­¾ï¼Œä½¿ç”¨åŸæœ‰çš„æ–¹æ³•
            exact_candidates = self._find_channels_with_all_tags(normalized_positive, model_cache)
        else:
            # æœ‰è´Ÿæ ‡ç­¾ï¼Œä½¿ç”¨æ–°çš„æ­£è´Ÿæ ‡ç­¾åŒ¹é…æ–¹æ³•
            exact_candidates = self._find_channels_with_positive_negative_tags(normalized_positive, normalized_negative, model_cache)
        
        if exact_candidates:
            logger.info(f"ğŸ¯ STRICT MATCH: Found {len(exact_candidates)} channels matching positive: {normalized_positive}, excluding: {normalized_negative}")
            return exact_candidates
        
        logger.warning(f"âŒ NO MATCH: No channels found matching positive: {normalized_positive}, excluding: {normalized_negative}")
        return []
    
    def _find_channels_with_all_tags(self, tags: List[str], model_cache: dict) -> List[ChannelCandidate]:
        """æŸ¥æ‰¾åŒ…å«æ‰€æœ‰æŒ‡å®šæ ‡ç­¾çš„æ¸ é“å’Œæ¨¡å‹ç»„åˆ
        
        æ ‡ç­¾åŒ¹é…è§„åˆ™ï¼š
        1. é¦–å…ˆæ£€æŸ¥æ¸ é“çº§åˆ«çš„æ ‡ç­¾ (channel.tags)
        2. å¦‚æœæ¸ é“æ ‡ç­¾åŒ¹é…ï¼Œè¯¥æ¸ é“ä¸‹æ‰€æœ‰æ¨¡å‹éƒ½è¢«è§†ä¸ºåŒ¹é…
        3. å¦åˆ™æ£€æŸ¥ä»æ¨¡å‹åç§°æå–çš„æ ‡ç­¾
        4. å¯¹äº 'free' æ ‡ç­¾ï¼Œéœ€è¦ä¸¥æ ¼éªŒè¯æ¸ é“æ˜¯å¦çœŸçš„å…è´¹
        """
        candidate_channels = []
        matched_models = []
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«freeæ ‡ç­¾ - éœ€è¦ä¸¥æ ¼éªŒè¯
        has_free_tag = any(tag.lower() in {"free", "å…è´¹", "0cost", "nocost"} for tag in tags)
        
        # éå†æ‰€æœ‰æœ‰æ•ˆæ¸ é“
        for channel in self.config_loader.get_enabled_channels():
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨API Keyçº§åˆ«ç¼“å­˜æŸ¥æ‰¾æ–¹æ³•
            discovered_info = self.config_loader.get_model_cache_by_channel(channel.id)
            if not isinstance(discovered_info, dict):
                continue
                
            models = discovered_info.get("models", [])
            if not models:
                continue
            
            logger.debug(f"ğŸ” TAG MATCHING: Checking channel {channel.id} ({channel.name}) with {len(models)} models")
            
            # ç»Ÿä¸€çš„æ ‡ç­¾åˆå¹¶åŒ¹é…ï¼šæ¸ é“æ ‡ç­¾ + æ¨¡å‹æ ‡ç­¾
            channel_tags = getattr(channel, 'tags', []) or []
            channel_matches = 0
            
            for model_name in models:
                if not model_name:
                    continue
                    
                # ä»æ¨¡å‹åç§°æå–æ ‡ç­¾ï¼ˆå·²ç»è½¬ä¸ºå°å†™ï¼‰
                model_tags = self._extract_tags_from_model_name(model_name)
                
                # åˆå¹¶æ¸ é“æ ‡ç­¾å’Œæ¨¡å‹æ ‡ç­¾ï¼Œå¹¶è§„èŒƒåŒ–ä¸ºå°å†™
                combined_tags = list(set([tag.lower() for tag in channel_tags] + model_tags))
                
                # éªŒè¯æ‰€æœ‰æŸ¥è¯¢æ ‡ç­¾éƒ½åœ¨åˆå¹¶åçš„æ ‡ç­¾ä¸­ï¼ˆcase-insensitiveåŒ¹é…ï¼‰
                normalized_query_tags = [tag.lower() for tag in tags]
                if all(tag in combined_tags for tag in normalized_query_tags):
                    # ğŸ”¥ ä¸¥æ ¼éªŒè¯ free æ ‡ç­¾ - ç¡®ä¿æ¸ é“çœŸçš„å…è´¹
                    if has_free_tag:
                        free_score = self._calculate_free_score(channel, model_name)
                        # åªæœ‰çœŸæ­£å…è´¹çš„æ¸ é“æ‰ä¼šè¢«åŒ¹é… (free_score >= 0.9)
                        if free_score < 0.9:
                            logger.debug(f"âŒ FREE TAG VALIDATION FAILED: Channel '{channel.name}' model '{model_name}' has free_score={free_score:.2f} < 0.9, not truly free")
                            continue
                        else:
                            logger.debug(f"âœ… FREE TAG VALIDATED: Channel '{channel.name}' model '{model_name}' confirmed as truly free (score={free_score:.2f})")
                    
                    # è¿‡æ»¤æ‰ä¸é€‚åˆchatçš„æ¨¡å‹ç±»å‹
                    if self._is_suitable_for_chat(model_name):
                        candidate_channels.append(ChannelCandidate(
                            channel=channel,
                            matched_model=model_name
                        ))
                        matched_models.append(model_name)
                        channel_matches += 1
                        logger.debug(f"âœ… MERGED TAG MATCH: Channel '{channel.name}' model '{model_name}' -> tags: {combined_tags}")
                    else:
                        logger.debug(f"âš ï¸ FILTERED: Model '{model_name}' not suitable for chat (appears to be embedding/vision model)")
            
            if channel_matches > 0:
                logger.info(f"ğŸ¯ CHANNEL SUMMARY: Found {channel_matches} matching models in channel '{channel.name}' via merged channel+model tags")
        
        if matched_models:
            logger.info(f"ğŸ¯ TOTAL MATCHED MODELS: {len(matched_models)} models found: {matched_models[:5]}{'...' if len(matched_models) > 5 else ''}")
        
        return candidate_channels

    def _find_channels_with_positive_negative_tags(self, positive_tags: List[str], negative_tags: List[str], model_cache: dict) -> List[ChannelCandidate]:
        """æŸ¥æ‰¾åŒ¹é…æ­£æ ‡ç­¾ä½†æ’é™¤è´Ÿæ ‡ç­¾çš„æ¸ é“å’Œæ¨¡å‹ç»„åˆ
        
        Args:
            positive_tags: å¿…é¡»åŒ…å«çš„æ ‡ç­¾åˆ—è¡¨
            negative_tags: å¿…é¡»æ’é™¤çš„æ ‡ç­¾åˆ—è¡¨
            model_cache: æ¨¡å‹ç¼“å­˜
            
        Returns:
            ç¬¦åˆæ¡ä»¶çš„å€™é€‰æ¸ é“åˆ—è¡¨
        """
        candidate_channels = []
        matched_models = []
        
        # éå†æ‰€æœ‰æœ‰æ•ˆæ¸ é“
        for channel in self.config_loader.get_enabled_channels():
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨API Keyçº§åˆ«ç¼“å­˜æŸ¥æ‰¾æ–¹æ³•
            discovered_info = self.config_loader.get_model_cache_by_channel(channel.id)
            if not isinstance(discovered_info, dict):
                continue
                
            models = discovered_info.get("models", [])
            if not models:
                continue
            
            logger.debug(f"ğŸ” POSITIVE/NEGATIVE TAG MATCHING: Checking channel {channel.id} ({channel.name}) with {len(models)} models")
            
            # ç»Ÿä¸€çš„æ ‡ç­¾åˆå¹¶åŒ¹é…ï¼šæ¸ é“æ ‡ç­¾ + æ¨¡å‹æ ‡ç­¾
            channel_tags = getattr(channel, 'tags', []) or []
            channel_matches = 0
            
            for model_name in models:
                if not model_name:
                    continue
                    
                # ä»æ¨¡å‹åç§°æå–æ ‡ç­¾ï¼ˆå·²ç»è½¬ä¸ºå°å†™ï¼‰
                model_tags = self._extract_tags_from_model_name(model_name)
                
                # åˆå¹¶æ¸ é“æ ‡ç­¾å’Œæ¨¡å‹æ ‡ç­¾ï¼Œå¹¶è§„èŒƒåŒ–ä¸ºå°å†™
                combined_tags = list(set([tag.lower() for tag in channel_tags] + model_tags))
                
                # æ£€æŸ¥æ­£æ ‡ç­¾ï¼šæ‰€æœ‰æ­£æ ‡ç­¾éƒ½å¿…é¡»åœ¨åˆå¹¶åçš„æ ‡ç­¾ä¸­ï¼ˆcase-insensitiveï¼‰
                positive_match = True
                if positive_tags:
                    normalized_positive = [tag.lower() for tag in positive_tags]
                    positive_match = all(tag in combined_tags for tag in normalized_positive)
                
                # æ£€æŸ¥è´Ÿæ ‡ç­¾ï¼šä»»ä½•è´Ÿæ ‡ç­¾éƒ½ä¸èƒ½åœ¨åˆå¹¶åçš„æ ‡ç­¾ä¸­ï¼ˆcase-insensitiveï¼‰
                negative_match = True
                if negative_tags:
                    normalized_negative = [tag.lower() for tag in negative_tags]
                    negative_match = not any(tag in combined_tags for tag in normalized_negative)
                
                # åªæœ‰åŒæ—¶æ»¡è¶³æ­£æ ‡ç­¾å’Œè´Ÿæ ‡ç­¾æ¡ä»¶çš„æ¨¡å‹æ‰è¢«é€‰ä¸­
                if positive_match and negative_match:
                    # è¿‡æ»¤æ‰ä¸é€‚åˆchatçš„æ¨¡å‹ç±»å‹
                    if self._is_suitable_for_chat(model_name):
                        candidate_channels.append(ChannelCandidate(
                            channel=channel,
                            matched_model=model_name
                        ))
                        matched_models.append(model_name)
                        channel_matches += 1
                        logger.debug(f"âœ… POSITIVE/NEGATIVE TAG MATCH: Channel '{channel.name}' model '{model_name}' -> tags: {combined_tags}")
                    else:
                        logger.debug(f"âš ï¸ FILTERED: Model '{model_name}' not suitable for chat (appears to be embedding/vision model)")
                else:
                    if not positive_match:
                        logger.debug(f"âŒ POSITIVE MISMATCH: Model '{model_name}' missing required tags from {positive_tags}")
                    if not negative_match:
                        logger.debug(f"âŒ NEGATIVE MISMATCH: Model '{model_name}' contains excluded tags from {negative_tags}")
            
            if channel_matches > 0:
                logger.info(f"ğŸ¯ CHANNEL SUMMARY: Found {channel_matches} matching models in channel '{channel.name}' via positive/negative tag filtering")
        
        if matched_models:
            logger.info(f"ğŸ¯ TOTAL MATCHED MODELS: {len(matched_models)} models found: {matched_models[:5]}{'...' if len(matched_models) > 5 else ''}")
        
        return candidate_channels

    def get_available_models(self) -> List[str]:
        """è·å–ç”¨æˆ·å¯ç›´æ¥è¯·æ±‚çš„ã€åœ¨é…ç½®ä¸­å®šä¹‰çš„æ¨¡å‹åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        if self._available_models_cache is not None:
            return self._available_models_cache
        
        models = set()
        all_tags = set()
        
        # æ·»åŠ ç‰©ç†æ¨¡å‹åç§°
        for ch in self.config.channels:
            if ch.enabled and ch.model_name:
                models.add(ch.model_name)
                
                # ä»é…ç½®çš„tagsä¸­æ·»åŠ 
                for tag in ch.tags:
                    if tag:
                        all_tags.add(f"tag:{tag}")
        
        # ä»æ¨¡å‹ç¼“å­˜ä¸­æå–è‡ªåŠ¨æ ‡ç­¾
        model_cache = self.config_loader.get_model_cache()
        if model_cache:
            for channel_id, cache_info in model_cache.items():
                if not isinstance(cache_info, dict):
                    continue
                
                models_list = cache_info.get("models", [])
                if not isinstance(models_list, list):
                    continue
                    
                for model_name in models_list:
                    if model_name:
                        models.add(model_name)
                        # æå–è‡ªåŠ¨æ ‡ç­¾
                        auto_tags = self._extract_tags_from_model_name(model_name)
                        for tag in auto_tags:
                            if tag:
                                all_tags.add(f"tag:{tag}")
        
        # ç¼“å­˜ç»“æœ
        result = sorted(list(models | all_tags))
        self._available_models_cache = result
        return result
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        self._tag_cache.clear()
        self._available_tags_cache = None
        self._available_models_cache = None
        logger.info("Router cache cleared")
    
    def update_channel_health(self, channel_id: str, success: bool, latency: Optional[float] = None):
        """æ›´æ–°æ¸ é“å¥åº·çŠ¶æ€"""
        self.config_loader.update_channel_health(channel_id, success, latency)

# å…¨å±€è·¯ç”±å™¨å®ä¾‹
_router: Optional[JSONRouter] = None

def get_router() -> JSONRouter:
    """è·å–å…¨å±€è·¯ç”±å™¨å®ä¾‹"""
    global _router
    if _router is None:
        _router = JSONRouter()
    return _router